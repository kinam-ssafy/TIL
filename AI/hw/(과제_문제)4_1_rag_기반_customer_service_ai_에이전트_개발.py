# -*- coding: utf-8 -*-
"""(과제_문제)4_1_RAG_기반_Customer_Service_AI_에이전트_개발.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfsKRrpZLo7GEfJt_d1l0OENNrl4FOFv

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : 본 자료는 삼성청년SW·AI아카데미의 컨텐츠 자산으로, 보안서약서에 의거하여 어떠한 사유로도 임의로 복사, 촬영, 녹음, 복제, 보관, 전송하거나 허가 받지 않은 저장매체를 이용한 보관, 제3자에게 누설, 공개 또는 사용하는 등의 무단 사용 및 불법 배포 시 법적 조치를 받을 수 있습니다.

# **Customer Service AI 에이전트 개발 : RAG 기반**

## **1. 과제 개요**

본 과제는 RAG(Retrieval-Augmented Generation) 기술을 핵심으로 하는 Customer Service AI 에이전트를 개발하는 과정을 학습합니다. AI 온라인 서점 시나리오를 통해, 에이전트가 사용자의 질문을 이해하고, 자체 보유한 지식 기반(배송 정책 등)에서 관련 정보를 검색하여 정확하고 신뢰성 있는 답변을 생성하는 능력을 구축하는 데 초점을 맞춥니다. 텍스트 데이터의 효율적인 처리(청킹, 파싱), 검색 가능한 데이터베이스(Vector Store) 구축, 그리고 검색기(Retriever)를 에이전트의 도구로 활용하는 방법을 단계별로 실습합니다.

## **2. 과제 진행 목적 및 배경**

*   **목적:** LLM의 정보 한계 및 환각 문제를 극복하고 특정 도메인 지식을 활용하기 위한 핵심 기술인 RAG 파이프라인의 구축 과정을 이해하고 실습하여, 실제 서비스에 적용 가능한 AI 에이전트 개발 역량을 강화합니다.
*   **배경:** 현대 AI 챗봇은 단순 대화를 넘어 특정 업무를 수행하거나 전문 지식에 기반한 답변을 제공해야 합니다. RAG는 이러한 요구사항을 충족시키는 효과적인 접근 방식이며, LangChain과 같은 프레임워크는 RAG 시스템 구축을 용이하게 합니다. 본 실습을 통해 RAG의 중요성을 인지하고, 관련 기술 스택(LangChain, Vector Store 등)을 활용하는 실무 역량을 배양합니다.

## **3. 과제 수행으로 얻어갈 수 있는 역량**

*   **RAG 파이프라인 이해 및 구축 능력:** 데이터 준비(청킹, 파싱), 임베딩, 벡터 스토어 구축, 검색기 생성으로 이어지는 RAG의 전체 흐름을 이해하고 각 구성 요소를 직접 구현할 수 있습니다.
*   **텍스트 처리 및 임베딩 활용 능력:** 텍스트 데이터를 효율적으로 분할하는 청킹 기법과 텍스트의 의미를 벡터로 변환하는 임베딩 모델의 역할을 이해하고 활용할 수 있습니다.
*   **Vector Store 및 검색기 활용 능력:** Vector Store에 데이터를 저장하고, 저장된 데이터에서 관련 정보를 검색하는 Retriever를 구축하고 활용할 수 있습니다.
*   **LangChain 활용 능력:** LangChain 프레임워크를 사용하여 RAG 구성 요소를 연결하고 에이전트의 도구로 통합하는 기본 방법을 습득합니다.

## **4. 과제 핵심 내용**

*   **환경 설정:** 실습에 필요한 라이브러리(LangChain, Vector Store 관련 라이브러리 등) 설치 및 필요한 API 키(임베딩 모델 사용 시) 설정.
*   **텍스트 데이터 준비 및 처리:**
    *   **청킹 및 토크나이징 이해:** 텍스트 분할 단위의 개념 이해 및 실습 (과제 1). `chunk_size` 및 `chunk_overlap` 설정에 따른 결과 비교.
    *   **자체 데이터 로딩 및 분할:** `TextLoader`를 사용하여 `shipping_policy.txt`와 같은 자체 데이터를 불러오고, `RecursiveCharacterTextSplitter`를 사용하여 검색에 용이한 청크로 분할 (과제 2). 다양한 자료 형식 파싱 및 멀티모달 파서 개념 논의.
*   **Vector Store 및 검색기 구축:**
    *   **임베딩 생성:** 분할된 텍스트 청크를 임베딩 모델을 사용하여 벡터로 변환.
    *   **Vector Store 구축 및 데이터 저장:** ChromaDB와 같은 Vector Store를 설정하고 임베딩된 벡터 데이터 저장.
    *   **Retriever 생성:** Vector Store를 기반으로 관련 문서를 검색하는 Retriever 구축.
*   **Retriever를 에이전트 도구로 통합:** `create_retriever_tool`을 사용하여 생성된 Retriever를 에이전트가 사용할 수 있는 '배송 정책 검색 도구'와 같은 형태로 정의.

# **Prerequisites**

코랩에서 기본적으로 설치된 라이브러리와 새로 설치하는 라이브리리 사이에서 발생하는 의존성 문제입니다. 이는 Python 패키지 관리 구조상, 특정 버전 조합이 완벽히 호환되지 않는 경우가 많기에 발생하고, 강의 실습에 큰 영향을 주지 않는 단순 Error이니 안심하고 실습을 진행해주셔도 됩니다.
"""

# 버전 명시
# 실제 사용 시 호환성을 확인하여 버전을 조정해주세요.
!pip install langchain-text-splitters==0.3.9
!pip install tiktoken==0.11.0
!pip install langchain-community==0.3.27
!pip install langchain-openai==0.3.31
!pip install chromadb==1.0.20
!pip install langchain-upstage==0.7.3
!pip install pypdf==4.3.1
!pip install pymupdf==1.26.3
!pip install pypdfium2==4.30.0
!pip install langchain-mcp-adapters==0.1.10

"""# **Exercise Overview**

## **과제 목차**
- **1. 환경 설정**: 실습에 필요한 환경을 미리 세팅해둡니다. <br>
  - 1. Hello World
    - 1-1. 자료에 청킹을 적용해보기
    - 1-2. API Key를 사용하여 파싱에 활용해보기

- **2. Langchain 이해하기** : LangChain을 사용하여 LLM Chain을 구현하는 방법을 배웁니다. <br>
    - 2-1 LLM Chain
    - 2-2 What is Langchain?

- **3. RAG & Tool 이해하기** : Vector Store 및 Retriever를 생성 구현하는 방법을 배웁니다. <br>
    - 3-1 What is RAG?
    - 3-2 What is Tool?
    - 3-3 Agent 평가하기

# 1. 환경 설정

**LLM이 문서를 이해하기 위해 청킹/파싱이 필요한 이유:**

LLM은 마치 매우 똑똑한 학생과 같아요. 하지만 이 학생에게도 몇 가지 한계가 있답니다.

---

**1. 한 번에 읽을 수 있는 양의 제한 (컨텍스트 창):**
- LLM은 한 번에 모든 문서를 통째로 읽고 이해할 수 없어요. 마치 사람이 책 한 권을 한 번에 다 읽고 내용을 기억하기 어려운 것처럼요. LLM이 한 번에 처리할 수 있는 텍스트의 양에는 제한이 있는데, 이걸 '컨텍스트 창(Context Window)'이라고 불러요.
만약 문서가 이 컨텍스트 창보다 훨씬 길다면, LLM은 문서의 앞부분만 읽거나 뒷부분만 읽게 되어 전체 내용을 파악하기 어렵겠죠?
- ✨ 청킹(Chunking)이 필요한 이유: 그래서 긴 문서를 컨텍스트 창 크기에 맞는 작은 '청크(Chunk)'들로 나누는 거예요. 이렇게 하면 LLM이 각 청크를 하나씩 집중해서 읽고 이해할 수 있게 됩니다. 마치 긴 책을 여러 장으로 나눠서 읽는 것과 같아요.

**2. 다양한 형태의 정보 처리의 어려움:**
- LLM은 기본적으로 '텍스트' 형태로 된 정보를 가장 잘 처리해요. 하지만 우리가 보는 문서는 텍스트만 있는 게 아니죠? PDF 파일에는 표, 그림, 글자 크기 변화가 있고, 웹페이지에는 이미지, 링크, 복잡한 구조가 있어요.
- ✨ 파싱(Parsing)이 필요한 이유: 파싱은 이렇게 다양한 형태의 문서에서 LLM이 이해할 수 있는 '순수한 텍스트'와 필요한 정보(메타데이터 등)를 뽑아내는 과정이에요. 마치 문서에서 중요한 내용만 필기하는 것과 같아요. 잘 파싱된 텍스트는 LLM이 정보를 더 정확하고 효율적으로 이해하고 활용할 수 있게 해줍니다.

**요약:**

- 청킹: LLM이 한 번에 처리할 수 있는 양의 한계를 극복하고 긴 문서를 나눠서 효율적으로 읽게 해줘요. (긴 책을 장별로 나눠 읽기)
- 파싱: 다양한 형식의 문서에서 LLM이 이해할 수 있는 필요한 정보(주로 텍스트)를 추출해줘요. (문서에서 중요한 내용만 필기하기)

## 1-1. 자료에 청킹을 적용해보기

긴 텍스트를 다루는 두 가지 방법인 **토크나이징(Tokenizing)**과 **청킹(Chunking)**의 차이를 알아봅시다.
. RAG 시스템에서 정보를 효율적으로 검색하고 LLM이 처리할 수 있도록 데이터를 준비하는 데 사용됩니다.

*   **토크나이징(Tokenizing):**
    *   텍스트를 LLM이 이해하는 작은 단위(단어, 구두점 등)인 '토큰'으로 쪼개는 과정입니다.
    *   `tiktoken` 라이브러리를 사용하여 특정 모델(`cl100k_base`)이 텍스트를 어떻게 토큰화하는지 보여줍니다.
    *   원문 텍스트의 길이(문자 수)와 토큰화했을 때의 토큰 수를 비교하여, 문자와 토큰이 1:1로 대응하지 않음을 보여줍니다.
    *   LLM은 토큰 단위로 텍스트를 읽고 처리하며, 모델마다 처리할 수 있는 최대 토큰 수(컨텍스트 창 크기)가 정해져 있습니다.

*   **청킹(Chunking):**
    *   긴 텍스트 문서를 의미적으로 관련 있는 더 작은 '청크(Chunk)' 단위로 묶어서 나누는 과정입니다.
    *   `RecursiveCharacterTextSplitter`라는 도구를 사용합니다. 이 도구는 정해진 규칙(예: 줄바꿈, 공백 등)에 따라 텍스트를 재귀적으로 분할합니다.
    *   **`chunk_size`:** 각 청크의 최대 크기를 문자의 개수로 지정합니다. 이 크기가 너무 크면 LLM의 컨텍스트 창을 넘어가거나 검색 시 불필요한 정보가 많이 포함될 수 있고, 너무 작으면 정보가 여러 청크에 흩어져 검색 품질이 떨어질 수 있습니다.
    *   **`chunk_overlap`:** 인접한 청크들 사이에 겹치는 부분의 크기를 문자의 개수로 지정합니다. 오버랩을 통해 청크가 분할될 때 문맥이 끊어지는 것을 방지하고, 검색 시 인접한 정보를 함께 찾을 확률을 높일 수 있습니다.
    *   다양한 `chunk_size`와 `chunk_overlap` 조합으로 청킹을 수행하여, 설정값에 따라 생성되는 청크의 개수와 각 청크의 내용이 어떻게 달라지는지 비교하여 보여줍니다.
    *   RAG 시스템에서는 사용자의 질문과 관련된 청크를 검색하여 LLM에게 제공하므로, 적절한 청킹은 검색 정확도와 LLM 응답 품질에 직접적인 영향을 미칩니다.



결론적으로, 토크나이징은 LLM의 '언어 이해 단위'이고, 청킹은 RAG 시스템의 '정보 검색 단위'를 만들기 위한 과정이라고 이해할 수 있습니다.

아래 예시 파일인 writefile shipping_policy.txt (AI 온라인 서점 배송 정책)를 이용하여 토크나이징, 청킹을 비교해보세요
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter
import tiktoken

# 예시 텍스트
text = """
AI 온라인 서점입니다. 고객님의 소중한 문의에 감사드립니다.
배송 정책에 대해 안내해 드리겠습니다.
일반 도서의 경우 오후 3시 이전 주문 시 당일 발송됩니다.
주말 및 공휴일은 배송이 어렵습니다.
제주 및 도서 산간 지역은 추가 배송비가 발생할 수 있습니다.
주문 번호 order-123의 배송 상태를 조회하시려면 마이페이지에서 확인 부탁드립니다.
"""

# 1. 토크나이징 (tiktoken 사용)
# 모델에 따라 인코딩 방식이 다를 수 있습니다. 여기서는 'cl100k_base'를 사용합니다.
encoding = tiktoken.get_encoding("cl100k_base")
tokens = encoding.encode(text)

print("--- 토크나이징 결과 ---")
print(f"원문 텍스트 길이: {len(text)} 문자")
print(f"토큰 수: {len(tokens)} 토큰")
print(f"토큰 예시: {tokens[:10]}...") # 첫 10개 토큰 예시
print("-" * 20)


# 2. 청킹 (RecursiveCharacterTextSplitter 사용)

# 다양한 chunk_size와 chunk_overlap으로 비교
chunking_configs = [
    {"chunk_size": 100, "chunk_overlap": 0},
    {"chunk_size": 100, "chunk_overlap": 20},
    {"chunk_size": 50, "chunk_overlap": 0},
    {"chunk_size": 50, "chunk_overlap": 10},
]

print("\n--- 청킹 결과 비교 ---")

for config in chunking_configs:
    chunk_size = config["chunk_size"]
    chunk_overlap = config["chunk_overlap"]

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len, # 길이를 문자로 계산
        is_separator_regex=False,
    )

    chunks = text_splitter.create_documents([text])

    print(f"\n[Chunk Size: {chunk_size}, Chunk Overlap: {chunk_overlap}]")
    print(f"생성된 청크 개수: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        print(f"  청크 {i+1} (길이: {len(chunk.page_content)}):")
        print(f"  '{chunk.page_content[:50]}...'") # 각 청크의 앞부분만 출력

print("-" * 20)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile shipping_policy.txt
# # AI 온라인 서점 배송 정책
# 
# ## 일반 배송
# - 평일 오후 3시 이전 주문 시 당일 발송됩니다.
# - 오후 3시 이후 주문 건은 익일 발송됩니다.
# - 주말 및 공휴일은 배송이 어렵습니다.
# 
# ## 도서 산간 지역 배송
# - 제주 및 도서 산간 지역은 추가 배송비가 발생할 수 있습니다.
# - 추가 배송비 및 예상 소요 시간은 주문 시 확인 가능합니다.
# 
# ## 배송 조회
# - 주문 번호 order-123의 배송 상태는 마이페이지에서 조회 가능합니다.
# - 회원 및 비회원 모두 주문 번호로 배송 조회가 가능합니다.
# - 배송 관련 문의는 고객센터로 연락 주시기 바랍니다.

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. TextLoader를 사용하여 파일 불러오기
loader = TextLoader("/content/shipping_policy.txt")
documents = loader.load()

print("--- 로드된 문서 정보 ---")
print(f"문서 개수: {len(documents)}")
if documents:
    print(f"첫 번째 문서 내용 (일부): {documents[0].page_content[:100]}...")
    print(f"첫 번째 문서 메타데이터: {documents[0].metadata}")
print("-" * 20)

# 2. RecursiveCharacterTextSplitter를 사용하여 문서 분할
# 적절한 chunk_size와 chunk_overlap을 설정합니다.
# 여기서는 예시로 설정하며, 실제 적용 시에는 데이터 특성에 맞게 조정해야 합니다.
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,  # 청크 최대 크기 (문자 단위)
    chunk_overlap=20, # 청크 간 겹치는 부분 (문자 단위)
    length_function=len, # 길이를 문자로 계산
    is_separator_regex=False,
)

chunks = text_splitter.split_documents(documents)

print("\n--- 분할된 청크 정보 ---")
print(f"생성된 청크 개수: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"  청크 {i+1} (길이: {len(chunk.page_content)}):")
    print(f"  '{chunk.page_content[:100]}...'") # 각 청크의 앞부분만 출력
    print(f"  메타데이터: {chunk.metadata}")
print("-" * 20)

"""## 1-2 UPSTAGE Credit 및 API Key 발급 받기

1. 회원 가입 진행
  1. <a href = "https://console.upstage.ai/">업스테이지 콘솔</a> 에 방문합니다.
  2. 계정이 없다면, 구글 계정을 통해 회원가입을 진행합니다
  3.  계정에 로그인 합니다.

2. API Key 발급
  1. <a href = "https://console.upstage.ai/api-keys">업스테이지 콘솔 - API Keys</a>페이지를 클릭합니다.
  2. Create New key를 누르고, 발급받은 API key를 복사합니다.
  3. 하단 셀을 실행한 후, 복사한 API Key를 넣습니다.
  4. 세션 재시작시에는 업스테이지 api 를 사용하는 코드를 사용하기 위해 반드시 다시 설정해야 합니다.

"""

# @title set API key
import os
import getpass
import warnings

warnings.filterwarnings("ignore")

# Get the Upstage API key using getpass
# if "UPSTAGE_API_KEY" not in os.environ or not os.environ["UPSTAGE_API_KEY"]:
os.environ["UPSTAGE_API_KEY"] = getpass.getpass("Enter your Upstage API key: ")

print("API key has been set successfully.")

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. TextLoader를 사용하여 파일 불러오기
loader = TextLoader("./shipping_policy.txt")
documents = loader.load()

print("--- 로드된 문서 정보 ---")
print(f"문서 개수: {len(documents)}")
if documents:
    print(f"첫 번째 문서 내용 (일부): {documents[0].page_content[:100]}...")
    print(f"첫 번째 문서 메타데이터: {documents[0].metadata}")
print("-" * 20)

# 2. RecursiveCharacterTextSplitter를 사용하여 문서 분할
# 적절한 chunk_size와 chunk_overlap을 설정합니다.
# 여기서는 예시로 설정하며, 실제 적용 시에는 데이터 특성에 맞게 조정해야 합니다.
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,  # 청크 최대 크기 (문자 단위)
    chunk_overlap=20, # 청크 간 겹치는 부분 (문자 단위)
    length_function=len, # 길이를 문자로 계산
    is_separator_regex=False,
)

chunks = text_splitter.split_documents(documents)

print("\n--- 분할된 청크 정보 ---")
print(f"생성된 청크 개수: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"  청크 {i+1} (길이: {len(chunk.page_content)}):")
    print(f"  '{chunk.page_content[:100]}...'") # 각 청크의 앞부분만 출력
    print(f"  메타데이터: {chunk.metadata}")
print("-" * 20)

"""# Langchain 이해하기


 ### 📘 LLM Chain이란?

 LLM Chain(체인)이란, **여러 구성 요소들을 결합**하여 **LLM의 출력을 생성하는 프로세스**를 의미합니다. 이는 입력 프롬프트를 모델에 전달하고, 모델의 출력을 받아 원하는 형식으로 처리할 수 있도록 구성된 파이프라인입니다. 각 단계별로 어떻게 Chain을 구성하는 지 알아봅시다.

#### LLM Chain 구성 요소
- **LLM(Large Language Model)**  : 모델 정의
- **Prompt(프롬프트)** : LLM에게 쿼리(질문)과 함께 입력하는 예제와 지시사항  
- Output Parser(출력 파서, *optional*) : 출력 결과의 형태를 지정할 수 있는 도구

### (Recap.) LLM의 구성요소

1. `User Input` : 사용자가 LLM에게 보낼 질문/쿼리/입력
2. `Prompt`
  - Task Description : LLM에게 특정 역할/task를 부여함. (e.g. 질의응답, 요약, 번역 등)
  - Output Indicator : 대화 스타일, 어조 등을 부여하는 등 output 형식들을 지정
  - Example Inputs : Few-Shot Prompting, LLM이 맥락을 더 파악할 수 있도록 예제를 넣어줌
3. `LLM Model` : user input을 바탕으로 output을 생성할 대규모 텍스트 데이터로 사전학습된 AI 모델
4. `Output` : LLM이 생성해내는 Text 답변/응답

## 2-1 What is Langchain?

<img src="https://media.licdn.com/dms/image/D4D12AQGQQFHNeQJRgQ/article-cover_image-shrink_720_1280/0/1711873462713?e=2147483647&v=beta&t=u5ls9p4LHatE_PxtiNIm23lIFGMaAjp-XHdV7TwwDxE" alt="Langchain" width="300" />

- LLM Chain을 제공해주는 라이브러리!

- LangChain은 대규모 언어 모델(LLM)을 활용해 AI Application(e.g.챗봇, QA 시스템 등)을 쉽게 개발할 수 있도록 도와주는 프레임워크입니다. 이는 누구나 LLM을 이용해 쉽고 빠르게 애플리케이션을 만들고 배포할 수 있도록 지원합니다. 프로그래밍 경험이 없어도 LangChain의 다양한 도구와 템플릿을 통해 필요한 AI 애플리케이션을 만들 수 있습니다. 복잡한 개발 과정을 간소화하여 AI 애플리케이션 개발을 더 쉽게 접근할 수 있게 해줍니다.


#### Langchain의 구성요소
1. [Langchain Library](https://python.langchain.com/v0.2/docs/introduction/) : Lanchain의 다양한 기능을 사용할 수 있게 구현해둔 패키지
   - **langchain-core**: LangChain의 가장 기본 문법
   - **Integrated Package** : 다른 AI 도구와 LangChain을 쉽게 연결할 수 있는 패키지 (e.g. [langchain-upstage](https://python.langchain.com/docs/integrations/providers/upstage/), [langchain_chroma](https://python.langchain.com/docs/integrations/providers/chroma/))
   - **langchain**: 애플리케이션 구성에 필요한 체인, 에이전트, 정보 검색 전략 등을 제공하여 애플리케이션의 '두뇌' 역할을 합니다.

2. [Langchain Templates](https://templates.langchain.com/): 다양한 작업에 맞춘 템플릿으로, 개발자들이 애플리케이션을 더 빠르게 설정하고 실행할 수 있도록 돕습니다.
3. [LangServe](https://python.langchain.com/docs/langserve/): LangChain으로 만든 애플리케이션을 REST API로 쉽게 배포할 수 있게 해주는 도구입니다.
4. [LangSmith](https://docs.smith.langchain.com/): 개발자가 애플리케이션을 디버그하고 테스트하며 모니터링할 수 있도록 도와주는 플랫폼입니다.
5. [LangGraph](https://www.langchain.com/langgraph): LLM이 가지는 여러 상태들을 관리하여 Agent를 구조화하여 구현할 수 있는 프레임워크입니다.

더 자세한 Langchain에 대한 설명은 <a href = "https://python.langchain.com/docs/introduction/">Langchain Documentation</a> 을 확인하세요!

### 코드 설명

1. 선호하는 LLM 정의:
   - `langchain_upstage`에서 `ChatUpstage` 클래스를 가져옵니다.
   - `ChatUpstage` 인스턴스를 생성하고 변수 `llm`에 할당합니다.

2. 입력 프롬프트 정의:
   - `langchain_core.prompts`에서 `ChatPromptTemplate` 클래스를 가져옵니다.
   - `from_messages()` 메서드를 사용해 `ChatPromptTemplate` 인스턴스를 생성합니다.
   -  system prompt, few-shot prompting을 위한 예제들, user input 등을 포함한 메시지 리스트를 제공합니다.

3. 출력 파서 정의 :
   - `langchain_core.output_parsers`에서 `StrOutputParser` 클래스를 가져옵니다.
   - 더 자세한 Parser에 대한 내용은 [Langchain Guide](https://python.langchain.com/docs/concepts/output_parsers/) 참고 바랍니다.

4. 체인 정의 :
   - `rag_with_history_prompt`, `llm`, `StrOutputParser()`를 파이프(`|`) 연산자를 사용하여 결합해 체인을 만듭니다.

4. 체인 호출:
   - `chain` 객체의 `invoke()` 메서드를 호출하고 빈 딕셔너리(`{}`)를 입력값으로 전달합니다.
   - 체인에서 얻은 응답을 출력합니다.
"""

# LLM Chain 구성하는 법.
# 1. llm 정의, 2. prompt 정의, 3. chain 정의, 4. chain 호출

# 1. define your favorate llm, solar
from langchain_upstage import ChatUpstage

llm = ChatUpstage()

# 2. define chat prompt
from langchain_core.prompts import ChatPromptTemplate # '대화' 형태로 prompt template 생성

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "모든 답변은 존댓말로 답변해줘."),

        # few-shot prompting
        ("human", "프랑스의 수도는 어딘지 알아?"),  # human request
        ("ai", "알고 있습니다. 파리입니다."),      # LLM response
        ("human", "일본의 수도는 어딘지 알아?"),
        ("ai",  "알고 있습니다. 도쿄입니다."),

        # User Query
        ("human", "그렇다면, 한국은?"),
    ]
)

# 3. define chain
from langchain_core.output_parsers import StrOutputParser #문자열(text, string)만 나오게 하는 출력 파서

# chain = prompt | llm  # without output parser

chain = prompt | llm | StrOutputParser() # with output parser

# 4. invoke the chain
c_result = chain.invoke({})
print(c_result)

"""# RAG & Tool 이해하기

**RAG : Retrieval-Augmented Generation**

* **RAG (Retrieval-Augmented Generation)**:
  * 단어의 뜻 (RAG 구성 요소):
    * **Retrieval (검색):** 사용자의 질문과 관련된 정보를 외부 지식 기반(Vector Store)에서 찾아오는 과정입니다.
    * **Augmented (증강된):** 검색된 정보를 LLM에게 추가로 제공하여, LLM이 그 정보를 바탕으로 더 풍부하고 정확한 답변을 생성하도록 돕는다는 의미입니다.
    * **Generation (생성):** LLM이 검색된 정보와 사용자의 질문을 바탕으로 자연어 답변을 만들어내는 과정입니다.RAG는 "검색 증강 생성"의 줄임말입니다. LLM이 답변을 생성할 때, 단순히 학습된 지식에만 의존하는 것이 아니라, 외부의 신뢰할 수 있는 정보원(우리의 Vector Store)에서 관련 정보를 검색(Retrieval)하여, 그 정보를 참고해서(Augmented) 답변을 생성(Generation)하도록 돕는 기술입니다.
  * **RAG 사용 이유:**
정보의 최신성: LLM은 학습된 시점까지의 정보만 알고 있습니다. RAG를 사용하면 LLM이 학습된 이후의 최신 정보나 특정 도메인(예: 우리 온라인 서점의 배송 정책)에 대한 정보를 활용할 수 있습니다.
    * **환각 (Hallucination) 문제 감소**: LLM은 때때로 사실이 아닌 내용을 지어내서 말하는 '환각' 현상을 보일 수 있습니다. RAG는 답변의 근거가 되는 실제 문서를 제공함으로써 LLM이 정확하고 신뢰성 있는 답변을 생성하도록 돕습니다.
    * **특정 도메인 지식 활용:** 모든 정보를 LLM에게 학습시키는 것은 비효율적이고 비용이 많이 듭니다. RAG는 특정 분야의 전문 지식 문서들만 Vector Store에 넣어두고 필요할 때 검색해서 사용하게 함으로써 효율성을 높입니다.


* **파싱 (Parsing):** 파싱은 "분석" 또는 "구문
분석"이라고도 이해할 수 있습니다. 컴퓨터가 이해하기 쉬운 구조로 데이터를 변환하는 과정을 말해요. 우리가 보는 문서는 글자, 그림, 표 등 다양한 형태로 복잡하게 구성되어 있죠. PDF 파일이나 웹 페이지처럼요. 파싱은 이런 복잡한 문서에서 우리가 필요한 '순수한 텍스트' 내용과 그 외 중요한 정보(예: 제목, 작성자, 페이지 번호 등 메타데이터)를 뽑아내서, LLM이 처리하기 좋은 단순한 텍스트 형태로 만드는 작업을 의미합니다.
  * **왜 파싱이 필요한가?** LLM은 기본적으로 텍스트를 가장 잘 처리합니다. 복잡한 구조나 그림이 있는 문서 전체를 그대로 주면 제대로 이해하기 어렵습니다. 파싱을 통해 핵심 텍스트만 추출하고 정돈하면, LLM이 문서를 더 빠르고 정확하게 이해하고 분석할 수 있게 됩니다.


* **파싱 <-> 청킹 <-> 인덱싱 관계:**

  이 세 과정은 RAG 시스템에서 외부 문서를 LLM이 활용할 수 있는 형태로 준비하는 파이프라인의 주요 단계입니다.
  1. **파싱:**  가장 먼저, 다양한 형식의 원본 문서(PDF, 웹페이지 등)에서 LLM이 처리하기 좋은 '순수한 텍스트'와 메타데이터를 추출하는 과정입니다.
  2. **청킹:** 파싱된 긴 텍스트를 LLM의 컨텍스트 창 한계와 검색 효율성을 고려하여 의미 있는 작은 단위인 '청크'들로 나누는 과정입니다.
  3. **인덱싱 (Indexing):**  이렇게 분할된 텍스트 청크들을 임베딩 모델을 사용하여 벡터로 변환하고, 이 벡터들을 Vector Store에 저장하는 과정 전체를 '인덱싱'이라고 부르기도 합니다. Vector Store는 이 벡터들을 효율적으로 검색할 수 있도록 일종의 '색인(Index)'을 만듭니다. 사용자의 질문이 들어오면, 질문도 벡터로 변환한 후, Vector Store의 색인을 사용하여 질문 벡터와 유사한 청크 벡터들을 빠르게 찾아냅니다. Retriever가 이 인덱싱된 Vector Store를 활용하여 검색을 수행합니다.

## 3-1 What is RAG?


1. Vector Store 생성: 앞서 준비한 텍스트 조각(청크)들을 컴퓨터가 이해할 수 있는 숫자의 배열(벡터)로 변환하고, 이 벡터들을 효율적으로 저장하고 검색할 수 있는 데이터베이스를 만드는 것입니다. 이 데이터베이스를 Vector Store라고 부릅니다. 여기서는 Chroma라는 Vector Store를 사용합니다.
2. Retriever 생성: Vector Store에 저장된 벡터들 중에서 사용자의 질문과 가장 관련이 깊은 벡터(즉, 질문의 의미와 가장 유사한 텍스트 청크)를 찾아주는 도구를 만듭니다. 이것을 Retriever (검색기)라고 합니다.
3. Retriever를 도구로 만들기: 이렇게 만든 Retriever를 우리 AI 에이전트가 사용할 수 있도록 '도구(Tool)' 형태로 만듭니다. 에이전트는 이 도구를 사용하여 사용자의 질문에 답변하기 위해 필요한 정보를 Vector Store에서 찾아올 수 있습니다.



- **처리순서**

  원본 문서 -(파싱)-> 정제된 텍스트 -(청킹)-> 텍스트 청크 리스트 -(인덱싱: 임베딩 & Vector Store 저장)-> Vector Store (검색 가능한 벡터 데이터)



### 코드설명

과제 3의 코드는 이 과정 중 3번(인덱싱 - Vector Store 생성)과 4번(Retriever 생성 및 도구화) 부분을 구현하는 것입니다.

코드를 다시 보시면, UpstageEmbeddings를 사용하여 텍스트 청크를 벡터로 변환하고, Chroma.from_documents로 Vector Store를 만들고 데이터를 저장하는 부분이 바로 인덱싱 단계에 해당합니다. 그리고 vectorstore.as_retriever()로 검색기를 만들고, create_retriever_tool로 에이전트 도구로 만드는 과정이 포함되어 있습니다.

### 실제 데이터 로드 및 파싱

### Subtask 1 :
`TextLoader`를 사용하여 배송정책 데이터 파일(`shipping_policy.txt`)을 로드하고 파싱합니다.

### Subtask 2 :

별도로 제공된 **yes24 보상·혜택 제도 관련 PDF 파일**을 `PyPDFLoader`로 로드하고 파싱합니다.  
- 아래 yes24 링크는 **출처 안내용**이며, 실습은 **이미 출처에서 추출해 제공된 PDF 파일**로 진행합니다.  
- Colab 환경에서는 아래 제공된 Google Drive 마운트 후 제공된 PDF를 드라이브에 업로드하고 진행합니다.

출처: [yes24 보상/혜택 제도 페이지](https://www.yes24.com/Mall/Help/FAQ?faqGb=34&faqSubGb=YD3#:~:text=*%20%EB%B3%80%EC%8B%AC%2C%20%EC%98%A4%EC%A3%BC%EB%AC%B8%EC%97%90%20%EC%9D%98%ED%95%9C,%ED%9B%84%20%EB%B0%98%ED%92%88%EC%9D%B4%20%EA%B0%80%EB%8A%A5%ED%95%A9%EB%8B%88%EB%8B%A4.)
"""

from google.colab import drive
drive.mount('/content/drive')

"""1. 코드를 실행하면 "Go to this URL in a browser:" 라는 메시지와 함께 링크가 나타납니다. 이 링크를 클릭하세요.
2. Google 계정 선택 화면이 나타나면 Google Drive에 연결할 계정을 선택합니다.
3. Google Drive 권한 요청 화면이 나타나면 "허용"을 클릭합니다.
4. 인증 코드가 나타나면 이 코드를 복사합니다.
5. Colab으로 돌아와 "Enter your authorization code:" 아래에 복사한 인증 코드를 붙여넣고 Enter 키를 누릅니다.

정상적으로 마운트가 완료되면 `/content/drive` 경로를 통해 Google Drive에 접근할 수 있습니다.
"""

import glob # 파일을 검색하기 위한 glob 모듈을 가져옵니다.
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyMuPDFLoader

# './' 경로에 있는 모든 PDF 파일을 찾습니다.
pdf_files = glob.glob('/content/drive/*.pdf')

all_documents = [] # 모든 PDF 파일에서 로드된 문서를 저장할 빈 리스트를 생성합니다.

print(f"--- './' 경로에서 찾은 PDF 파일 목록: {pdf_files} ---")

# 찾은 각 PDF 파일에 대해 로드 및 파싱을 수행합니다.
for pdf_filepath in pdf_files:
    try:
        #loader = PyPDFLoader(pdf_filepath)
        loader = PyMuPDFLoader(pdf_filepath)
        pages = loader.load()
        all_documents.extend(pages) # 로드된 페이지들을 all_documents 리스트에 추가합니다.
        print(f"'{pdf_filepath}' 파일 로드 완료. {len(pages)} 페이지 로드됨.")
    except Exception as e:
        print(f"'{pdf_filepath}' 파일 로드 중 오류 발생: {e}")

# shipping_policy.txt 파일도 로드하여 all_documents에 추가합니다.
shipping_policy_file_path = './shipping_policy.txt'

try:
    # Create a TextLoader instance with the file path
    loader = TextLoader(shipping_policy_file_path)

    # Load the document
    text_documents = loader.load()

    # Extend all_documents with text file documents
    all_documents.extend(text_documents)
    print(f"'{shipping_policy_file_path}' 파일 로드 완료. {len(text_documents)} 문서 로드됨.")


except FileNotFoundError:
    print(f"Error: File not found at {shipping_policy_file_path}. Please ensure the file exists at this location.")
except Exception as e:
    print(f"An error occurred while loading {shipping_policy_file_path}: {e}")


# 모든 로드된 총 문서(PDF 페이지 + Text 문서) 정보 출력
print("\n--- 모든 파일에서 로드된 총 문서 정보 ---")
print(f"총 문서 개수: {len(all_documents)}")
if all_documents:
    print(f"첫 번째 문서 내용 (일부): {all_documents[0].page_content[:200]}...")
    print(f"첫 번째 문서 메타데이터: {all_documents[0].metadata}")
else:
    print("로드된 문서가 없습니다.")
print("-" * 20)

# 다음 단계를 위해 변수 이름을 'documents'로 맞춥니다.
documents = all_documents

"""### 텍스트 청킹

### Subtask:
로드된 실제 데이터를 검색에 용이한 청크로 분할합니다.
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# 2. RecursiveCharacterTextSplitter를 사용하여 문서 분할
# 적절한 chunk_size와 chunk_overlap을 설정합니다.
# 여기서는 예시로 설정하며, 실제 적용 시에는 데이터 특성에 맞게 조정해야 합니다.
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,  # 청크 최대 크기 (문자 단위)
    chunk_overlap=20, # 청크 간 겹치는 부분 (문자 단위)
    length_function=len, # 길이를 문자로 계산
    is_separator_regex=False,
)

chunks = text_splitter.split_documents(documents)

print("\n--- 분할된 청크 정보 ---")
print(f"생성된 청크 개수: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"  청크 {i+1} (길이: {len(chunk.page_content)}):")
    print(f"  '{chunk.page_content[:100]}...'") # 각 청크의 앞부분만 출력
    print(f"  메타데이터: {chunk.metadata}")
print("-" * 20)

"""### Vector store 및 retriever 생성

### Subtask:
청킹된 실제 데이터를 사용하여 Vector Store를 생성하고 Retriever를 구축합니다.

"""

from langchain_community.vectorstores import Chroma
from langchain_upstage import UpstageEmbeddings

# 1. UpstageEmbeddings 인스턴스 생성
# 환경 변수에 Upstage API 키가 올바르게 설정되어 있는지 확인하세요.
embeddings = UpstageEmbeddings(model="embedding-query")

# 2. 청크로부터 Chroma Vector Store 생성
# 'chunks' 변수는 이전 하위 작업에서 성공적으로 생성되었습니다.
vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)

print("\n--- Vector Store 생성 완료 ---")
# 선택 사항: 확인을 위해 벡터 스토어에 있는 문서 수를 출력합니다.
try:
    print(f"Vector Store에 저장된 문서 개수: {vectorstore._collection.count()}")
except Exception as e:
    print(f"Vector Store 문서 개수 확인 중 오류 발생: {e}")


# 3. 벡터 스토어를 검색기로 변환
retriever = vectorstore.as_retriever()

print("--- Retriever 생성 완료 ---")
print("-" * 20)

"""### Naive RAG 수행"""

from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_upstage import ChatUpstage
from operator import itemgetter # itemgetter 임포트

# 이전 단계에서 retriever와 llm이 준비되었다고 가정합니다.
# 그렇지 않다면 여기서 정의하거나 임포트해야 합니다.
llm = ChatUpstage()

# RAG 체인을 위한 프롬프트 템플릿 정의
# 이 템플릿은 검색된 컨텍스트와 사용자 질문을 위한 플레이스홀더를 포함합니다.
rag_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "너는 AI 온라인 서점의 고객 서비스 에이전트야.\n모든 답변은 존댓말로 답변해줘.\n다음 컨텍스트를 참고해서 질문에 답변해:\n\n{context}"),
        ("human", "{question}"),
    ]
)

# 검색된 문서 목록을 하나의 문자열로 포맷하는 헬퍼 함수
def format_docs(docs):
    """문서 목록을 하나의 문자열로 포맷합니다."""
    return "\n\n".join(doc.page_content for doc in docs)


# LCEL을 사용하여 Naive RAG 체인 구성
# 체인은 다음을 수행합니다:
# 1. 사용자 질문을 입력으로 받습니다 ('question' 키를 가진 딕셔너리 형태).
# 2. itemgetter('question')을 사용하여 질문 문자열을 추출합니다.
# 3. 질문 문자열을 retriever에게 전달하여 관련 문서를 가져옵니다.
# 4. format_docs를 사용하여 검색된 문서를 포맷합니다.
# 5. 포맷된 컨텍스트와 원본 질문 (RunnablePassthrough 사용)을 rag_prompt에게 전달합니다.
# 6. 결과 프롬프트를 LLM에게 보내 최종 답변을 생성합니다.
rag_chain = (
    RunnablePassthrough.assign(context=itemgetter("question") | retriever | format_docs)
    | rag_prompt
    | llm
)

print("--- Naive RAG 체인 구성 완료 ---")
# 확인을 위해 체인 구조 출력
print(rag_chain)


# 테스트 질문으로 RAG 체인 실행
test_question = "주말에도 배송되나요?"

print(f"\n--- 질문 '{test_question}'으로 RAG 체인 테스트 중 ---")

# 테스트 질문으로 체인 호출
# 출력은 ChatMessage 객체이므로 content 속성에 접근합니다.
try:
    response = rag_chain.invoke({"question": test_question})
    print("\n--- RAG 체인 응답 ---")
    print(response.content)
    print("-" * 20)
except Exception as e:
    print(f"\n--- RAG 체인 실행 오류 ---")
    print(f"오류: {e}")
    print("'retriever'와 'llm'이 이전 셀에 정의되어 있는지 확인하십시오.")
    print("-" * 20)


# 다른 질문으로도 테스트할 수 있습니다.
# test_question_2 = "제주도 추가 배송비가 있나요?"
# print(f"\n--- 질문 '{test_question_2}'으로 RAG 체인 테스트 중 ---")
# try:
#     response_2 = rag_chain.invoke({"question": test_question_2})
#     print("\n--- RAG 체인 응답 ---")
#     print(response_2.content)
#     print("-" * 20)
# except Exception as e:
#     print(f"\n--- RAG 체인 실행 오류 ---")
#     print(f"오류: {e}")
#     print("'retriever'와 'llm'이 이전 셀에 정의되어 있는지 확인하십시오.")
#     print("-" * 20)

"""## 3-2 What is Agent & Tool?

### Agent (에이전트)란?

**정의:**

AI 에이전트는 목표 달성을 위해 계획을 세우고, 필요한 도구(Tool)를 사용하며, 관찰(Observation) 결과를 바탕으로 다음 행동을 결정하는 자율적인 시스템입니다. LLM은 에이전트의 '두뇌' 역할을 하지만, 에이전트는 단순히 텍스트를 생성하는 것을 넘어 외부 환경과 상호작용하고 문제를 해결하기 위해 설계됩니다.

Agent는 다음의 핵심 요소들로 구성될 수 있습니다:

*   **LLM (Large Language Model):** 에이전트의 의사결정 및 텍스트 생성 능력의 기반입니다. 사용자의 요청을 이해하고, 어떤 도구를 사용할지, 다음 단계는 무엇일지 등을 판단하는 역할을 합니다.
*   **Planner (계획자):** 복잡한 목표를 달성하기 위해 일련의 단계(Sub-goals)를 설정하고, 각 단계에 필요한 도구 사용 계획을 세웁니다.
*   **Tool (도구):** 에이전트가 특정 작업을 수행하거나 외부 정보에 접근하기 위해 사용하는 기능입니다. 예를 들어, 검색 엔진 도구, 계산기 도구, 데이터베이스 조회 도구 등이 있습니다.
*   **Memory (메모리):** 과거 대화 기록이나 작업 수행 정보를 기억하여 현재의 의사결정이나 다음 행동에 활용합니다.
*   **Observation (관찰):** 도구 사용 결과나 외부 시스템과의 상호작용 결과를 받아들이고 해석하는 과정입니다. 이 관찰 결과를 바탕으로 계획을 수정하거나 다음 행동을 결정합니다.

**사용 이유:**

LLM은 뛰어난 텍스트 이해 및 생성 능력을 가지고 있지만, 몇 가지 근본적인 한계가 있습니다. Agent는 이러한 LLM의 한계를 극복하고 더 복잡하고 실제적인 문제를 해결하기 위해 사용됩니다. Agent를 사용하는 주요 이유는 다음과 같습니다.

1.  **최신 정보 접근 및 활용:** LLM은 학습된 시점까지의 데이터만 알고 있습니다. Agent는 검색 엔진이나 데이터베이스와 같은 'Tool'을 사용하여 실시간 정보나 최신 데이터를 검색하고, 이를 바탕으로 답변하거나 작업을 수행할 수 있습니다. (RAG의 확장된 개념)
2.  **정확하고 신뢰성 있는 정보 제공 (환각 감소):** LLM은 때때로 사실과 다른 내용을 지어내거나 잘못된 정보를 제공하는 '환각(Hallucination)' 현상을 보입니다. Agent는 외부의 신뢰할 수 있는 정보 소스(Tool을 통해 접근)를 통해 정보를 확인하고 근거를 바탕으로 답변함으로써 환각을 줄이고 정보의 신뢰성을 높일 수 있습니다.
3.  **특정 작업 수행 및 자동화:** Agent는 계산, API 호출, 파일 조작, 외부 시스템과의 상호작용 등 다양한 'Tool'을 사용하여 특정 작업을 수행할 수 있습니다. 이를 통해 단순 질의응답을 넘어 복잡한 업무를 자동화하고 사용자 요청을 직접 처리하는 에이전트 역할을 할 수 있습니다.
4.  **복잡한 문제 해결:** Agent는 복잡한 문제를 작은 단계로 나누고, 각 단계에 맞는 도구를 사용하여 문제를 해결해 나가는 추론 과정을 수행할 수 있습니다. 이는 LLM 단독으로는 어려운 다단계 작업을 가능하게 합니다.
5.  **외부 환경과의 상호작용:** Agent는 Tool을 통해 단순히 텍스트를 주고받는 것을 넘어 외부 API를 호출하거나 데이터베이스를 업데이트하는 등 실제 환경과 상호작용하는 애플리케이션을 구축할 수 있게 합니다.

요약하자면, Agent는 LLM에 계획, 도구 사용, 관찰, 기억 등의 능력을 부여하여 LLM의 지능을 확장하고, 더 넓은 범위의 문제를 해결하며 실제 세계와 상호작용할 수 있도록 만드는 개념입니다.

### What is Tool?

- **정의:** AI 에이전트가 특정 작업을 수행하기 위해 사용하는 기능 또는 외부 리소스에 접근하는 방법입니다. LangChain에서는 @tool 데코레이터를 사용하여 Python 함수를 도구로 만들 수 있습니다.
- **사용 이유:**
LLM 자체의 정보/기능 한계 극복.
실시간 정보, 특정 데이터베이스 정보 등 외부 데이터 접근. 계산, API 호출 등 다양한 작업 수행을 통한 에이전트 기능 확장.

### Retriever 도구화

생성된 Retriever를 에이전트 도구로 만듭니다.
"""

# langchain 패키지의 tools 모듈에서 retriever 도구를 생성하는 함수를 가져옵니다.
from langchain.tools.retriever import create_retriever_tool


# retriever tool 생성
if 'retriever' in locals():
    shipping_policy_retriever_tool = create_retriever_tool(
        retriever,
        "shipping_policy_search_tool", # Tool name for the agent
        "Searches and returns information about the AI online bookstore's shipping policy." # Tool description (English is recommended)
    )

    # Create a list of tools
    agent_tools = [shipping_policy_retriever_tool]

    print("\n--- 배송 정책 검색 도구 생성 완료 ---")
    print(f"생성된 도구: {agent_tools}")
    print("-" * 20)
else:
    print("오류: 'retriever' 변수가 생성되지 않았습니다. Vector Store 및 Retriever 생성 단계를 먼저 실행해주세요.")

from langchain_core.tools import tool
@tool
def get_order_status(order_id: str) -> str:
    """주문 번호를 입력받아 해당 주문의 배송 상태를 반환합니다.
    알 수 없는 주문 번호의 경우, 해당 주문을 찾을 수 없음을 알립니다.
    """
    # 실제 시스템에서는 데이터베이스 조회 등의 로직이 들어갑니다.
    # 여기서는 예시를 위해 더미 데이터를 사용합니다.
    order_statuses = {
        "order-123": "상품 준비 중",
        "order-456": "배송 중",
        "order-789": "배송 완료",
    }

    status = order_statuses.get(order_id, f"주문 번호 {order_id}를 찾을 수 없습니다.")
    return status

print("--- 주문 조회 도구 생성 완료 ---")
print(f"생성된 도구 이름: {get_order_status.name}")
print(f"생성된 도구 설명: {get_order_status.description}")
print("-" * 20)

# 생성된 도구를 리스트에 추가 (이후 에이전트 빌드 시 사용)
# 이전에 생성된 tools 리스트에 추가하거나, 새로운 리스트를 생성합니다.
# 여기서는 새로운 리스트를 생성합니다.
agent_tools.append(get_order_status)

print("\n--- 현재 에이전트 도구 목록 ---")
print(agent_tools)
print("-" * 20)

# 도구 사용 예시 (실제 에이전트가 호출하는 방식)
# print(get_order_status.invoke("order-123"))
# print(get_order_status.invoke("order-999"))

from langchain.tools import tool

@tool
def greet_customer(query: str) -> str:
    """고객의 인삿말(예: 안녕, 안녕하세요)이 포함되어 있으면, 정해진 환영 메시지를 반환합니다."""
    greeting_keywords = ["안녕", "안녕하세요", "하이", "반가워요"]
    if any(keyword in query for keyword in greeting_keywords):
        return "안녕하세요! AI 온라인 서점입니다. 무엇을 도와드릴까요?"
    else:
        # 인삿말이 아닌 경우, 도구를 사용하지 않도록 빈 문자열 또는 None을 반환할 수도 있습니다.
        # 여기서는 간단히 인삿말이 아님을 알립니다.
        return "" # 에이전트가 이 도구를 사용하지 않도록 유도

print("--- 인사 도구 생성 완료 ---")
print(f"생성된 도구 이름: {greet_customer.name}")
print(f"생성된 도구 설명: {greet_customer.description}")
print("-" * 20)

# 생성된 도구를 기존 에이전트 도구 리스트에 추가
agent_tools.append(greet_customer)

print("\n--- 현재 에이전트 도구 목록 ---")
print(agent_tools)
print("-" * 20)

# 도구 사용 예시
# print(greet_customer.invoke("안녕하세요! 배송 문의드려요."))
# print(greet_customer.invoke("주문 상태 확인 부탁드립니다."))

"""## 4. Agent 성과 평가하기"""

def evaluate_response(question: str, agent_response: str, used_tool_name: str) -> int:
    """
    주어진 질문, 에이전트 응답, 사용된 도구 이름을 바탕으로 점수를 반환하는 평가 함수.
    Reward Model의 개념을 간접적으로 체험하기 위한 예시 함수입니다.
    실제 Reward Model은 훨씬 더 복잡하며, 에이전트의 행동(Action)과 결과(Observation)를 기반으로 학습됩니다.

    Args:
        question (str): 고객의 질문.
        agent_response (str): 에이전트의 최종 응답.
        used_tool_name (str): 에이전트가 질문 처리 시 사용한 도구의 이름 (없으면 None 또는 빈 문자열).

    Returns:
        int: 평가 점수 (Reward). 점수가 높을수록 더 나은 응답으로 간주합니다.
    """
    score = 0

    # 1. 질문 유형에 따른 올바른 도구 사용 여부 평가
    if "배송" in question and used_tool_name == "shipping_policy_search_tool":
        score += 50 # 배송 관련 질문에 정책 검색 도구 사용 시 높은 점수
    elif "주문 번호" in question and used_tool_name == "get_order_status":
         score += 50 # 주문 번호 관련 질문에 주문 조회 도구 사용 시 높은 점수
    elif any(keyword in question for keyword in ["안녕", "안녕하세요", "하이"]) and used_tool_name == "greet_customer":
         score += 30 # 인사 관련 질문에 인사 도구 사용 시 점수
    elif used_tool_name is None or used_tool_name == "":
        # 특정 도구가 필요 없는 일반적인 질문이거나, 도구를 사용하지 않는 것이 적절한 경우
        score += 10 # 기본 점수

    # 도구를 잘못 사용했거나 불필요하게 사용한 경우 감점 (예시)
    if used_tool_name is not None and used_tool_name != "":
        if ("배송" not in question and used_tool_name == "shipping_policy_search_tool") or \
           (("주문 번호" not in question or "상태" not in question) and used_tool_name == "get_order_status") or \
           (not any(keyword in question for keyword in ["안녕", "안녕하세요", "하이"]) and used_tool_name == "greet_customer" ):
            score -= 20 # 질문과 관련 없는 도구 사용 시 감점


    # 2. 응답 내용의 적절성 평가 (매우 기본적인 예시)
    # 실제로는 응답 내용의 정확성, 자연스러움 등을 복합적으로 평가해야 합니다.
    if "찾을 수 없습니다" in agent_response and "주문 번호" in question and "order-" in question:
        # 주문 번호가 잘못되었을 때 "찾을 수 없습니다"라고 응답하는 경우 (올바른 동작)
        score += 20
    elif "안녕하세요!" in agent_response and any(keyword in question for keyword in ["안녕", "안녕하세요"]):
         # 인삿말에 대해 올바르게 응답하는 경우
         score += 20

    # 점수가 음수가 되지 않도록 처리
    score = max(0, score)

    return score

print("--- 평가 함수 (evaluate_response) 정의 완료 ---")


# 에이전트 실행 및 평가 예시 (실제 에이전트 코드는 아직 빌드되지 않음)
# 여기서는 각 도구가 사용되었을 때의 가상 응답과 도구 사용 정보를 simulate하여 평가합니다.

test_cases = [
    {"question": "안녕하세요!", "simulated_tool_used": "greet_customer", "simulated_response": "안녕하세요! AI 온라인 서점입니다. 무엇을 도와드릴까요?"},
    {"question": "order-123 배송 상태 알려줘", "simulated_tool_used": "get_order_status", "simulated_response": "주문 번호 order-123의 배송 상태는 '상품 준비 중' 입니다."}, # get_order_status 도구의 예상 응답
    {"question": "주말에도 배송되나요?", "simulated_tool_used": "shipping_policy_search_tool", "simulated_response": "배송 정책에 따르면 주말 및 공휴일은 배송이 어렵습니다."}, # shipping_policy_search_tool 도구의 예상 응답
    {"question": "다른 문의사항이 있어요", "simulated_tool_used": None, "simulated_response": "네, 다른 문의사항이 있으시면 말씀해주세요."}, # 도구 사용이 불필요한 경우
    {"question": "order-999 배송 조회", "simulated_tool_used": "get_order_status", "simulated_response": "주문 번호 order-999를 찾을 수 없습니다."}, # 없는 주문 번호 조회 시
    {"question": "안녕하세요!", "simulated_tool_used": "get_order_status", "simulated_response": "주문 번호가 필요합니다."}, # 잘못된 도구 사용 예시
]

print("\n--- 에이전트 응답 평가 예시 ---")

for i, case in enumerate(test_cases):
    question = case["question"]
    simulated_tool_used = case["simulated_tool_used"]
    simulated_response = case["simulated_response"]

    # 실제 에이전트라면 여기서 에이전트를 실행하고 tool_used와 response를 얻어와야 합니다.
    # 여기서는 시뮬레이션된 값을 사용합니다.

    score = evaluate_response(question, simulated_response, simulated_tool_used)

    print(f"\n[테스트 {i+1}]")
    print(f"  질문: {question}")
    print(f"  시뮬레이션된 사용 도구: {simulated_tool_used}")
    print(f"  시뮬레이션된 에이전트 응답: {simulated_response}")
    print(f"  평가 점수 (Reward): {score}")

print("-" * 20)

"""# 과제 마무리

이번 과제를 통해 RAG의 절차에 대한 이해 및 VectorDB와 Retriever을 사용하여 검색 단계를 구현하는 방법을 배웠습니다. 또한, Retriever를  Tool로 사용하여 Agent가 사용하는 방법을 수행했습니다.

Parding, Chunking, Indexing이 무엇인지 이해하고 각각에 대한 결과는 어떠한지 비교하는 실험까지 다루어보았습니다.

이를 통해 여러분이 각자 프로젝트에서 설계한 서비스에 적절한 검색 방법론을 설계하고 구현할 수 있는 기초를 다질 수 있기를 바랍니다.
"""