# -*- coding: utf-8 -*-
"""(ì‹¤ìŠµ_ë¬¸ì œ)5-1_PEFT(íŒŒë¼ë¯¸í„°_íš¨ìœ¨ì _íŠœë‹)_Unsloth_ì‹¤ìŠµ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kul-gaOrpm-qMVu9AO2DZNJhiFmoGLaL

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**
1. ì‹¤ìŠµëª… : PEFT(íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŠœë‹)
2. í•µì‹¬ ì£¼ì œ: ì‹¤ìŠµ ì „ë°˜ì— ê±¸ì³ì„œ ìµíˆê²Œ ë˜ëŠ” í•µì‹¬ ì£¼ì œ 3ê°€ì§€ë¥¼ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
    1. Unslothì— ëŒ€í•œ ì´í•´
    2. LoRAì— ëŒ€í•œ ì´í•´
    3. LoRAë¥¼ ì´ìš©í•œ í•™ìŠµ
3. í•™ìŠµ ëª©í‘œ :
    1. Base ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ loadí•  ìˆ˜ ìˆë‹¤.
    2. LoRA ì„¤ì • ë° í•´ë‹¹ ì„¤ì •ì„ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆë‹¤.
    3. LoRA í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„ì™€ ì „ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë‹¤.
    4. LoRA ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•  ìˆ˜ ìˆë‹¤.
    5. LoRA fine-tuned modelì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆë‹¤.
4. í•™ìŠµ ê°œë…:
    1. UnSloth
    2. LoRA
    3. fine-tuning
5. í•™ìŠµ ë°©í–¥ :
   - ì‹¤ìŠµì€ ì½”ë“œë¥¼ ì‹¤í–‰í•´ê°€ë©° LoRAë¥¼ ì´ìš©í•œ í•™ìŠµ ë°©ì‹ì— ëŒ€í•´ì„œ ì´í•´í•˜ê³  ì§ì ‘ í•™ìŠµì„ ì§„í–‰í•´ë´…ë‹ˆë‹¤.
   - ì‹¤ìŠµ ì½”ë“œëŠ” ì¡°êµê°€ ì§ì ‘ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
   - í•´ë‹¹ ì‹¤ìŠµì€ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ë¯€ë¡œ ëª¨ë¸ì´ ì–´ë– í•œ ê³¼ì •ì„ ê±°ì³ì„œ í•™ìŠµë˜ëŠ”ì§€ì— ëŒ€í•œ ê°œê´„ì ì¸ ì´í•´ë„ë¥¼ ë†’ì´ëŠ”ë° ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.
6. ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´
    - ë°ì´í„°ì…‹ ëª… : [arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome)
    - ë°ì´í„°ì…‹ ê°œìš” : instruction followingì„ ìœ„í•œ curated dataset
    - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ : MIT

### **Prerequisites**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, re
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     import torch; v = re.match(r"[0-9\.]{3,}", str(torch.__version__)).group(0)
#     xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth
# !pip install transformers==4.55.4

"""# 1. Unsloth

- Unslothì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤.
- Unslothë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ë´…ë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  Unslothë€?</b><br>
UnslothëŠ” íš¨ìœ¨ì ì¸ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) íŒŒì¸íŠœë‹ ë° ê°•í™”í•™ìŠµ(RL) í”„ë ˆì„ì›Œí¬ë¡œ, ì†ë„ì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì¸¡ë©´ì—ì„œ ìµœì í™”ëœ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
</blockquote>

- ìˆ˜ì‘ì—…ìœ¼ë¡œ ìœ ë„ëœ ìˆ˜í•™ ì—°ì‚°ê³¼ GPU ì»¤ë„ì„ ì§ì ‘ ì‘ì„±í•´ì„œ ê¸°ì¡´ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë¨  ï¿¼ ï¿¼.
- ì˜¤í”ˆì†ŒìŠ¤ì´ë©° ë¬´ë£Œ ë²„ì „ ì œê³µ, Google Colabì´ë‚˜ Kaggleì—ì„œë„ ì†ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥  ï¿¼.

1. í•™ìŠµ ì†ë„ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨
- ê¸°ì¡´ FlashAttention 2 ê¸°ë°˜ë³´ë‹¤ ìµœëŒ€ 30ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ë©”ëª¨ë¦¬( VRAM) 30% ì ˆê°  ï¿¼.
- ì•ŒíŒŒì¹´(Alpaca) ë°ì´í„°ì…‹ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ìœ¼ë¡œ Llama 3.3 (70B) ëª¨ë¸ì—ì„œ í•™ìŠµ ì†ë„ 2ë°°, VRAM 75% ì´ìƒ ì ˆê°, ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ 13ë°° í–¥ìƒ  ï¿¼.

2. ì„¸ë¶€ ê¸°ìˆ ì  ì¥ì 
- ìµœì í™”ëœ AttentionÂ·MLPÂ·Normalization êµ¬í˜„, ì§ì ‘ ì‘ì„±ëœ Triton ì»¤ë„, torch.compile ì‚¬ìš© ë“±ì„ í†µí•œ ì„±ëŠ¥ ê°œì„   ï¿¼.
- QLoRA, LoRA, 4â€‘/8â€‘/16â€‘bit ë“± ì €ì •ë°€ í›ˆë ¨ ì§€ì›, VRAM ìš”êµ¬ëŸ‰ì„ ìµœì†Œí™”í•˜ë©´ì„œë„ ì •í™•ë„ ìœ ì§€  ï¿¼.
- RSLORA, LoftQ, ì¥ê¸° ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ VLM ë“± ìµœì‹  ê¸°ë²• ì§€ì›

ì‚¬ìš©í•  ëª¨ë¸ : [google/gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it)
- êµ¬ê¸€ì—ì„œ ê³µê°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

ì—¬ê¸°ì„œ íŠ¹ì´í•œ ì ì€ `google/gemma-3-270m-it` ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ [`unsloth/gemma-3-270m-it`](https://huggingface.co/unsloth/gemma-3-270m-it)ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. Fine-tuningì— íŠ¹í™”ëœ ë²„ì „ìœ¼ë¡œ Unslothê°€ ì–‘ìí™”í•˜ì—¬ ë³€í˜•í•œ ëª¨ë¸ì…ë‹ˆë‹¤.
"""

from unsloth import FastModel
import torch
max_seq_length = 2048 # í•™ìŠµ ë©”ëª¨ë¦¬ë¥¼ ìœ„í•´ì„œ ìš°ì„  ì‘ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.
model_name = "unsloth/gemma-3-1b-it-unsloth-bnb-4bit"

model, tokenizer = FastModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length, # Choose any for long context!
)

"""# 2. LoRA ì„¤ì •

- LoRAì˜ ê°œë…ê³¼ ê·¸ì— ë”°ë¥¸ ì„¤ì •ê°’ë“¤ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤.

### **LoRAì˜ ì›ë¦¬**
- LoRAì™€ ê´€ë ¨í•´ì„œëŠ” ì•„ë˜ ì´ë¯¸ì§€ê°€ ì•„ì£¼ ìœ ëª…í•©ë‹ˆë‹¤.
  
<img src = "https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/LoRA_Adaptation.png" />
    
  ê¸°ì¡´ì— ëª¨ë¸ì„ êµ¬ì„±í•˜ê³  ìˆë˜ ê³„ì¸µë“¤ê³¼ëŠ” ë³„ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°œë… ì¤‘ í•˜ë‚˜ê°€ Adapterì…ë‹ˆë‹¤.
    
  ìƒ‰ìƒìœ¼ë¡œë„ ë‚˜ë¦„ í‘œì‹œë¥¼ í•´ë‘” ê²ƒì´, ê¸°ì¡´ ëª¨ë¸ì˜ Pretrained WeightsëŠ” í•™ìŠµì„ í•˜ì§€ ì•Šê³  (Freezeâ„ï¸), Adapterì— í•´ë‹¹ë˜ëŠ” ëª¨ë“ˆë§Œ í•™ìŠµì„ í•©ë‹ˆë‹¤. (TrainableğŸ”¥)
    
- ëª¨ë¸ì€ ì…ë ¥(í…ìŠ¤íŠ¸)ì„ ë°›ìœ¼ë©´ ì´ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•´ ìˆ˜ë§ì€ layerë“¤ì„ í†µê³¼í•˜ë©´ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜(output)í•©ë‹ˆë‹¤.
    
<img src="https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/LoRA.png" />
    
  ì´ë•Œ ê²°ê³¼ë¬¼ê³¼ ì‹¤ì œ ì •ë‹µ(label) ê°„ì˜ ì˜¤ì°¨ ì •ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ë©° íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ”ë°ìš”, LoRAì—ì„œëŠ” ì—…ë°ì´íŠ¸ë¥¼ ê·¸ë¦¼ ì¤‘ â€˜ìš°ì¸¡ğŸ”¥â€™ì—ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
  ì¦‰, forward ì—°ì‚°ì€ pretrained weights ë”°ë¡œ, adapter ë”°ë¡œ ìˆ˜í–‰í•˜ê³ ìš” ê°ê°ì˜ ê²°ê³¼ë¥¼ í•©ì³ ë‹¤ìŒ ë ˆì´ì–´ì— ë³´ë‚´ëŠ”ë°, backwardì—ì„œ gradient updateëŠ” adapterğŸ”¥ì—ë§Œ ì ìš©í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
    
- **ì´ë ‡ê²Œ í•˜ë©´ ì¥ì ì´ ë­”ê°€ìš”?**
    
    LLMì„ í•™ìŠµí•  ë•Œ ëŒ€ë¶€ë¶„ì˜ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì„ ì°¨ì§€í•˜ëŠ” ê²ƒì€ ì—­ì „íŒŒ ê³¼ì •ì…ë‹ˆë‹¤.
    
    ì—­ì „íŒŒë¥¼ ìœ„í•´ì„œëŠ” forwardì— ì‚¬ìš©ëœ QKV ê°’ë“¤ì„ ì €ì¥í•´ë‘ì–´ì•¼ í•˜ê³  ì´ê²ƒì´ ë©”ëª¨ë¦¬ì— ë‹´ê²¨ ìˆê¸° ë•Œë¬¸ì´ì£ .
    
    ë˜í•œ ì—­ì „íŒŒ ìì²´ë„ ê¸°ì¡´ì˜ ìˆ˜ì¹˜ë¯¸ë¶„ ë°©ì‹ì— ë¹„í•˜ë©´ ì—„ì²­ë‚˜ê²Œ íš¨ìœ¨ì ì´ì§€ë§Œ ìˆ˜ì–µ ~ ìˆ˜ë°±ì–µê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•´ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œëŠ” ë³‘ëª©ì´ ë  ìˆ˜ë°–ì— ì—†ìŠµë‹ˆë‹¤.
    
    ê·¸ëŸ°ë° pretrained weightsâ„ï¸Â ëŠ” ì—­ì „íŒŒê°€ í•„ìš”í•˜ì§€ ì•Šê³ , adapterğŸ”¥Â ëŠ” íŒŒë¼ë¯¸í„°ìˆ˜ê°€ êµ‰ì¥íˆ ì ìŠµë‹ˆë‹¤.
    
    ì‹¤ì œë¡œ LoRA ì„¸íŒ… ì´í›„ trainable parametersë¥¼ ì¶œë ¥í•´ë³´ë©´ ê¸°ì¡´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ 1% ë‚´ì™¸ë¥¼ ì°¨ì§€í•˜ê²Œ ë©ë‹ˆë‹¤.
    
    **â†’ ë”°ë¼ì„œ í•™ìŠµì— í•„ìš”í•œ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
    
- **ê·¸ëŸ°ë° ê²°êµ­ pretrained weights â„ï¸, adapter ğŸ”¥Â ê°€ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤ë©´, forwardë§Œ í•˜ëŠ” ì¶”ë¡  ì‹œì—ëŠ” ë¶ˆë¦¬í•œ ê²ƒ ì•„ë‹Œê°€ìš”?**
    
    ì—„ì²­ë‚˜ê²Œ ë‚ ì¹´ë¡­ê³  ë›°ì–´ë‚œ ì§€ì ì…ë‹ˆë‹¤.
    
    ë§ìŠµë‹ˆë‹¤. í•™ìŠµì´ ëë‚œ ì´í›„ ì¶”ë¡ í•˜ëŠ” ìƒí™©ì„ ìƒê°í•´ ë³¸ë‹¤ë©´ ë‘ ê°ˆë˜ë¡œ forward í•˜ëŠ” ê²ƒì€ ë” ë§ì€ ì—°ì‚°ëŸ‰ì„ ìš”êµ¬í•˜ê²Œ ë©ë‹ˆë‹¤.
    
    ê·¸ë˜ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´..
    
    **â€œí•™ìŠµì´ ëë‚œ adapterğŸ”¥ì˜ ê°€ì¤‘ì¹˜ë¥¼ pretrained weightsâ„ï¸ì— ë”í•©ë‹ˆë‹¤(merge)â€**
    
    adapterëŠ” $A \in \mathbb{R}^{r \times d}, B \in \mathbb{R}^{d \times r}$  ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    ë”°ë¼ì„œ $BA \in \mathbb{R}^{d \times d}$ ê°€ ë˜ë¯€ë¡œ $W \in \mathbb{R}^{d \times d}$ ì™€ ì°¨ì›ì´ ë™ì¼í•˜ì—¬ í•©ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    ì–´ë–»ê²Œ ì´ê±¸ ê·¸ëƒ¥ í•©í•´ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëƒ.. ì‹¶ì§€ë§Œì„œë„ Microsoft í˜•ë‹˜ë“¤ì˜ ë§ì´ë‹ˆ ê·¸ëƒ¥ ë”°ë¼ë„ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

### **LoRA êµ¬í˜„**
- ì°¸ê³  ê°€ëŠ¥í•œ ë§í¬ ğŸ”—: https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#lora-examples, https://huggingface.co/docs/peft/en/index
- HuggingFaceì˜ Trainer í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ ë‘ê³ , ì¶”ê°€ë¡œ ì‘ì„±í•´ì•¼ í•˜ëŠ” ì½”ë“œë§Œ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤. (ìœ„ ë§í¬ì˜ ë‚´ìš©ê³¼ ì‚´ì§ ë‹¤ë¦…ë‹ˆë‹¤)
- LoRAë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” HuggingFaceì—ì„œ ì§€ì›í•˜ëŠ” PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.
    
    Parameter Efficient Fine-Tuning ì˜ ì•½ìë¡œ LoRA, Quantization, Prefix tuning ë“± ë‹¤ì–‘í•œ í•™ìŠµ ë°©ì‹ë“¤ì„ ì§€ì›í•©ë‹ˆë‹¤.
    
- ê¸°ì¡´ í•™ìŠµ/ì¶”ë¡  ë¡œì§
    
    â€œë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° â†’ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¡œ ë§Œë“¤ê¸° â†’ ëª¨ë¸, í•™ìŠµ args ë¶ˆëŸ¬ì˜¤ê¸° â†’ Trainerë¡œ â€˜ë°ì´í„°, ëª¨ë¸, argsâ€™ ê°ì‹¸ê¸° â†’ `Trainer.train()`â€
    
- LoRA ì¶”ê°€ ì‹œ í•™ìŠµ/ì¶”ë¡  ë¡œì§
    
    â€œë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° â†’ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¡œ ë§Œë“¤ê¸° â†’ ëª¨ë¸, í•™ìŠµ args ë¶ˆëŸ¬ì˜¤ê¸° â†’ **PEFT(LoRA) config** ì„¤ì •í•˜ê¸° â†’ Trainerë¡œ â€˜ë°ì´í„°, ëª¨ë¸, args, **PEFT(LoRA) config**â€™ ê°ì‹¸ê¸° â†’ `Trainer.train()`â€
    
- í•™ìŠµ ì½”ë“œ ì˜ˆì‹œ
    
```python
from peft import LoraConfig

...

training_args = TrainingArguments(
        ...
)

peft_config = LoraConfig(
    r=args.lora_r,
    lora_alpha=args.lora_alpha,
    lora_dropout=args.lora_dropout,
    bias=args.lora_bias,
    task_type=args.task_type,
    target_modules=args.target_modules
)

trainer = MyTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    peft_config=peft_config if args.peft else None,
)

trainer.train()
trainer.save_model(args.output_dir)

```
    
- ì¶”ë¡  ì‹œ merge ì½”ë“œ
    
```python
if peft_config is not None:
    model = trainer.model.merge_and_unload()

with torch.no_grad():
        ...

```

### **arguments ê°„ë‹¨íˆ ì•Œì•„ë³´ê¸°**
    
ë‹¹ì—°íˆ ì—¬ëŸ¬ ìš”ì†Œë“¤ì´ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ë§Œ.. ì˜í–¥ë ¥ì´ ê°€ì¥ í° ì„¸ ê°œì˜ argumentë§Œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì¢€ë” ìì„¸í•œ ì„¤ëª…ì€ [LoRA Hyperparameters Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#hyperparameters-and-recommendations)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”

- `r`: $A \in \mathbb{R}^{r \times d}, B \in \mathbb{R}^{d \times r}$ë¥¼ êµ¬ì„±í•˜ëŠ” ì°¨ì› rì„ ê²°ì •í•©ë‹ˆë‹¤. ë³´í†µ 8, 16, 32, 64 ì¤‘ í•˜ë‚˜ì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.
- `lora_alpha`: adapterì˜ weightë¥¼ scaling í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. weight ì „ì²´ì— $\alpha/r$ ë¥¼ ê³±í•©ë‹ˆë‹¤. ì¦‰, $\alpha$ì˜ ê°’ì´ í´ìˆ˜ë¡ adapterì˜ ì˜í–¥ë ¥ì´ ì»¤ì§€ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `target_modules`: ì–´ë–¤ layerì— adapterë¥¼ ì¶”ê°€í•  ê²ƒì¸ì§€ ê²°ì •í•©ë‹ˆë‹¤. ì»¤ìŠ¤í…€ ì—¬ì§€ê°€ ë§ì€ ë¶€ë¶„ì¸ë° ë³´í†µì€ `Linear` ë ˆì´ì–´ì— LoRAë¥¼ ì ìš©í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê²€ìƒ‰í•´ë³´ì‹œê¸¸ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.
    - Attention: `q_proj`, `k_proj`, `v_proj`, `o_proj`
    - MLP: `gate_proj`, `up_proj`, `down_proj`
    - PEFT documentation ì¼ë¶€ ë‚´ìš©
        - **target_modules**Â (`Optional[Union[List[str], str]]`) â€” The names of the modules to apply the adapter to. If this is specified, only the modules with the specified names will be replaced. When passing a string, a regex match will be performed. When passing a list of strings, either an exact match will be performed or it is checked if the name of the module ends with any of the passed strings. If this is specified as â€˜all-linearâ€™, then all linear/Conv1D modules are chosen, excluding the output layer. If this is not specified, modules will be chosen according to the model architecture. If the architecture is not known, an error will be raised â€” in this case, you should specify the target modules manually.
"""

r = 8 # ì–´ë–¤ ìˆ«ìë¥¼ ì‚¬ìš©í•´ë„ ìƒê´€ì—†ì§€ë§Œ, 8, 16, 32, 64, 128 ë‹¨ìœ„ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤!
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_alpha = 16 # r * 2 = alphaê°€ ë˜ë„ë¡ ê°’ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.
lora_dropout = 0.0 # ì–´ë–¤ ê°’ì„ ì„¤ì •í•´ë„ ìƒê´€ì—†ì§€ë§Œ 0ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.
bias = "none" # noneìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.

model = FastModel.get_peft_model(
    model,
    r = r,
    target_modules = target_modules,
    lora_alpha = lora_alpha,
    lora_dropout = lora_dropout,
    bias = bias
)

"""# 3. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì „ì²˜ë¦¬
ìš°ë¦¬ëŠ” ì´ì œ chat fine-tuningì„ ìœ„í•´ Gemma-3 í¬ë§·ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” [Thytu's ChessInstruct](https://huggingface.co/datasets/Thytu/ChessInstruct) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
Gemma-3ëŠ” ë©€í‹° í„´ ëŒ€í™”ë¥¼ ì•„ë˜ì™€ ê°™ì´ ë Œë”ë§í•©ë‹ˆë‹¤:

```
<bos><start_of_turn>user
Hello!<end_of_turn>
<start_of_turn>model
Hey there!<end_of_turn>
```

UnslothëŠ” `get_chat_template` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¬ë°”ë¥¸ ì±„íŒ… í…œí”Œë¦¿ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
í˜„ì¬ zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3 ë“± ë‹¤ì–‘í•œ í…œí”Œë¦¿ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "gemma3",
)

"""`reasoning_effort`ì„ `medium`ìœ¼ë¡œ ë³€ê²½í•˜ë©´ ëª¨ë¸ì´ ë” ì˜¤ë˜ ìƒê°í•˜ê²Œ ë©ë‹ˆë‹¤.

ë”°ë¼ì„œ ìƒì„±ë˜ëŠ” í† í° ìˆ˜ë¥¼ ê°ë‹¹í•  ìˆ˜ ìˆë„ë¡ `max_new_tokens` ê°’ì„ ëŠ˜ë ¤ì•¼ í•˜ì§€ë§Œ, ê·¸ ê²°ê³¼ ë” ë‚˜ì€ ë‹µë³€ê³¼ ë” ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from datasets import load_dataset
dataset = load_dataset("Thytu/ChessInstruct", split = "train[:10000]")

# ë°©ë²• 1: ì²« ë²ˆì§¸ ì˜ˆì‹œ ì¶œë ¥
print(dataset[0])

# ë°©ë²• 2: ì»¬ëŸ¼ëª… í™•ì¸
print(dataset.column_names)

# ë°©ë²• 3: features í™•ì¸
print(dataset.features)

"""ì´ì œ fine-tuningìš© ë°ì´í„°ì…‹ì„ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ `convert_to_chatml`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!"""

def convert_to_chatml(example):
    # TODO: ì•„ë˜ conversationsì— ë“¤ì–´ê°ˆ ë‚´ìš©ì„ ì±„ì›Œì£¼ì„¸ìš”.
    '''
    example["instruction"]: ë°ì´í„°ì…‹ì˜ instruction í•„ë“œ(ì‚¬ìš©ìì˜ ì§ˆë¬¸/ëª…ë ¹)ë¥¼ ê°€ì ¸ì™€ "user" ì—­í• ë¡œ ì„¤ì •
    example["response"]: ë°ì´í„°ì…‹ì˜ response í•„ë“œ(ëª¨ë¸ì˜ ì‘ë‹µ)ë¥¼ ê°€ì ¸ì™€ "assistant" ì—­í• ë¡œ ì„¤ì •
    ë°˜í™˜ í˜•ì‹: conversations í‚¤ì— ì—­í• (role)ê³¼ ë‚´ìš©(content)ì„ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í• ë‹¹
    '''
    return {
        "conversations": [
            {"role": "user", "content": example["task"] + "\n\n" + example["input"]},
            {"role": "assistant", "content": example["expected_output"]}
        ]
    }


dataset = dataset.map(
    convert_to_chatml
)

dataset[100]

"""ì´ì œ chatì— Gemma3 chat templateì„ ì ìš©í•˜ê³ , textë¡œ ì €ì¥í•©ë‹ˆë‹¤."""

def formatting_prompts_func(examples):
   convos = examples["conversations"]
   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]
   return { "text" : texts, }

dataset = dataset.map(formatting_prompts_func, batched = True)

dataset[100]['text']

"""# 4. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
- unslothë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
- ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í•™ìŠµí•œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.

ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë´…ì‹œë‹¤. ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ `100` ìŠ¤í…ë§Œ ì‹¤í–‰í•˜ì§€ë§Œ, ì „ì²´ í•™ìŠµì„ ì›í•œë‹¤ë©´ `num_train_epochs=1`ë¡œ ì„¤ì •í•˜ê³  `max_steps=None`ìœ¼ë¡œ ë¹„í™œì„±í™”í•˜ë©´ ë©ë‹ˆë‹¤.
"""

from trl import SFTConfig, SFTTrainer

dataset_text_field = "text" # datasetì—ì„œ ì „ì²˜ë¦¬í•œ ì—´ ì´ë¦„
per_device_train_batch_size = 8 # GPUê°€ ë¶€ì¡±í•˜ë‹¤ë©´ í•´ë‹¹ ê°’ì„ ì¤„ì—¬ì£¼ì„¸ìš”.
gradient_accumulation_steps = 1
warmup_steps = 5
max_steps = 100 # í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ë©´ í•´ë‹¹ ê°’ì„ ì¤„ì—¬ì£¼ì„¸ìš”.
learning_rate = 5e-5
logging_steps = 1
optim = "adamw_8bit"
weight_decay = 0.01
lr_scheduler_type = "linear"
output_dir = "outputs"
report_to = "none" # Use this for WandB etc

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    args = SFTConfig(
        dataset_text_field=dataset_text_field,
        per_device_train_batch_size = per_device_train_batch_size,
        gradient_accumulation_steps = gradient_accumulation_steps,
        warmup_steps = warmup_steps,
        max_steps = max_steps,
        learning_rate = learning_rate,
        logging_steps = logging_steps,
        optim = optim,
        weight_decay = weight_decay,
        lr_scheduler_type = lr_scheduler_type,
        output_dir = output_dir,
        report_to = report_to,
    ),
)

"""ìš°ë¦¬ëŠ” Unslothì˜ `train_on_completions` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ User inputì— ëŒ€í•œ lossëŠ” ë¬´ì‹œí•˜ê³  ì˜¤ì§ assistant(ì •ë‹µ ë¼ë²¨) outputë§Œ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.

ì´ ë°©ì‹ì€ íŒŒì¸íŠœë‹ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤!
"""

from unsloth.chat_templates import train_on_responses_only
# TODO: instruction partì™€ response partë¥¼ chat templateì„ ì°¸ê³ í•˜ì—¬ ì±„ì›Œë„£ì–´ì£¼ì„¸ìš”.
'''
Gemma3 chat templateì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ì‚¬ìš©

<start_of_turn>user\n + ì‚¬ìš©ì ë©”ì‹œì§€ + <end_of_turn>\n
<start_of_turn>model\n + ëª¨ë¸ ì‘ë‹µ + <end_of_turn>\n


instruction_part: ì‚¬ìš©ìì˜ ì§ˆë¬¸/ì§€ì‹œê°€ ì‹œì‘ë˜ëŠ” ë¶€ë¶„ì˜ í† í°
response_part: ëª¨ë¸ì˜ ì‘ë‹µì´ ì‹œì‘ë˜ëŠ” ë¶€ë¶„ì˜ í† í°

train_on_responses_only í•¨ìˆ˜ëŠ” ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬:

instruction ë¶€ë¶„(user turn)ì—ì„œëŠ” lossë¥¼ ê³„ì‚°í•˜ì§€ ì•Šê³ 
response ë¶€ë¶„(model turn)ì—ì„œë§Œ lossë¥¼ ê³„ì‚°í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì„

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ì‘ë‹µ ìƒì„±ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ í•™ìŠµ
'''
instruction_part = "<start_of_turn>user\n"
response_part = "<start_of_turn>model\n"

trainer = train_on_responses_only(
    trainer,
    instruction_part = instruction_part,
    response_part = response_part,
)

"""ì§€ì‹œë¬¸ ë¶€ë¶„ì´ ë§ˆìŠ¤í‚¹ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤! 100ë²ˆì§¸ í–‰ì„ ë‹¤ì‹œ ì¶œë ¥í•´ ë³´ê² ìŠµë‹ˆë‹¤."""

tokenizer.decode(trainer.train_dataset[100]["input_ids"])

"""ì´ì œ ë§ˆìŠ¤í‚¹ëœ ì˜ˆì‹œë¥¼ ì¶œë ¥í•´ ë´…ì‹œë‹¤ â€” ë‹µë³€ë§Œ ë‚¨ì•„ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤:"""

tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100]["labels"]]).replace(tokenizer.pad_token, " ")

"""í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

"""í•™ìŠµì´ ì™„ë£Œëœ ìƒíƒœì—ì„œì˜ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""

used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### ì¶”ë¡ 

ëª¨ë¸ì„ ì‹¤í–‰í•´ ë´…ì‹œë‹¤!

ì§€ì‹œë¬¸ê³¼ ì…ë ¥ì€ ë³€ê²½í•  ìˆ˜ ìˆìœ¼ë©°, ì¶œë ¥ì€ ë¹„ì›Œ ë‘ì„¸ìš”!

ì¶”ì²œí•˜ëŠ” ì„¤ì •ì…ë‹ˆë‹¤.
`temperature = 1.0, top_p = 0.95, top_k = 64`
"""

messages = [
    {'role': 'system','content':dataset['conversations'][10][0]['content']},
    {"role" : 'user', 'content' : dataset['conversations'][10][1]['content']}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt = True, # Must add for generation
).removeprefix('<bos>')

from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text, return_tensors = "pt").to("cuda"),
    max_new_tokens = 125,
    temperature = 1, top_p = 0.95, top_k = 64,
    streamer = TextStreamer(tokenizer, skip_prompt = True),
)

"""# 5. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
- íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì¶”ë¡ ì„ í•©ë‹ˆë‹¤.
- ì¶”ë¡ í•œ ê²°ê³¼ë¬¼ì„ í™•ì¸í•©ë‹ˆë‹¤.

íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°

ìµœì¢… ëª¨ë¸ì„ LoRA ì–´ëŒ‘í„°ë¡œ ì €ì¥í•˜ë ¤ë©´, Huggingfaceì˜ `push_to_hub`ì„ ì‚¬ìš©í•´ ì˜¨ë¼ì¸ì— ì €ì¥í•˜ê±°ë‚˜ `save_pretrained`ë¥¼ ì‚¬ìš©í•´ ë¡œì»¬ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[ì°¸ê³ ] ì´ëŠ” ì „ì²´ ëª¨ë¸ì´ ì•„ë‹ˆë¼ LoRA adapterë§Œ ì €ì¥í•©ë‹ˆë‹¤.
16bit ë˜ëŠ” GGUF í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ë ¤ë©´ ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì„¸ìš”!
"""

model.save_pretrained("gemma-3")  # Local saving
tokenizer.save_pretrained("gemma-3")
# model.push_to_hub("your_name/gemma-3", token = "...") # Online saving
# tokenizer.push_to_hub("your_name/gemma-3", token = "...") # Online saving

"""float16 í˜•ì‹ìœ¼ë¡œ ì§ì ‘ ì €ì¥í•˜ëŠ” ê²ƒë„ ì§€ì›í•©ë‹ˆë‹¤.
- float16 â†’ merged_16bit ì„ íƒ
- int4 â†’ merged_4bit ì„ íƒ

Merged ë°©ì‹
- merged ì˜µì…˜ì€ LoRA ëª¨ë“ˆì„ base modelì— ë³‘í•©í•˜ì—¬ ì €ì¥í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
- ìœ„ì˜ ë°©ì‹ì²˜ëŸ¼ LoRA adapterë§Œ ì €ì¥í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì›ë³¸ ëª¨ë¸ë„ ì €ì¥í•˜ë¯€ë¡œ ì €ì¥ ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤.
"""

# Merge to 16bit
if False:
    model.save_pretrained_merged("gemma-3-finetune", tokenizer, save_method = "merged_16bit")
if False: # Pushing to HF Hub
    model.push_to_hub_merged("hf/gemma-3-finetune", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False:
    model.save_pretrained_merged("gemma-3-finetune", tokenizer, save_method = "merged_4bit",)
if False: # Pushing to HF Hub
    model.push_to_hub_merged("hf/gemma-3-finetune", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False:
    model.save_pretrained("gemma-3-finetune")
    tokenizer.save_pretrained("gemma-3-finetune")
if False: # Pushing to HF Hub
    model.push_to_hub("hf/gemma-3-finetune", token = "")
    tokenizer.push_to_hub("hf/gemma-3-finetune", token = "")

"""ì•„ë˜ ì½”ë“œë¥¼ í†µí•´ í•™ìŠµí•œ ëª¨ë¸ì„ ëŒë ¤ë³´ì„¸ìš”!

`ëŸ°íƒ€ì„` -> `ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘(Ctrl+M)`ì„ í´ë¦­í•˜ì—¬ ì„¸ì…˜ì„ ë‹¤ì‹œ ì‹œì‘í•œ í›„ì— ì½”ë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”!
"""

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "gemma-3", # YOUR MODEL YOU USED FOR TRAINING
    max_seq_length = 2048,
    load_in_4bit = False,
)

import datasets

messages = [
    {'role': 'system','content' : "Given an incomplit set of chess moves and the game's final score, write the last missing chess move.\n\nInput Format: A comma-separated list of chess moves followed by the game score.\nOutput Format: The missing chess move"},
    {"role" : 'user', 'content' : '{"moves": ["e2e4", "c7c5", "g1f3", "e7e6", "d2d4", "c5d4", "f3d4", "g8f6", "b1c3", "f8b4", "e4e5", "f6e4", "d1g4", "e4c3", "g4g7", "h8f8", "a2a3", "c3b5", "a3b4", "b5d4", "c1g5", "d8b6", "g5h6", "d4c2", "e1d1", "b6b4", "d1c2", "b8c6", "f1e2", "d7d5", "g7f8", "b4f8", "h6f8", "e8f8", "f2f4", "a7a5", "a1a3", "c8d7", "h1d1", "c6e7", "c2d2", "a5a4", "d1c1", "d7c6", "g2g3", "a8b8", "c1c5", "b7b5", "b2b4", "a4b3", "a3b3", "b8a8", "b3b2", "a8a3", "b2c2", "c6e8", "c5c3", "a3a4", "c3c7", "a4a1", "c7b7", "a1h1", "b7b8", "b5b4", "e2b5", "b4b3", "c2c1", "h1h2", "d2c3", "e7f5", "b5e8", "b3b2", "e8a4", "f8g7", "b8b2", "h2h3", "b2b7", "h3g3", "c3b2", "g3g2", "b2b1", "f5d4", "a4e8", "d4e2", "e8f7", "e2c1", "f7e6", "g7f8", "b1c1", "g2e2", "b7f7", "f8e8", "e6d5", "h7h6", "e5e6", "e8d8", "c1d1", "e2b2", "d5c6", "b2b1", "d1d2", "b1b2", "d2e3", "b2b3", "e3e4", "b3b4", "e4e5", "b4b1", "e6e7", "d8c7", "e7e8q", "c7b6", "e8b8", "b6c5", "b8b1", "c5c4", "b1c2", "c4b4", "f7b7", "b4a3", "?"], "result": "1-0"}'}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt = True, # Must add for generation
).removeprefix('<bos>')

from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text, return_tensors = "pt").to("cuda"),
    max_new_tokens = 125,
    temperature = 1, top_p = 0.95, top_k = 64,
    streamer = TextStreamer(tokenizer, skip_prompt = True),
)