# -*- coding: utf-8 -*-
"""(ê³¼ì œ-ë¬¸ì œ) 1-2_MLP êµ¬í˜„.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tkfe_7RKdCMrlwJa_kpEBfzkiyrNTf4F

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**

1. **ê³¼ì œëª…**: MLP êµ¬í˜„

2. **í•µì‹¬ ì£¼ì œ**
  - `scikit-learn Pipeline` ë° `GridSearchCV`ë¥¼ ì´ìš©í•œ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•  
  - PyTorch Lightning DataModuleì„ í™œìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”© ê´€ë¦¬  
  - LightningModuleì„ í†µí•œ MLP ë„¤íŠ¸ì›Œí¬ êµ¬í˜„ ë° í•™ìŠµ/í‰ê°€ ìë™í™”

3. **í•™ìŠµ ëª©í‘œ**
  - `sklearn Pipeline`ì„ ì„¤ê³„í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í•™ìŠµê¹Œì§€ ì¼ê´„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.  
  - `GridSearchCV`ë¥¼ í™œìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.  
  - PyTorch Lightning DataModuleì„ ì •ì˜í•˜ì—¬ ë°ì´í„° ë¡œë”© ê³¼ì •ì„ êµ¬ì¡°í™”í•  ìˆ˜ ìˆë‹¤.  
  - `LightningModule`ì„ êµ¬í˜„í•˜ì—¬ MLP ëª¨ë¸ì˜ training/validation ë£¨í”„ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤.  
  - ì½œë°±ì„ ì´ìš©í•´ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° EarlyStopping ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ê³¼ì •ì„ ì œì–´í•  ìˆ˜ ìˆë‹¤.  

4. **í•™ìŠµ ê°œë…**
  - `Pipeline`: ì „ì²˜ë¦¬ì™€ í•™ìŠµ ë‹¨ê³„ë¥¼ ì—°ê²°í•´ ì¼ê´„ ì‹¤í–‰í•˜ëŠ” scikit-learn êµ¬ì„± ìš”ì†Œ  
  - `GridSearchCV`: êµì°¨ ê²€ì¦ì„ í†µí•´ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰í•˜ëŠ” ê¸°ë²•  
  - `DataModule`: PyTorch Lightningì—ì„œ ë°ì´í„° ì¤€ë¹„ì™€ DataLoader ê´€ë¦¬ë¥¼ ìº¡ìŠí™”í•˜ëŠ” ëª¨ë“ˆ  
  - `LightningModule`: ëª¨ë¸ ì •ì˜, í•™ìŠµ/ê²€ì¦ ìŠ¤í…, ì˜µí‹°ë§ˆì´ì € ì„¤ì • ë“±ì„ ì¶”ìƒí™”í•œ PyTorch Lightning ì»´í¬ë„ŒíŠ¸  
  - `MLP`: ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  êµ¬ì¡°ë¡œ êµ¬ì„±ëœ ì¸ê³µ ì‹ ê²½ë§  

5. **í•™ìŠµ ë°©í–¥**
  - scikit-learn Pipelineê³¼ GridSearchCVë¥¼ ì‹¤ìŠµí•´ ì „ì²˜ë¦¬Â·í•™ìŠµÂ·íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•œë‹¤.  
  - torchvisionì˜ FashionMNIST ë°ì´í„°ë¥¼ DataModuleë¡œ ìº¡ìŠí™”í•˜ì—¬ ë¡œë”©ê³¼ ì „ì²˜ë¦¬ë¥¼ ì¼ì›í™”í•œë‹¤.  
  - LightningModuleì„ í†µí•´ MLP ëª¨ë¸ êµ¬ì¡°ì™€ í•™ìŠµ/ê²€ì¦ ë£¨í”„ë¥¼ êµ¬í˜„í•˜ê³  Trainerë¡œ ì‹¤í–‰í•œë‹¤.  
  - ModelCheckpoint, EarlyStopping, WandbLogger ë“± ì½œë°±ê³¼ ë¡œê±°ë¥¼ ì ìš©í•´ ëª¨ë¸ ì €ì¥, í•™ìŠµ ì¤‘ë‹¨, ì‹¤í—˜ ê´€ë¦¬ë¥¼ ìˆ˜í–‰í•œë‹¤.  

6. **ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´**
  - ë°ì´í„°ì…‹ ëª…: FashionMNIST  
  - ë°ì´í„°ì…‹ ê°œìš”: 10ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬(í‹°ì…”ì¸ , ë°”ì§€ ë“±)ì˜ 28Ã—28 í‘ë°± ì´ë¯¸ì§€ í•™ìŠµ 60,000ì¥, í…ŒìŠ¤íŠ¸ 10,000ì¥ìœ¼ë¡œ êµ¬ì„±  
  - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ: Zalando SE ì†Œìœ , ì—°êµ¬ ë° êµìœ¡ ëª©ì ìœ¼ë¡œ ê³µê°œ. torchvision ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì œê³µ

### **Prerequisites**
```
numpy>=1.26
pandas>=2.0
scikit-learn>=1.4
seaborn>=0.12
torch>=2.2
matplotlib>=3.8
pytorch-lightning==2.5.3
```

# `scikit-learn`ì„ ì¶”ê°€ë¡œ ì‘ìš©í•´ë³´ê¸°
**í•™ìŠµ ëª©í‘œ**  
  - sklearn Pipelineì„ êµ¬ì¶•í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ì™€ ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ë¥¼ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•œë‹¤.  
  - `GridSearchCV`ë¥¼ í™œìš©í•´ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰í•˜ê³  ìµœì  ëª¨ë¸ì„ ì„ ì •í•œë‹¤.  
  - êµì°¨ ê²€ì¦ ê²°ê³¼ì™€ í‰ê°€ ì§€í‘œ(MSE, RÂ²)ë¥¼ í†µí•´ íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ì„ ê²€ì¦í•œë‹¤.  

**í•™ìŠµ ê°œë…**  
  - `Pipeline`: ì „ì²˜ë¦¬(`StandardScaler`)ì™€ ëª¨ë¸(`LinearRegression`, `Ridge`, `Lasso`, `MLPRegressor`)ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•´ ìë™ ì‹¤í–‰í•œë‹¤.  
  - `GridSearchCV`: íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ dict í˜•íƒœë¡œ ì •ì˜í•´ êµì°¨ ê²€ì¦ìœ¼ë¡œ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ”ë‹¤.  

**ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½**  
  1. seabornì˜ mpg ë°ì´í„°ì…‹ì„ ë¡œë“œí•´ ê²°ì¸¡ì¹˜ ì œê±°, ì›-í•« ì¸ì½”ë”©, íƒ€ê¹ƒ ë¶„ë¦¬, train/test ë¶„í• ì„ ìˆ˜í–‰í•œë‹¤.  
  2. `StandardScaler`+`LinearRegression` íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ê¸°ë³¸ íšŒê·€ ëª¨ë¸ì„ ì‹¤ìŠµí•˜ê³  ê²°ê³¼(MSE, RÂ²) í™•ì¸.  
  3. `Ridge`, `Lasso`, `MLPRegressor`ë¥¼ ê°ê° `Pipeline`ìœ¼ë¡œ ì •ì˜í•´ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•œë‹¤.  
  4. `GridSearchCV`ë¡œ ê° íŒŒì´í”„ë¼ì¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ í›„ ìµœì  ëª¨ë¸ì„ ì„ ì •í•˜ê³  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.

ì´ë²ˆ ê³¼ì œì—ì„œëŠ” ì´ë¡ ì‹œê°„ì— ë‹¤ë£¬ ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ì§ì ‘ `scikit-learn`ì—ì„œ ë¶ˆëŸ¬ì„œ í•™ìŠµí•´ë´…ì‹œë‹¤. ì—¬ëŸ¬ ì´ˆë§¤ê°œë³€ìˆ˜ë„ ë°”ê¾¸ê³  ëª¨ë¸ë„ ë°”ê¿”ë³´ë©´ì„œ ì–´ë–¤ ê²½ìš°ì— ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•´ë³´ëŠ” ê°„ë‹¨í•œ ì½”ë“œ ì‹¤ìŠµì„ í•´ë´…ì‹œë‹¤.

ì˜¤ëŠ˜ì€ `sklearn.pipeline.Pipeline`ì„ ì´ìš©í•´ì„œ ë°ì´í„° ì²˜ë¦¬ê³¼ì •ì„ í•˜ë‚˜ì˜ íŒŒì´í”„ë¡œ ë„£ì–´ì„œ ê°„í¸í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ê³¼ optunaê°€ ì•„ë‹Œ `sklearn.model_selection.GridSearchCV`ë¥¼ í†µí•´ ì´ˆë§¤ê°œë³€ìˆ˜ë¥¼ íƒìƒ‰í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œë„ ë°°ì›Œë´…ë‹ˆë‹¤. íšŒê·€ì™€ ë¶„ë¥˜ê³¼ì œ ê°ê° ì§„í–‰í•´ë´…ì‹œë‹¤.

### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: íšŒê·€ ê³¼ì œ ë‹¤ì‹œ ìˆ˜í–‰í•´ë³´ê¸°

`mpg` ë°ì´í„°ì…‹ì„ ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸ì„ í†µí•´ í•™ìŠµí•´ë´…ì‹œë‹¤. ìš°ì„  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•´ì¤ì‹œë‹¤.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import torch

RANDOM_STATE = 42
df = sns.load_dataset('mpg').dropna()

print("ì›ë³¸ ë°ì´í„° shape:", df.shape)
print("\nì»¬ëŸ¼ ëª©ë¡:")
print(df.columns.tolist())

print(df.head())

# ì•„ë˜ëŠ” ë‚œìˆ˜ë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
# ì¬í˜„ì„±ì„ ìœ„í•´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
import random
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(seed=RANDOM_STATE)

# TODO
# 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
# ì§€ë‚œ ì‹¤ìŠµì—ì„œ `name` columnì€ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. ì´ ì  ìœ ì˜í•´ì„œ X, y ìƒì„±í•´ì£¼ì„¸ìš”
# ìƒì„±í•œ X, y ê¸°ë°˜ìœ¼ë¡œ X_train, X_test, y_train, y_test ìƒì„±í•´ì£¼ì„¸ìš”.
# í…ŒìŠ¤íŠ¸ë°ì´í„°ëŠ” ì „ì²´ í•™ìŠµë°ì´í„°ì˜ 20%ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.
from sklearn.model_selection import train_test_split
X = df.drop(columns=['mpg', 'name'])  # mpgëŠ” íƒ€ê²Ÿ, nameì€ ì‚¬ìš© ì•ˆ í•¨
y = df['mpg']

print(f"\nX shape: {X.shape}")
print(f"y shape: {y.shape}")
print(f"\nX ì»¬ëŸ¼ë“¤: {X.columns.tolist()}")
print(X.dtypes)

# originì€ ë²”ì£¼í˜•ì´ë¯€ë¡œ ì›-í•« ì¸ì½”ë”©
X = pd.get_dummies(X, columns=['origin'], drop_first=False)

# train/test ë¶„í•  (80% / 20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=RANDOM_STATE
)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""ğŸªˆ `sklearn.pipeline.Pipeline`

ì•„ë˜ëŠ” ê³µì‹ API Referenceì—ì„œ ëª…ì‹œëœ Pipelineì˜ ì˜ˆì‹œì½”ë“œì…ë‹ˆë‹¤.
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
X, y = make_classification(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    random_state=0)
pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
# The pipeline can be used as any other estimator
# and avoids leaking the test set into the train set
pipe.fit(X_train, y_train).score(X_test, y_test)
# An estimator's parameter can be set using '__' syntax
pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)
```

ì˜ ì½ì–´ë³´ë©´ ë‘ ê°€ì§€ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ëˆ„ìˆ˜ë¥¼ ì•Œì•„ì„œ ë°©ì§€í•´ì¤ë‹ˆë‹¤.
- ëª¨ë¸ë„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ë¯¸ ì„ ì–¸ëœ íŒŒì´í”„ë¼ì¸ ë‚´ì—ì„œ ì†ì„±ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•´ë‹¹ ë‚´ìš©ì„ í†µí•´ ì•„ë˜ ê³¼ì œë¥¼ ìˆ˜í–‰í•´ë´…ì‹œë‹¤.
"""

# TODO
# 2. StandardScalerì™€ LinearRegressionì„ Pipelineìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# Pipeline ìƒì„±: ì „ì²˜ë¦¬(StandardScaler) â†’ ëª¨ë¸(LinearRegression)
pipeline = Pipeline([
    ('scaler', StandardScaler()),      # 1ë‹¨ê³„: í‘œì¤€í™”
    ('regressor', LinearRegression())  # 2ë‹¨ê³„: ì„ í˜• íšŒê·€
])

# Pipeline êµ¬ì¡° í™•ì¸
print("=== Pipeline êµ¬ì¡° ===")
print(pipeline)
print()

# 3. í•™ìŠµë°ì´í„°ì— ëŒ€í•´ í•™ìŠµì„ ì§„í–‰í•´ì£¼ê³ , íšŒê·€ì„±ëŠ¥ì„ í‰ê°€í•´ì£¼ì„¸ìš”
from sklearn.metrics import mean_squared_error, r2_score

# í•™ìŠµ (fit ì‹œ ìë™ìœ¼ë¡œ scaler.fit_transform â†’ regressor.fit ìˆœì„œë¡œ ì‹¤í–‰)
pipeline.fit(X_train, y_train)

# ì˜ˆì¸¡ (predict ì‹œ ìë™ìœ¼ë¡œ scaler.transform â†’ regressor.predict ìˆœì„œë¡œ ì‹¤í–‰)
y_pred = pipeline.predict(X_test)

# ì„±ëŠ¥ í‰ê°€
mse = mean_squared_error(y_true=y_test, y_pred=y_pred)
r2 = r2_score(y_true=y_test, y_pred=y_pred)

print(f"MSE: {mean_squared_error(y_true=y_test, y_pred=y_pred):.4f}")
print(f"R2: {r2_score(y_true=y_test, y_pred=y_pred):.4f}")

"""ì„±ê³µì ìœ¼ë¡œ `Pipeline`ìœ¼ë¡œ ë§ì€ ê³¼ì •ì„ ì†ì‰½ê²Œ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤!

ğŸ” `sklearn.model_selection.GridSearchCV`

ì•„ë˜ëŠ” ê³µì‹ API Referenceì—ì„œ ëª…ì‹œëœ GridSearchCVì˜ ì˜ˆì‹œì½”ë“œì…ë‹ˆë‹¤.
```python
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svc = svm.SVC()
clf = GridSearchCV(svc, parameters)
clf.fit(iris.data, iris.target)
sorted(clf.cv_results_.keys())
```

ì‚¬ìš©ë°©ë²•ì€ `GridSearchCV`ì— estimatorì™€ íƒìƒ‰ëŒ€ìƒì¸ parametersë¥¼ dict í˜•íƒœë¡œ ì œê³µí•©ë‹ˆë‹¤. `scoring`ì„ ë„£ì–´ì£¼ë©´ í•´ë‹¹ ì§€í‘œì— ë”°ë¼ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì—¬ ì´ˆë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” Pipelineê³¼ GridSearchCVë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ íšŒê·€ê³¼ì œë¥¼ ì œì¼ ì˜ ìˆ˜í–‰í•˜ëŠ” ì²œí•˜ì œì¼íšŒê·€ëª¨ë¸ì„ ì°¾ì•„ì¤ì‹œë‹¤.
"""

# 3. ì—¬ëŸ¬ ëª¨ë¸ì˜ Pipelinesë¥¼ ì •ì˜í•´ì¤ì‹œë‹¤.
# dict í˜•íƒœë¡œ ì œê°€ keyë§Œ ë„£ì–´ë‘ì—ˆìŠµë‹ˆë‹¤. valueë“¤ì€ ëª¨ë‘ Pipelineìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
from sklearn.linear_model import Ridge, Lasso
from sklearn.neural_network import MLPRegressor

pipelines = {
    "Ridge": Pipeline([
        ('scaler', StandardScaler()),
        ('model', Ridge())
    ]),
    "Lasso": Pipeline([
        ('scaler', StandardScaler()),
        ('model', Lasso())
    ]),
    "MLPRegressor": Pipeline([
        ('scaler', StandardScaler()),
        ('model', MLPRegressor(max_iter=1000, random_state=RANDOM_STATE))
    ]),
}

# 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
# ìœ„ì— `pipelines`ì— ìˆëŠ” ëª¨ë¸ë“¤ ë³„ë¡œ hyperparameterë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤.
# âš ï¸ ì œê°€ GridSearchCVì˜ API Referenceë¥¼ ë“¤ê³ ì˜¨ ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤.
# Search space keyë¥¼ ì–´ë–»ê²Œ ì •ì˜í•´ì¤˜ì•¼í• ê¹Œìš”?

# Pipelineì—ì„œëŠ” 'model__íŒŒë¼ë¯¸í„°ëª…' í˜•ì‹ìœ¼ë¡œ ì ‘ê·¼!
param_grids = {
    "Ridge": {
        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],  # ì •ê·œí™” ê°•ë„
        'model__solver': ['auto', 'svd', 'cholesky']     # ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    },
    "Lasso": {
        'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0],  # ì •ê·œí™” ê°•ë„
        'model__max_iter': [1000, 5000]                  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
    },
    "MLPRegressor": {
        'model__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # ì€ë‹‰ì¸µ êµ¬ì¡°
        'model__activation': ['relu', 'tanh'],                                # í™œì„±í™” í•¨ìˆ˜
        'model__alpha': [0.0001, 0.001, 0.01],                               # L2 ì •ê·œí™”
        'model__learning_rate_init': [0.001, 0.01]                           # í•™ìŠµë¥ 
    },
}

# 5. ì²œí•˜ì œì¼íšŒê·€ëª¨ë¸ ì°¾ì•„ì£¼ê¸°
# ì•„ë˜ ë£¨í”„ë¥¼ ëŒë¦¬ë©´ì„œ GridSearchCVë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score


results = []

for name, pipeline in pipelines.items():
    # ëª¨ë¸ ë³„ë¡œ GridSearchCVë¥¼ ìˆ˜í–‰í•´ì¤ë‹ˆë‹¤.
    # Fold ìˆ˜ë‚˜ scoringì€ ì—¬ëŸ¬ë¶„ì´ ììœ ë¡­ê²Œ ì„ íƒí•´ì„œ ì œì¼ ì¢‹ì€ íšŒê·€ëª¨ë¸ì„ ì°¾ì•„ì£¼ì„¸ìš”.
    # GridSearchCVì˜ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ ë³„ë¡œ ì œì¼ ì˜ í•™ìŠµëœ estimatorë¥¼ ì°¾ì•„ `best_model`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
    # Grid Searchê°€ ì–´ë–»ê²Œ ìˆ˜í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ë³´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”? hint: verbose

    # GridSearchCV ìˆ˜í–‰
    grid = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grids[name],
        cv=5,                      # 5-Fold Cross Validation
        scoring='neg_mean_squared_error',  # MSEë¥¼ ìµœì†Œí™” (ìŒìˆ˜ë¡œ í‘œí˜„)
        n_jobs=-1,                 # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
        verbose=2                  # ì§„í–‰ ìƒí™© ì¶œë ¥ (0: ì—†ìŒ, 1: ê°„ë‹¨, 2: ìƒì„¸)
    )

    # í•™ìŠµ
    grid.fit(X_train, y_train)

    # ìµœì  ëª¨ë¸ ì¶”ì¶œ
    best_model = grid.best_estimator_

    # `best_model`ë¡œ `y_pred`ë¥¼ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”.
    # ê·¸ í›„ì— MSE, RÂ²ë¥¼ resultsì— ê¸°ë¡í•´ì£¼ì„¸ìš”.

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
    y_pred = best_model.predict(X_test)

    mse = mean_squared_error(y_true=y_test, y_pred=y_pred)
    r2  = r2_score(y_true=y_test, y_pred=y_pred)
    results.append({
        "Model":       name,
        "Best Params": grid.best_params_,
        "MSE":         mse,
        "RÂ²":          r2,
    })

# ê²°ê³¼ ì¶œë ¥: ì–´ë–¤ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì€ê°€ìš”?
pd.DataFrame(results).sort_values(by="MSE", ascending=True)

"""### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: ë¶„ë¥˜ ê³¼ì œ ë‹¤ì‹œ ìˆ˜í–‰í•´ë³´ê¸°

ì´ë²ˆì—ëŠ” ì§€ë‚œ ì‹œê°„ì— ì‚¬ìš©í•œ ìˆ«ìí•„ê¸° ë°ì´í„°ì…‹ì„ `sklearn`ì— ìˆëŠ” ëª¨ë¸ë“¤ë¡œ í•™ìŠµí•´ë´…ì‹œë‹¤. MLPì˜ ê²½ìš° MLPClassifierë¥¼ í™œìš©í•©ì‹œë‹¤.


"""

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# load_digits ë°ì´í„°ì…‹ ë¡œë“œ
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target

# TODO: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# train/test ë¶„í• 
# í…ŒìŠ¤íŠ¸ë°ì´í„°ëŠ” ì „ì²´ ë°ì´í„°ì˜ 20%ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# TODO:
# 2. ì—¬ëŸ¬ ëª¨ë¸ì˜ Pipelinesë¥¼ ì •ì˜í•´ì¤ì‹œë‹¤.
# ë‹¤ìŒ ëª¨ë¸ë“¤ì„ ë¹„êµí•´ì£¼ì„¸ìš”: LogisticRegression, DecisionTree, Bagging, RandomForest, AdaBoost, MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier

pipelines = {
    "LogisticRegression": Pipeline([
        ('scaler', StandardScaler()),
        ('model', LogisticRegression(max_iter=1000, random_state=42))
    ]),
    "DecisionTree": Pipeline([
        ('scaler', StandardScaler()),
        ('model', DecisionTreeClassifier(random_state=42))
    ]),
    "Bagging": Pipeline([
        ('scaler', StandardScaler()),
        ('model', BaggingClassifier(random_state=42))
    ]),
    "RandomForest": Pipeline([
        ('scaler', StandardScaler()),
        ('model', RandomForestClassifier(random_state=42))
    ]),
    "AdaBoost": Pipeline([
        ('scaler', StandardScaler()),
        ('model', AdaBoostClassifier(random_state=42))
    ]),
    "MLPClassifier": Pipeline([
        ('scaler', StandardScaler()),
        ('model', MLPClassifier(max_iter=1000, random_state=42))
    ]),
}

# TODO:
# 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
# ìœ„ì— `pipelines`ì— ìˆëŠ” ëª¨ë¸ë“¤ ë³„ë¡œ hyperparameterë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤.
param_grids = {
    "LogisticRegression": {
        'model__C': [0.1, 1.0, 10.0],
        'model__solver': ['lbfgs', 'liblinear']
    },
    "DecisionTree": {
        'model__max_depth': [5, 10, 20, None],
        'model__min_samples_split': [2, 5, 10]
    },
    "Bagging": {
        'model__n_estimators': [10, 50, 100],
        'model__max_samples': [0.5, 0.7, 1.0]
    },
    "RandomForest": {
        'model__n_estimators': [50, 100, 200],
        'model__max_depth': [10, 20, None]
    },
    "AdaBoost": {
        'model__n_estimators': [50, 100, 200],
        'model__learning_rate': [0.01, 0.1, 1.0]
    },
    "MLPClassifier": {
        'model__hidden_layer_sizes': [(50,), (100,), (100, 50)],
        'model__activation': ['relu', 'tanh'],
        'model__alpha': [0.0001, 0.001, 0.01]
    }
}

# TODO:
# 4. ì²œí•˜ì œì¼ë¶„ë¥˜ëª¨ë¸ ì°¾ì•„ì£¼ê¸°
# ì•„ë˜ ë£¨í”„ë¥¼ ëŒë¦¬ë©´ì„œ GridSearchCVë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
from sklearn.metrics import accuracy_score, f1_score

results = []
for name, pipe in pipelines.items():
    # ëª¨ë¸ ë³„ë¡œ GridSearchCVë¥¼ ìˆ˜í–‰í•´ì¤ë‹ˆë‹¤.
    # Fold ìˆ˜ë‚˜ scoringì€ ì—¬ëŸ¬ë¶„ì´ ììœ ë¡­ê²Œ ì„ íƒí•´ì„œ ì œì¼ ì¢‹ì€ íšŒê·€ëª¨ë¸ì„ ì°¾ì•„ì£¼ì„¸ìš”.
    # GridSearchCVì˜ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ ë³„ë¡œ ì œì¼ ì˜ í•™ìŠµëœ estimatorë¥¼ ì°¾ì•„ `best_model`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
    # ë§ˆì°¬ê°€ì§€ë¡œ Grid Searchì˜ ë‚´ë¶€ ë™ì‘ì„ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ argumentë¥¼ ì¡°ì ˆí•´ì£¼ì„¸ìš”.
    grid = GridSearchCV(
        estimator=pipe,
        param_grid=param_grids[name],
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    grid.fit(X_train, y_train)

    best_model = grid.best_estimator_

    # `best_model`ë¡œ `y_pred`ë¥¼ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”.
    # ê·¸ í›„ì— ACC, F1-scoreë¥¼ resultsì— ê¸°ë¡í•´ì£¼ì„¸ìš”.
    y_pred = best_model.predict(X_test)

    acc = accuracy_score(y_true=y_test, y_pred=y_pred)
    f1  = f1_score(y_true=y_test, y_pred=y_pred, average='macro')

    results.append({
        "Model":       name,
        "Best Params": grid.best_params_,
        "Accuracy":    acc,
        "F1 Score":    f1,
    })

# 5. ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
pd.DataFrame(results).sort_values(by="Accuracy", ascending=False)

"""# âš¡ï¸ PyTorch Lightningì´ë€?

**í•™ìŠµ ëª©í‘œ**  
  - `LightningDataModule`ë¡œ `FashionMNIST` ë°ì´í„° ë¡œë”©ê³¼ ì „ì²˜ë¦¬ë¥¼ ëª¨ë“ˆí™”í•œë‹¤.  
  - `LightningModule`ì„ í™œìš©í•´ MLP ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì˜ í•™ìŠµ/ê²€ì¦ ë£¨í”„ë¥¼ ì¶”ìƒí™”í•œë‹¤.  
  - `Trainer`ì—ì„œ `ModelCheckpoint`, `EarlyStopping`, `WandbLogger` ë“± ì½œë°±ì„ ì ìš©í•´ í•™ìŠµ ê³¼ì •ì„ ê´€ë¦¬í•œë‹¤.  

**í•™ìŠµ ê°œë…**  
  - `LightningDataModule`: `prepare_data`, `setup`, `train_dataloader`, `val_dataloader` ë©”ì„œë“œë¡œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¡°í™”  
  - `LightningModule`: `forward`, `training_step`, `validation_step`, `configure_optimizers`ë¡œ ëª¨ë¸ í•™ìŠµ ë¡œì§ ìº¡ìŠí™”  
  - `Trainer`: `max_epochs`, `accelerator`, `callbacks`, `logger` ì˜µì…˜ìœ¼ë¡œ í•™ìŠµ ì‹¤í–‰ í™˜ê²½ ì œì–´  
  - ì½œë°±(Callbacks) ë° ë¡œê±°(Loggers):  
    - `ModelCheckpoint`: ìµœì ì˜ ê²€ì¦ ì„±ëŠ¥ ì‹œì  ëª¨ë¸ ìë™ ì €ì¥  
    - `EarlyStopping`: ì§€ì • ì—í­ ìˆ˜ ë™ì•ˆ ì„±ëŠ¥ í–¥ìƒ ì—†ì„ ì‹œ í•™ìŠµ ì¡°ê¸° ì¤‘ë‹¨  
    - `WandbLogger`: ì‹¤í—˜ ë©”íŠ¸ë¦­ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ Weights & Biasesì— ê¸°ë¡  

**ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½**  
  1. FashionMNIST ë°ì´í„°ë¥¼ `ToTensor`, `Normalize`ë¡œ ì „ì²˜ë¦¬í•˜ê³  DataModuleì— ìº¡ìŠí™”í•œë‹¤.  
  2. MLPClassifier `LightningModule`ì„ êµ¬í˜„í•´ 2ê°œ íˆë“  ë ˆì´ì–´, ë“œë¡­ì•„ì›ƒ, CrossEntropyLoss ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ì„ ì •ì˜í•œë‹¤.  
  3. `pl.Trainer`ë¥¼ ì‚¬ìš©í•´ max_epochs=10, accelerator='auto', callbacksì™€ loggerë¥¼ ì„¤ì •í•˜ì—¬ í•™ìŠµì„ ì‹¤í–‰í•œë‹¤.  
  4. ìµœì í™”ëœ ì²´í¬í¬ì¸íŠ¸ ì €ì¥, ì¡°ê¸° ì¤‘ë‹¨ì„ êµ¬í˜„í•œë‹¤.

ì§€ë‚œ ì‹œê°„ì— PyTorchë¥¼ ì²˜ìŒìœ¼ë¡œ ë‹¤ë¤„ë³´ì•˜ìŠµë‹ˆë‹¤. ê³¼ê±°ì—ëŠ” PyTorch í•˜ë‚˜ë§Œ ê°€ì§€ê³  ëª¨ë“  ê°œë°œì„ ë‹¤í–ˆì–´ì•¼ í•˜ëŠ”ë°, ì§€ê¸ˆì€ ë°˜ë³µì ì¸ ì½”ë“œë¥¼ ë‹¨ìˆœí™”ì‹œì¼œë‘” ë§ì€ í”„ë ˆì„ì›Œí¬ê°€ ìƒê²¼ìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ê²ƒì´ PyTorch Lightningì…ë‹ˆë‹¤.

PyTorchê°€ ê¸°ë³¸ ë¸”ë¡ë§Œ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¼ë©´, PyTorch Lightningì€ ì´ ë¸”ë¡ë“¤ì„ ì¡°ê¸ˆ ë” ìœ ê¸°ì ìœ¼ë¡œ ì˜ ì¡°í•©í•´ë‘” êµ¬ì¡°í™”ëœ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë¸ ì •ì˜, í•™ìŠµ ë£¨í”„, í‰ê°€, ë¡œê¹…, GPU ì„¤ì • ë“± ë°˜ë³µì ìœ¼ë¡œ ì‘ì„±í•˜ë˜ ì½”ë“œë¥¼ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë•ë¶„ì— ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ í•µì‹¬ ë¡œì§ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆê³ , ì‹¤í—˜ ê´€ë¦¬ë‚˜ í™•ì¥ë„ í›¨ì”¬ ì‰¬ì›Œì§‘ë‹ˆë‹¤. íŠ¹íˆ í•™ìŠµ ì½”ë“œê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ Lightningì˜ ì¥ì ì€ ë”ìš± ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” PyTorch Lightningì„ í™œìš©í•´ MLP ëª¨ë¸ì„ FashionMNIST ë°ì´í„°ì…‹ì— ì ìš©í•´ë³´ê³ , ê¸°ì¡´ PyTorch ì½”ë“œì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì§ì ‘ ì²´ê°í•´ë³´ëŠ” ì‹œê°„ì„ ê°€ì ¸ë³´ê² ìŠµë‹ˆë‹¤.
"""

!pip install pytorch-lightning

"""### ğŸ“– **ììŠµ**: PyTorch-Lightningìœ¼ë¡œ FashionMNIST í•™ìŠµí•˜ê¸°

í¬ê²Œ 3ê°€ì§€ ìˆœì„œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
1. ë°ì´í„°ì…‹ì„ ì •ì˜í•˜ëŠ” ë¶€ë¶„: ì‚¬ì‹¤ í•„ìˆ˜ì ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•Œë˜ `torch.utils.data.DataLoader`ë¥¼ ê·¸ëŒ€ë¡œ í™œìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
2. ëª¨ë¸ì„ ì •ì˜í•˜ëŠ” ë¶€ë¶„: `torch.nn.Module` ëŒ€ì‹  `pl.LightningModule`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë¶€ë¶„: `pl.Trainer`ê°€ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë§ì€ ì½”ë“œë¥¼ ëŒ€ì‹ í•´ì¤ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ìì„¸íˆ ì½ì–´ë³´ê³  ì°¨ì´ì ì„ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ëŠ˜ì˜ ê³¼ì œëŠ” ì œê°€ ì‘ì„±í•´ë“œë¦° ì½”ë“œì— ì¶”ê°€ë¡œ Callbackì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì•„ë˜ ì½”ë“œë¥¼ ì˜ ì´í•´í•˜ì…”ì•¼í•©ë‹ˆë‹¤.

"""

import pytorch_lightning as pl
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# PyTorch Lightningì˜ DataModuleì€ ë°ì´í„° ê´€ë ¨ ë¡œì§ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ê¹”ë”í•˜ê²Œ ë¶„ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ
# PyTorchì—ì„œëŠ” ë°ì´í„°ì…‹ ë¡œë”©, ì „ì²˜ë¦¬, DataLoader ì •ì˜ë¥¼ main ì½”ë“œì— ì§ì ‘ ì‘ì„±í•´ì•¼ í–ˆì§€ë§Œ,
# Lightningì—ì„œëŠ” ì´ë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ ê°€ë…ì„±ì„ ë†’ì„
# ì´ë ‡ê²Œ ì‘ì„±í•˜ë©´ ì§€ë‚œ ì‹œê°„ì— ë°˜ë³µì ìœ¼ë¡œ ì‘ì„±í•œ train_loader, valid_loaderë¥¼ ë³„ë„ì˜ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ì§€ ì•Šì•„ë„ ë¨
class FashionMNISTDataModule(pl.LightningDataModule):
    def __init__(self, batch_size=64):
        super().__init__()
        self.batch_size = batch_size

        # transform ì •ì˜ë„ í´ë˜ìŠ¤ ë‚´ë¶€ì— í¬í•¨ì‹œì¼œ ê´€ë¦¬ ê°€ëŠ¥
        # PyTorchì—ì„œëŠ” transformì„ ë”°ë¡œ ì •ì˜í•˜ê³  ì „ë‹¬í•´ì•¼ í–ˆì§€ë§Œ,
        # Lightningì—ì„œëŠ” ì´ì²˜ëŸ¼ ëª¨ë“ˆ ë‚´ë¶€ì—ì„œ ì¼ê´€ë˜ê²Œ ê´€ë¦¬ ê°€ëŠ¥
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])

    # prepare_dataëŠ” ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë“± í•œ ë²ˆë§Œ ìˆ˜í–‰ë˜ëŠ” ì‘ì—…ì„ ì •ì˜
    # PyTorchì—ì„œëŠ” ì´ëŸ° ì‘ì—…ë„ main ì½”ë“œì— ì§ì ‘ ì‘ì„±í•´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ ë¶„ë¦¬ëœ ë©”ì„œë“œë¡œ ê´€ë¦¬í•˜ì—¬ ì½”ë“œ êµ¬ì¡°ë¥¼ ëª…í™•íˆ í•¨
    def prepare_data(self):
        datasets.FashionMNIST(root='./data', train=True, download=True)
        datasets.FashionMNIST(root='./data', train=False, download=True)

    # setupì€ train/val/test ë°ì´í„°ì…‹ì„ ì •ì˜í•˜ëŠ” ë‹¨ê³„
    # stage ì¸ìë¥¼ í†µí•´ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •ë„ ê°€ëŠ¥
    # PyTorchì—ì„œëŠ” ì´ëŸ° ë¶„ê¸° ì²˜ë¦¬ë¥¼ ì§ì ‘ êµ¬í˜„í•´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ ì´ë¥¼ í‘œì¤€í™”ëœ ë°©ì‹ìœ¼ë¡œ ì œê³µ
    def setup(self, stage=None):
        self.train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=self.transform)
        self.val_dataset = datasets.FashionMNIST(root='./data', train=False, transform=self.transform)

    # train_dataloaderëŠ” í•™ìŠµìš© DataLoader ë°˜í™˜
    # PyTorchì—ì„œëŠ” ì§ì ‘ DataLoaderë¥¼ ë§Œë“¤ê³  ì „ë‹¬í•´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ Trainerê°€ ìë™ìœ¼ë¡œ ì´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•´ ì‚¬ìš©í•¨
    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)

    # val_dataloaderëŠ” ê²€ì¦ìš© DataLoader ë°˜í™˜
    # PyTorchì—ì„œëŠ” ê²€ì¦ ë£¨í”„ë¥¼ ì§ì ‘ ì‘ì„±í•´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ validation_stepê³¼ í•¨ê»˜ ìë™ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size)

    # predict_dataloader ê²€ì¦ìš© DataLoader ë°˜í™˜
    # ì¶”í›„ì— `trainer.predict`ë¥¼ í†µí•´ ìì„¸í•œ ê²€ì¦ê²°ê³¼ í™•ì¸ì„ ìœ„í•´ êµ¬í˜„
    # Lightningì€ predict_step í•¨ê»˜ ìë™ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
    def predict_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size)

import torch
from torch import nn
import torch.nn.functional as F

# LightningModuleì€ ëª¨ë¸ ì •ì˜, í•™ìŠµ/ê²€ì¦ ë¡œì§, ì˜µí‹°ë§ˆì´ì € ì„¤ì •ê¹Œì§€ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì— í†µí•©
# PyTorchì—ì„œëŠ” ì´ ëª¨ë“  ê²ƒì„ main ì½”ë“œì—ì„œ ë”°ë¡œ ê´€ë¦¬í•´ì•¼ í–ˆì§€ë§Œ,
# Lightningì€ ì´ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì½”ë“œ ì¬ì‚¬ìš©ì„±ê³¼ ê°€ë…ì„±ì„ ë†’ì„
# nn.Moduleì„ ìƒì† ë°›ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ pl.LightningModuleì„ ìƒì† ë°›ìŠµë‹ˆë‹¤.
class MLPClassifier(pl.LightningModule):
    def __init__(self, input_dim=28*28, hidden_dims=(128, 64), dropout=0.2, num_classes=10):
        # PyTorchë¥¼ ì‚¬ìš©í•  ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë°˜ë“œì‹œ ì‘ì„±í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.
        super().__init__()

        # ëª¨ë¸ êµ¬ì¡° ì •ì˜: Sequentialë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬ì„±
        # PyTorchì—ì„œëŠ” forward í•¨ìˆ˜ ë‚´ë¶€ì— ì§ì ‘ ë ˆì´ì–´ë¥¼ í˜¸ì¶œí•´ì•¼ í–ˆì§€ë§Œ,
        # Lightningì—ì„œëŠ” ì´ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬ ê°€ëŠ¥
        h1, h2 = hidden_dims
        self.model = nn.Sequential(
            nn.Flatten(),  # ì…ë ¥ ì´ë¯¸ì§€(28x28)ë¥¼ 1D ë²¡í„°ë¡œ ë³€í™˜
            nn.Linear(input_dim, h1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h1, h2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h2, num_classes)
        )

    # forward í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ìˆœì „íŒŒ ì •ì˜
    # Lightningì€ ì´ í•¨ìˆ˜ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ í˜¸ì¶œí•˜ë¯€ë¡œ ì‚¬ìš©ìëŠ” ë”°ë¡œ ì‹ ê²½ ì“¸ í•„ìš” ì—†ìŒ
    def forward(self, x):
        return self.model(x)

    # training_stepì€ í•™ìŠµ ë£¨í”„ì˜ í•œ ìŠ¤í…ì„ ì •ì˜ (ë°°ì¹˜ë‹¨ìœ„)
    # PyTorchì—ì„œëŠ” epoch ë£¨í”„, ë°°ì¹˜ ë£¨í”„, loss ê³„ì‚°, optimizer step ë“±ì„ ì§ì ‘ ì‘ì„±í•´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ ì´ ë©”ì„œë“œë§Œ ì •ì˜í•˜ë©´ Trainerê°€ ìë™ìœ¼ë¡œ ë°˜ë³µ ì²˜ë¦¬
    # ì§€ë‚œ ì‹¤ìŠµì— ì œì‘í•œ `train_one_epoch` í•¨ìˆ˜ì™€ ë™ì¼í•œ ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤.
    # return ê°’ì—ëŠ” `.backward`ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í…ì„œê°€ ë°˜í™˜ë˜ê±°ë‚˜
    # dictë¡œ ë°˜í™˜ë  ê²½ìš° `loss`ë¼ëŠ” keyê°€ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()

        # log_dictì„ í†µí•´ lossì™€ accuracyë¥¼ ë¡œê¹…
        # PyTorchì—ì„œëŠ” TensorBoardë‚˜ tqdm ë“±ì„ ì§ì ‘ ì„¤ì •í•´ì•¼ í–ˆì§€ë§Œ,
        # Lightningì€ ìë™ ë¡œê¹… ë° í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì§€ì›
        result = {"loss": loss, "acc": acc}
        self.log_dict(result, prog_bar=True)
        return loss

    # validation_stepì€ ê²€ì¦ ë£¨í”„ì˜ í•œ ìŠ¤í…ì„ ì •ì˜
    # PyTorchì—ì„œëŠ” ê²€ì¦ ë£¨í”„ë„ ì§ì ‘ ì‘ì„±í•´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ ì´ ë©”ì„œë“œë§Œ ì •ì˜í•˜ë©´ ìë™ìœ¼ë¡œ validation_epoch_endê¹Œì§€ ì²˜ë¦¬
    # ì§€ë‚œ ì‹¤ìŠµì— ì œì‘í•œ `evaluate`ì™€ ê°™ì€ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()
        result = {"val_loss": loss, "val_acc": acc}
        self.log_dict(result, prog_bar=True)

    # configure_optimizersëŠ” ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    # PyTorchì—ì„œëŠ” optimizerë¥¼ ë”°ë¡œ ì •ì˜í•˜ê³  í•™ìŠµ ë£¨í”„ì— ë„£ì–´ì•¼ í–ˆì§€ë§Œ,
    # Lightningì€ ì´ ë©”ì„œë“œì—ì„œ ë°˜í™˜ë§Œ í•˜ë©´ Trainerê°€ ìë™ìœ¼ë¡œ ì‚¬ìš©
    # ë§¤ë²ˆ optimizerë¥¼ ë¶ˆëŸ¬ì„œ zero_grad, stepì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ë„ ì•Œì•„ì„œ ì‘ë™í•¨
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

# ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•˜ë©´ ìœ„ì—ì„œ ì œì‘í•œ ëª¨ë“ˆë“¤ì´ ìœ ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ì—¬ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.
# `trainer`ëŠ” í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ì—¬ëŸ¬ ì„¸íŒ…ê°’ë“¤ì„ ë°›ìŠµë‹ˆë‹¤.
# API Referenceë¥¼ ê¼­ í™•ì¸í•´ì£¼ì„¸ìš”.
# https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api
data_module = FashionMNISTDataModule(batch_size=1024)
model = MLPClassifier()
trainer = pl.Trainer(max_epochs=5, accelerator="auto")
trainer.fit(model, data_module)
trainer.validate(model, data_module)

"""### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: `pl.LightningModule`ì— êµ¬ì„±ìš”ì†Œ ì¶”ê°€í•˜ê¸°

ì§€ë‚œ ë²ˆì— ì œì‘í•œ ì½”ë“œì™€ ë¹„êµí–ˆì„ ë•Œ ë§ì´ ê°„ê²°í•´ì¡Œì£ ? ì—¬ê¸°ì— ë” ë§ì€ ê¸°ëŠ¥ë“¤ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
1. í•™ìŠµ ì „ê³¼ í›„: í˜„ì¬ `training_step`ì´ë‘ `validation_step`ë§Œ êµ¬í˜„í–ˆëŠ”ë°, ì¶”ê°€ë¡œ í•™ìŠµ ì „ì´ë‚˜ í›„, ê²€ì¦ ì „ì´ë‚˜ í›„ì— ì¶”ê°€ë¡œ ì•¡ì…˜ì„ ì·¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ì˜ epochê°€ ëë‚˜ê³ ë‚˜ì„œ epochì˜ í‰ê·  ì†ì‹¤ì´ë‚˜ ì •í™•ë„ ê°™ì€ ê²ƒì„ ê³„ì‚°í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ì–´ë–»ê²Œí•´ì•¼í• ê¹Œìš”? ê·¸ëŸ´ ë•ŒëŠ” `training_epoch_end`, `validation_epoch_end` ë©”ì†Œë“œë¥¼ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤.
2. `wandb logger`: ì§€ë‚œ ì‹¤ìŠµ ë§ë¯¸ì— `wandb` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. `pl.LightningModule`ì—ì„œ ì‚¬ìš©í•˜ëŠ” `log_dict` í•¨ìˆ˜ë¥¼ `wnadb`ì— ë°”ë¡œ ì—°ë™ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸°: ë§¤ë²ˆ ì €ì¥í•  ë¡œì§ì„ ì§œê³  `torch.save`ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒë„ ê·€ì°®ìŠµë‹ˆë‹¤. `pytorch_lightning.callbacks.ModelCheckpoint`ë¼ëŠ” ì½œë°±ì„ ì‚¬ìš©í•˜ë©´ ì´ë¥¼ ê°„í¸í•˜ê²Œ ì‘ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. í•™ìŠµ ì¡°ê¸°ì¢…ë£Œ ë¡œì§: ì´ ë¡œì§ë„ êµ¬í˜„í•˜ëŠ”ê²Œ ë§¤ë²ˆ ë²ˆê±°ë¡œìš´ë° ì´ ë˜í•œ `pytorch_lightning.callbacks.EarlyStopping`ì„ í†µí•´ì„œ ì‰½ê²Œ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

class MLPClassifier(pl.LightningModule):
    def __init__(self,
                 input_dim=28*28,
                 hidden_dims=(128, 64),
                 dropout=0.2,
                 num_classes=10,
                 lr=1e-3):
        super().__init__()

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ (ì´ê²Œ ì¤‘ìš”!)
        self.save_hyperparameters()

        # lrì„ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
        self.lr = lr

        h1, h2 = hidden_dims
        self.model = nn.Sequential(
            nn.Flatten(),  # ì…ë ¥ ì´ë¯¸ì§€(28x28)ë¥¼ 1D ë²¡í„°ë¡œ ë³€í™˜
            nn.Linear(input_dim, h1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h1, h2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h2, num_classes)
        )

        # í•œ epoch ë‚´ì—ì„œ ê¸°ë¡ë˜ëŠ” ê°’ë“¤ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        self.training_step_outputs = []
        self.validation_step_outputs = []

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        # TODO
        # ìœ„ì—ì„œ ì œì‘í•˜ì‹  í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì£¼ì„¸ìš”
        # ë°˜í™˜í•  ê²°ê³¼ê°’ì„ `self.training_step_outputs`ì— append í•´ì£¼ì„¸ìš”

        x, y = batch

        # Forward pass
        logits = self(x)

        # Loss ê³„ì‚°
        loss = F.cross_entropy(logits, y)

        # ì •í™•ë„ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)
        acc = (preds == y).float().mean()

        # ë¡œê¹… (progress barì™€ loggerì— ê¸°ë¡)
        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)
        self.log('train_acc', acc, prog_bar=True, on_step=True, on_epoch=True)

        # epoch ëì—ì„œ í‰ê·  ê³„ì‚°ì„ ìœ„í•´ ì €ì¥
        output = {'loss': loss, 'acc': acc}
        self.training_step_outputs.append(output)

        return loss

    def on_train_epoch_end(self):
        # TODO: train epochì˜ í‰ê·  Lossì™€ í‰ê·  ì •í™•ë„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.
        # ë§ˆì§€ë§‰ì— validation_step_outputsë¥¼ clearí•´ì£¼ì„¸ìš”

        # epoch ë™ì•ˆ ëª¨ë“  stepì˜ lossì™€ acc ìˆ˜ì§‘
        all_losses = torch.stack([x['loss'] for x in self.training_step_outputs])
        all_accs = torch.stack([x['acc'] for x in self.training_step_outputs])

        # í‰ê·  ê³„ì‚°
        avg_loss = all_losses.mean()
        avg_acc = all_accs.mean()

        # epoch í‰ê·  ë¡œê¹…
        self.log('train_loss_epoch', avg_loss, prog_bar=True)
        self.log('train_acc_epoch', avg_acc, prog_bar=True)

        # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.training_step_outputs.clear()


    def validation_step(self, batch, batch_idx):
        # TODO
        # ìœ„ì—ì„œ ì œì‘í•˜ì‹  í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì£¼ì„¸ìš”
        # ë°˜í™˜í•  ê²°ê³¼ê°’ì„ `self.validation_step_outputs`ì— append í•´ì£¼ì„¸ìš”

        x, y = batch

        # Forward pass
        logits = self(x)

        # Loss ê³„ì‚°
        loss = F.cross_entropy(logits, y)

        # ì •í™•ë„ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)
        acc = (preds == y).float().mean()

        # ë¡œê¹…
        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)
        self.log('val_acc', acc, prog_bar=True, on_step=False, on_epoch=True)

        # epoch ëì—ì„œ í‰ê·  ê³„ì‚°ì„ ìœ„í•´ ì €ì¥
        output = {'loss': loss, 'acc': acc}
        self.validation_step_outputs.append(output)

        return loss


    def on_validation_epoch_end(self):
        # TODO: validation epochì˜ í‰ê·  Lossì™€ í‰ê·  ì •í™•ë„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.
        # ë§ˆì§€ë§‰ì— validation_step_outputsë¥¼ clearí•´ì£¼ì„¸ìš”

        # epoch ë™ì•ˆ ëª¨ë“  stepì˜ lossì™€ acc ìˆ˜ì§‘
        all_losses = torch.stack([x['loss'] for x in self.validation_step_outputs])
        all_accs = torch.stack([x['acc'] for x in self.validation_step_outputs])

        # í‰ê·  ê³„ì‚°
        avg_loss = all_losses.mean()
        avg_acc = all_accs.mean()

        # epoch í‰ê·  ë¡œê¹…
        self.log('val_loss_epoch', avg_loss, prog_bar=True)
        self.log('val_acc_epoch', avg_acc, prog_bar=True)

        # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.validation_step_outputs.clear()


    def predict_step(self, batch, batch_idx) -> dict:
        # TODO: `trainer.predict`ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ `predict_stepì„ êµ¬í˜„í•´ë´…ì‹œë‹¤.
        # ê²°ê³¼ë¥¼ í•œë° ëª¨ìœ¼ê¸° ìœ„í•´ ì „ì²´ logitsë¥¼ ê³„ì‚°í•´ë´…ì‹œë‹¤. ë°˜í™˜ê°’ìœ¼ë¡œ `logits`ë¥¼ keyë¡œ í•˜ë„ë¡ dictì— ë„£ì–´ì£¼ì„¸ìš”.
        # ì¶”ê°€ë¡œ ë¼ë²¨ê°’ì„ ê°™ì´ ë°›ì•„ `y_true`ë¥¼ keyë¡œ í•˜ë„ë¡ dictì— ë„£ì–´ì£¼ì„¸ìš”.
        # ì°¸ê³ : https://lightning.ai/docs/pytorch/LTS/common/lightning_module.html#prediction-loop

        x, y = batch

        # Forward pass
        logits = self(x)

        # ê²°ê³¼ ë°˜í™˜
        return {
            'logits': logits,
            'y_true': y
        }


    def configure_optimizers(self):
        # TODO
        # ìœ„ì—ì„œ ì œì‘í•˜ì‹  í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì£¼ì„¸ìš”

        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

# TODO:
# W&B ë¡œê±°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
# ê³„ì • ìƒì„± í›„ API Keyë¥¼ ì…ë ¥í•´ì£¼ì–´ì•¼í•©ë‹ˆë‹¤.
# ë²ˆê±°ë¡œìš°ì‹œë‹¤ë©´ íŒ¨ìŠ¤í•´ë„ë©ë‹ˆë‹¤.
from pytorch_lightning.loggers import WandbLogger
# wandb_logger = None   # W&B ë¡œê±°ë¥¼ ìƒì„±í•  ê²½ìš°, ì£¼ì„í•´ì œ í›„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

# TODO:
# ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•´ì£¼ëŠ” ì½œë°±í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì£¼ì„¸ìš”.
# API Referenceë¥¼ ë³´ê³  ì–´ë–¤ ì¸ìê°€ ë“¤ì–´ê°€ì•¼í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html
from pytorch_lightning.callbacks import ModelCheckpoint
checkpoint_cb = ModelCheckpoint(
    monitor='val_acc_epoch',                      # ëª¨ë‹ˆí„°ë§í•  ì§€í‘œ
    dirpath='checkpoints/',                       # ì €ì¥ ê²½ë¡œ
    filename='mlp-{epoch:02d}-{val_acc_epoch:.4f}',  # íŒŒì¼ëª… í˜•ì‹
    save_top_k=3,                                 # ìƒìœ„ 3ê°œë§Œ ì €ì¥
    mode='max',                                   # val_accëŠ” ìµœëŒ€í™”
    save_last=True                                # ë§ˆì§€ë§‰ ì—í¬í¬ë„ ì €ì¥
)

# TODO:
# í•™ìŠµì˜ ì¡°ê¸°ì¢…ë£Œë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•´ì£¼ëŠ” ì½œë°±í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì£¼ì„¸ìš”.
# API Referenceë¥¼ ë³´ê³  ì–´ë–¤ ì¸ìê°€ ë“¤ì–´ê°€ì•¼í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html
from pytorch_lightning.callbacks import EarlyStopping
earlystop_cb = EarlyStopping(
    monitor='val_loss_epoch',    # ëª¨ë‹ˆí„°ë§í•  ì§€í‘œ
    min_delta=0.001,             # ìµœì†Œ ë³€í™”ëŸ‰
    patience=3,                  # 3 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    mode='min',                  # val_lossëŠ” ìµœì†Œí™”
    verbose=True
)

# TODO: Trainerë¥¼ í†µí•œ í•™ìŠµ
# callbacksì™€ loggerëŠ” ë‹¤ìŒì²˜ëŸ¼ ë„£ì–´ì£¼ë©´ ë©ë‹ˆë‹¤.
"""
ìš”êµ¬ì‚¬í•­
- max_epochsë¥¼ 10ìœ¼ë¡œ ëŠ˜ë ¤ì£¼ì„¸ìš”
- gradient accumulationì„ ììœ ë¡­ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš”.
- gradient clippingì„ ì„¤ì •í•´ì£¼ì„¸ìš” (1.0)
"""
trainer = pl.Trainer(
    max_epochs=10,                        # 10 ì—í¬í¬ë¡œ ì¦ê°€
    accelerator='auto',
    callbacks=[checkpoint_cb, earlystop_cb],
    accumulate_grad_batches=2,            # gradient accumulation (2ë°°ì¹˜ë§ˆë‹¤ ì—…ë°ì´íŠ¸)
    gradient_clip_val=1.0,                # gradient clipping
    log_every_n_steps=10
)

# ë°ì´í„°ëª¨ë“ˆ, ëª¨ë¸ ì„ ì–¸
dm = FashionMNISTDataModule(batch_size=1024)
model = MLPClassifier()

# í•™ìŠµ
trainer.fit(model, dm)

# Validation Datasetì— ëŒ€í•´ ê²€ì¦
trainer.validate(model, dm)

"""# ë§ˆì¹˜ë©°...

ì˜¤ëŠ˜ì€ `scikit-learn`ê³¼ `pytorch-lightning`ì„ í•œ ë²ˆ ë‹¤ì‹œ í†ºì•„ë³´ëŠ” ì‹œê°„ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤. í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë‹¤ì‹œ API Referenceë¥¼ í™•ì¸í•´ë³´ë©° ë‹¤ì ¸ë´…ì‹œë‹¤. ìœ„ì—ì„œ í™•ì¸í•œ í•™ìŠµ ê²°ê³¼ê°€ ì¢‹ì§€ ì•Šìœ¼ë©´, hyperparameterë¥¼ íŠœë‹í•˜ë©´ì„œ ì¡°ì ˆí•´ë´…ì‹œë‹¤.

## Further Readings

- Understanding the difficulty of training deep feedforward neural networks: MLP í•™ìŠµ ì´ˆë°˜ ê¸°ìš¸ê¸° ì†Œì‹¤/í­ì£¼ ë¬¸ì œì™€ ê°ì¢… ì´ˆê¸°í™” ê¸°ë²•(Xavier, He ë“±)ì˜ ì´ë¡ ì  ë°°ê²½ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤.
  - https://proceedings.mlr.press/v9/glorot10a.html

- Feature Visualization: How Neural Networks See the World: ì€ë‹‰ì¸µ ë‰´ëŸ°ì´ í•™ìŠµ ì¤‘ ì–´ë–¤ íŠ¹ì§•ì„ í¬ì°©í•˜ëŠ”ì§€ ì‹œê°í™” ê¸°ë²•ì„ í†µí•´ ë¶„ì„í•˜ë©°, MLP ë‚´ë¶€ í‘œí˜„ í•™ìŠµ ê³¼ì •ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.
  - https://distill.pub/2017/feature-visualization/

## Open-Ended Question

- **ì´ˆê¸°í™” ì „ëµ ë¹„êµ ë¶„ì„**: MLPì— Xavier, He, Orthogonal ì´ˆê¸°í™” ê¸°ë²•ì„ ê°ê° ì ìš©í•´ ìˆ˜ë ´ ì†ë„ ë° ìµœì¢… ì •í™•ë„ ì°¨ì´ë¥¼ ì‹¤í—˜í•˜ê³ , í•™ìŠµ ê³¡ì„ ì„ ì‹œê°í™”í•´ë³´ì„¸ìš”.

- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰**: Grid Searchë‚˜ Bayesian Optimizationìœ¼ë¡œ ì€ë‹‰ì¸µ ê°œìˆ˜, ë‰´ëŸ° ìˆ˜, í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ë“±ì„ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ì—¬ ì„±ëŠ¥ì´ ê°€ì¥ ë›°ì–´ë‚œ ì„¤ì •ì„ ì°¾ì•„ ë³´ì„¸ìš”.
"""