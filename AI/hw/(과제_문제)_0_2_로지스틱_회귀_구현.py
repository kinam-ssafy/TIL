# -*- coding: utf-8 -*-
"""(ê³¼ì œ_ë¬¸ì œ)_0-2_ë¡œì§€ìŠ¤í‹±_íšŒê·€_êµ¬í˜„.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nf8eJvCTtmzhu7T8rq3QKIiB1TOHmiHs

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**
1. **ê³¼ì œëª…**: ë¡œì§€ìŠ¤í‹± íšŒê·€ êµ¬í˜„

2. **í•µì‹¬ ì£¼ì œ**
  - ë¡œì§€ìŠ¤í‹± íšŒê·€ì— ëŒ€í•œ ì´í•´
  - NumPyë¥¼ í™œìš©í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ ëª¨ë¸ì˜ ìˆ˜ì‹í™” ë° êµ¬í˜„  

3. **í•™ìŠµ ëª©í‘œ**
  - ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê°€ì„¤ í•¨ìˆ˜ì™€ Binary Cross-Entropy ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê³  êµ¬í˜„í•œë‹¤.  
  - NumPyë¡œ ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì´ìš©í•´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤.  

4. **í•™ìŠµ ê°œë…**
  - Sigmoid í•¨ìˆ˜ : ì…ë ¥ ì‹¤ìˆ˜ ê°’ì„ 0~1 ì‚¬ì´ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜  
  - Binary Cross-Entropy : ì´ì§„ ë¶„ë¥˜ì—ì„œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜  
  - ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• : ì‘ì€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™” ê¸°ë²•  

5. **í•™ìŠµ ë°©í–¥**
  1. ë°ì´í„° ìƒì„± ë° ì‹œê°í™”  
  2. ë¡œì§€ìŠ¤í‹± íšŒê·€ ìˆ˜ì‹í™” ë° êµ¬í˜„  
  3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ì‹¤í—˜

6. **ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´**
  - ë°ì´í„°ì…‹ ëª… : Iris Dataset  
  - ë°ì´í„°ì…‹ ê°œìš” : ë¶“ê½ƒ 3ì¢…(setosa, versicolor, virginica)ìœ¼ë¡œ êµ¬ì„±ëœ ê³ ì „ì ì¸ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ìš© ë°ì´í„°ë¡œ, ê° ìƒ˜í”Œì€ ê½ƒë°›ì¹¨ ê¸¸ì´ã†ë„ˆë¹„ì™€ ê½ƒì ê¸¸ì´ã†ë„ˆë¹„ 4ê°œ íŠ¹ì„±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ 150ê°œ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ë°ì´í„° ì „ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ í•™ìŠµ ì‹¤ìŠµì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ : UCI Machine Learning Repositoryì— ê³µê°œëœ í¼ë¸”ë¦­ ë„ë©”ì¸ ë°ì´í„°ë¡œ, ì›ì €ì‘ê¶ŒìëŠ” Ronald A. Fisherì…ë‹ˆë‹¤.

### **Prerequisites**
```
numpy>=1.26
seaborn>=0.12
matplotlib>=3.8
```

# Logistic Regression êµ¬í˜„í•˜ê¸°
- í•™ìŠµ ëª©í‘œ
    - ì´ì§„ ë¶„ë¥˜ì—ì„œ ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê°€ì„¤ í•¨ìˆ˜, ì†ì‹¤ í•¨ìˆ˜, ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤.
    - ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ë¡œì§€ìŠ¤í‹± ì†ì‹¤ì„ ë²¡í„°í™”í•˜ì—¬ êµ¬í˜„í•œë‹¤.

- í•™ìŠµ ê°œë…
    - ê°€ì„¤ í•¨ìˆ˜: ì‹œê·¸ëª¨ì´ë“œ í™•ë¥  ì˜ˆì¸¡
        - ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ í™•ë¥  ì˜ˆì¸¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
  $$
  p(y=1 \mid \mathbf{x}) = \sigma(z), \quad z = \mathbf{x}^\top \mathbf{w} + b, \quad \sigma(z) = \frac{1}{1 + e^{-z}}
  $$
    - ì†ì‹¤ í•¨ìˆ˜: ì´ì§„ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼
        - $m$: ìƒ˜í”Œ ê°œìˆ˜
        - $\hat{p}_i$: ì‹œê·¸ëª¨ì´ë“œ í™•ë¥  ì¶œë ¥
  $$
  L(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\hat{p}_i) + (1 - y_i)\log(1 - \hat{p}_i) \right]
  $$

    - ê·¸ë˜ë””ì–¸íŠ¸ (ë²¡í„°í™”)
  $$
  \nabla_\mathbf{w} = \frac{1}{m} \mathbf{X}^\top (\hat{\mathbf{p}} - \mathbf{y}), \nabla_b = \frac{1}{m} \sum_{i=1}^m (\hat{p}_i - y_i)
  $$

    - ìˆ˜ì¹˜ ì•ˆì •ì„±
        - $\log(\hat{p})$, $\log(1 - \hat{p})$ ê³„ì‚° ì‹œ, $\hat{p}$ë¥¼ $[\epsilon, 1 - \epsilon]$ ë²”ìœ„ë¡œ í´ë¦¬í•‘í•˜ì—¬ ë¡œê·¸ ì…ë ¥ì´ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
    - í•©ì„± ì´ì§„ ë¶„ë¥˜ ë°ì´í„° ìƒì„± ë° í‘œì¤€í™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ìŠ¤ì¼€ì¼ë§)
    - ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, ì†ì‹¤/ê·¸ë˜ë””ì–¸íŠ¸, ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ NumPyë¡œ êµ¬í˜„
    - ë°°ì¹˜ Gradient Descentë¡œ í•™ìŠµí•˜ë©° ì†ì‹¤ ë° ì •í™•ë„ ê¸°ë¡

### 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ë€?
ì•ì„  ì‹¤ìŠµì—ì„œëŠ” ì„ í˜• íšŒê·€ë¥¼ í†µí•´ íšŒê·€ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” íšŒê·€ ê³¼ì œ ì¤‘ì— ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë¶„ë¥˜ê³¼ì œì—ì„œëŠ” ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì´ ê°€ì¥ ê¸°ì´ˆì ì¼ê¹Œìš”? ë°”ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ì…ë‹ˆë‹¤. ì´ë¦„ì— íšŒê·€ê°€ ìˆì§€ë§Œ ì‹¤ì œë¡œ ì´ ëª¨ë¸ì€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤ ğŸ˜…

ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì…ë ¥ ë²¡í„° $\mathbf{x}$ë¥¼ ì„ í˜• ê²°í•©í•œ ì ìˆ˜ $z = \mathbf{x}^\top \mathbf{w} + b$ë¥¼ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ $\sigma(z) = \frac{1}{1 + e^{-z}}$ë¥¼ í†µí•´ í™•ë¥ ë¡œ ë§¤í•‘í•˜ì—¬ ì´ì§„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” í™•ë¥ ì  ì„ í˜• ëª¨ë¸ì…ë‹ˆë‹¤. ì˜ ìƒê°í•´ë³´ë©´ ì•ì—ì„œ ë°°ìš´ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì— ë„£ì–´ì„œ í™•ë¥ í™”í•˜ëŠ” ê²ƒì´ ë©ë‹ˆë‹¤. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì…ë ¥ê°’ì„ 0ë¶€í„° 1ì‚¬ì´ë¡œ ë§¤í•‘í•´ì£¼ëŠ” í•¨ìˆ˜ë¡œ ì…ë ¥ê°’ì´ 0ì¼ ë•Œ 0.5ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme()

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-5, 5, 200)
p = sigmoid(z)

plt.figure(figsize=(8, 5))
sns.lineplot(x=z, y=p)
plt.title("Sigmoid Function")
plt.xlabel("z")
plt.ylabel("Ïƒ(z)")
plt.axhline(0.5, color='red', linestyle='--', linewidth=1)
plt.axhline(0.0, color='k', linestyle='-', linewidth=2)
plt.axhline(1.0, color='k', linestyle='-', linewidth=2)
plt.show()

"""### 2. ëª¨ë¸ í•™ìŠµí•˜ê¸°
#### 2.1 ì†ì‹¤í•¨ìˆ˜: ì´ì§„í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ (Binary Cross-Entropy)
ê·¸ëŸ¬ë©´ ê²°êµ­ í•™ìŠµí•´ì•¼ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ëŠ” ì„ í˜• íšŒê·€ ë•Œì™€ ì •í™•íˆ ê°™ìŠµë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•´ í•„ìš”í•œ ê²ƒì„ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ë©´, ìš°ë¦¬ê°€ í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ë§¤ê°œë³€ìˆ˜ê°€ ì–¼ë§ˆë‚˜ í•´ë‹¹ ê³¼ì œë¥¼ ì˜ ìˆ˜í–‰í•˜ê³  ìˆëŠ”ì§€ ë³´ê³ (ì†ì‹¤í•¨ìˆ˜), í•´ë‹¹ ì†ì‹¤í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì— ë§ì¶° ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì€ ë¶„ë¥˜ ëª¨ë¸ì…ë‹ˆë‹¤. íšŒê·€ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ í‰ê· ì œê³±ì˜¤ì°¨(MSE)ë¥¼ ì‚¬ìš©í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, íš¨ê³¼ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¶„ë¥˜ ëª¨ë¸ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ì†ì‹¤í•¨ìˆ˜ëŠ” ë°”ë¡œ Cross-Entropyë¼ëŠ” ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ìˆ˜ì‹ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

$$
L(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\hat{p}_i) + (1 - y_i)\log(1 - \hat{p}_i) \right]
$$

- ë°ì´í„° $x_i$ì— ëŒ€í•œ ëª¨ë¸ì˜ ì¶œë ¥ê°’ $\hat{p}_i$ê°€ ìˆë‹¤ê³  í•©ì‹œë‹¤. ê·¸ë¦¬ê³  í•´ë‹¹ ë°ì´í„° $x_i$ì˜ ë¼ë²¨ì€ 0 í˜¹ì€ 1 ê°’ì„ ê°€ì§€ëŠ” $y_i$ì…ë‹ˆë‹¤. ë‘ ê²½ìš°ë¡œ ë‚˜ëˆ„ì–´ì„œ $\Sigma$ ì•ˆì— ìˆëŠ” ê°’ì„ í™•ì¸í•´ë´…ì‹œë‹¤.
  - **ë§Œì•½ $y_i$ê°€ 0ì¼ ë•Œ**: ì•ì— í•­ì´ 0ì´ê³  ë’¤ì˜ í•­ $-\log(1-\hat{p}_i)$ë§Œ ë‚¨ìŠµë‹ˆë‹¤. $-\log$ í•¨ìˆ˜ëŠ” 0ê³¼ 1ì‚¬ì´ì—ì„œ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ 0ì´ ë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì˜ˆì¸¡ê°’ $\hat{p}_i$ê°€ ì˜ ì˜ˆì¸¡ëœë‹¤ë©´ (0ì— ê°€ê¹Œìš°ë©´) ì†ì‹¤í•¨ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.
  - **ë§Œì•½ $y_i$ê°€ 1ì¼ ë•Œ**: ë’¤ì— í•­ì´ 0ì´ê³  ì•ì˜ í•­ $-\log(\hat{p}_i)$ë§Œ ë‚¨ìŠµë‹ˆë‹¤. ì˜ˆì¸¡ê°’ $\hat{p}_i$ê°€ 1ì— ê°€ê¹Œìš°ë©´ ì†ì‹¤í•¨ìˆ˜ í•­ì´ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.

ìˆ˜ì‹ì´ ë³µì¡í•˜ê²Œ ìƒê²¨ ì´í•´ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆì§€ë§Œ, ê²½ìš°ë¥¼ ë‚˜ëˆ ì„œ ë”°ì ¸ë³´ë©´ ê·¸ë ‡ê²Œ ì–´ë µì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ì œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì•Œì•˜ìœ¼ë‹ˆ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë¯¸ë¶„ì„ êµ¬í•´ë´…ì‹œë‹¤.

#### 2.2 BCEì˜ ë¯¸ë¶„
ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµì„ ìœ„í•œ ì´ì§„í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ì˜ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.
$$
\nabla_\mathbf{\theta} = \frac{1}{m} \mathbf{X}^\top (\hat{\mathbf{p}} - \mathbf{y}),
\nabla_\mathbf{b} = \frac{1}{m} \mathbf{1}^\top (\hat{\mathbf{p}} - \mathbf{y})
$$

### 3. ëª¨ë¸ í‰ê°€í•˜ê¸°
ì„ í˜• íšŒê·€ì—ì„œ ì‚¬ìš©í•œ ì†ì‹¤í•¨ìˆ˜ í‰ê· ì œê³±ì˜¤ì°¨(MSE)ëŠ” ê·¸ ìì²´ë¡œ ì†ì‹¤í•¨ìˆ˜ì´ë©° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸°ì—ë„ ë¬¸ì œê°€ ì—†ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ì§„í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ëŠ” ê·¸ ê°’ì„ ë³´ê³  ì§ê´€ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì–´ëŠ ì •ë„ ë˜ëŠ”ì§€ ì‚¬ëŒì´ í•œ ëˆˆì— í‰ê°€í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.

ë³´í†µ ë¶„ë¥˜ë¬¸ì œë¥¼ í‰ê°€í•˜ëŠ”ë° ë„ë¦¬ ì“°ì´ëŠ” ê°’ì€ ì •í™•ë„(Accuracy)ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì˜ ë¹„ìœ¨ì„ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì •í™•ë„ëŠ” ì—¬ëŸ¬ ê°€ì§€ë¡œ ì·¨ì•½ì„±ì´ ë†’ì€ í‰ê°€ì§€í‘œì´ê¸° ë•Œë¬¸ì— ì¶”í›„ì— ë¶„ë¥˜ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ì—¬ëŸ¬ ê°€ì§€ ì§€í‘œì— ëŒ€í•´ ë°°ìš´ ë’¤ì— ì‚¬ìš©í•´ë³´ë„ë¡ í•©ì‹œë‹¤.

### ğŸ§‘â€ğŸ’» **ì‹¤ìŠµ**: ë¡œì§€ìŠ¤í‹± íšŒê·€ ìˆ˜í–‰í•˜ê¸°

#### 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° & ì „ì²˜ë¦¬í•˜ê¸°

ì´ë²ˆì—ëŠ” seabornì˜ iris datasetì„ ë¶ˆëŸ¬ì„œ ì „ì²˜ë¦¬í•´ë´…ì‹œë‹¤. ì•ì„  ì‹¤ìŠµì—ì„œ ì§„í–‰í•œëŒ€ë¡œ ìˆ˜í–‰í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. í•´ë‹¹ ë°ì´í„°ì…‹ì€ `sepal_length`, `sepal_width`, `petal_length`, `petal_width`ë¼ëŠ” íŠ¹ì„±ì„ í†µí•´ ê½ƒì˜ í’ˆì¢… (`species`)ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ë°°ìš´ ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì´ì§„ë¶„ë¥˜ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— `iris` ë°ì´í„°ì…‹ì— í¬í•¨ëœ 3ê°€ì§€ ì¢…ë¥˜ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `species`ì˜ `virginica`ë¥¼ 1, ë‚˜ë¨¸ì§€ ë‘ ì¢…ë¥˜ë¥¼ 0ìœ¼ë¡œ ë‘ê³  í•™ìŠµì„ ì§„í–‰í•´ë´…ì‹œë‹¤.
"""

import numpy as np
import seaborn as sns

# TODO: ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
df = sns.load_dataset("iris")
df.info()
df.head(5)

# Featureë§Œ í¬í•¨ëœ Xì™€ ë¼ë²¨ê°’ì¸ yë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”
# yëŠ” 0ê³¼ 1ë¡œ êµ¬ì„±ë˜ì–´ì•¼í•˜ë©°, virginicaê°€ 1ì´ ë˜ë„ë¡ êµ¬ì„±í•´ì£¼ì„¸ìš”.
X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = (df['species'] == 'virginica').astype(int).values
# boolean ë°°ì—´ë¡œ virginicaë©´ 1 ì•„ë‹ˆë©´ 0

# TODO: ë°ì´í„° ì„ì–´ì£¼ê¸°
# iris ë°ì´í„°ëŠ” speciesì— ë”°ë¼ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìˆœì„œë¥¼ ì„ì–´ì£¼ê³  í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# ìˆœì„œë¥¼ ì„ì€ í›„ì— ì „ì²´ ë°ì´í„°ì˜ 80%ë¥¼ í•™ìŠµë°ì´í„°, ë‚˜ë¨¸ì§€ 20% ë°ì´í„°ë¥¼ ê²€ì¦ë°ì´í„°ë¡œ ì‚¬ìš©í•´ë´…ì‹œë‹¤.
# ê° ë°ì´í„°ëŠ” X_train, y_train / X_valid, y_validì— í• ë‹¹í•´ì¤ì‹œë‹¤.
# HINT: https://numpy.org/doc/2.2/reference/random/generated/numpy.random.permutation.html
indices = np.random.permutation(len(X))
split_idx = int(len(X) * 0.8)

X_train = X[indices[:split_idx]]
X_valid = X[indices[split_idx:]]
y_train = y[indices[:split_idx]]
y_valid = y[indices[split_idx:]]


# TODO: ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°
# ì•ì„  ì‹¤ìŠµì—ì„œ ë°°ìš´ í‘œì¤€í™”ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”. ê°™ì€ ë³€ìˆ˜ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
# ê·¸ëŸ°ë° ê²€ì¦ë°ì´í„° X_validëŠ” í•™ìŠµë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê°’ì— ë§ì¶°ì„œ í‘œì¤€í™”ë¥¼ ì§„í–‰í•´ì•¼í•©ë‹ˆë‹¤.
# ì´ëŠ” ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ìƒˆë¡­ê²Œ ë“¤ì–´ì˜¤ëŠ” ê²€ì¦ë°ì´í„°ì˜ í†µê³„ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
# ì´ë²ˆì— ì ˆí¸í•­ì€ í•™ìŠµë˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ ë°ì´í„°ì— ì¶”ê°€í•˜ì§€ ì•Šê³  ì‚¬ìš©í•´ë´…ì‹œë‹¤.
mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0)

X_train = (X_train - mu) / sigma
X_valid = (X_valid - mu) / sigma

"""#### 2. ëª¨ë¸ í•™ìŠµí•˜ê¸°
ì¢‹ìŠµë‹ˆë‹¤. ì´ì œ ë°ì´í„°ê°€ í•™ìŠµì— ë“¤ì–´ê°€ê¸°ì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµ ë¶€ë¶„ì€ ì•ì—ì„œ ë°°ìš´ ì‹¤ìŠµê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ê³¼ì •ì„ ë‹¤ì‹œ ë³µìŠµí•´ë´…ì‹œë‹¤.

1. í˜„ì¬ ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ì˜ˆì¸¡ê°’ ë§Œë“¤ê¸°
  - ì„ í˜• íšŒê·€ì—ì„œëŠ” ë§¤ê°œë³€ìˆ˜ì™€ ì…ë ¥ê°’ì˜ í–‰ë ¬ê³±ë§Œ ì§„í–‰í•´ì£¼ë©´ ë˜ì—ˆëŠ”ë°ìš”, ì„ í˜• íšŒê·€ëŠ” í–‰ë ¬ê³± ê°’ì„ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
2. ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ ì†ì‹¤í•¨ìˆ˜ ê³„ì‚°í•˜ê¸°
  - MSEê°€ ì•„ë‹Œ ì´ì§„í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
3. 2ì—ì„œ ê³„ì‚°í•œ ì†ì‹¤ê°’ì— ëŒ€í•œ ë¯¸ë¶„ ê³„ì‚°í•˜ê¸°
  - ë¯¸ë¶„ì€ ìœ„ì˜ ìˆ˜ì‹ìœ¼ë¡œë¶€í„° $\frac{1}{m} \mathbf{X}^\top (\hat{\mathbf{p}} - \mathbf{y})$ ì…ë‹ˆë‹¤. ì—¬ê¸°ì— ë§ì¶°ì„œ ê³„ì‚°í•´ì£¼ì„¸ìš”. ìƒìˆ˜í•­ì˜ ë¯¸ë¶„ì€ $\frac{1}{m} \mathbf{1}^\top (\hat{\mathbf{p}} - \mathbf{y})$ ì…ë‹ˆë‹¤.
4. 3ì˜ ë¯¸ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸í•˜ê¸°
"""

from typing import Tuple, List


def sigmoid(z: np.ndarray) -> np.ndarray:
    # ì•ˆì •ì„± í–¥ìƒ: í° ìŒìˆ˜/ì–‘ìˆ˜ì—ì„œ overflow ë°©ì§€
    z = np.clip(z, -500, 500)
    return 1.0 / (1.0 + np.exp(-z))


# TODO: í•™ìŠµë°ì´í„°ì™€ ë§¤ê°œë³€ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í™•ë¥ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
def predict_proba(
    W: np.ndarray,
    b: np.ndarray,
    X: np.ndarray
  ) -> np.ndarray:
    z = X @ W + b
    return sigmoid(z)


# TODO: Logistic regressionì„ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.
# ì´ë²ˆì—ëŠ” ë°°ì¹˜í•™ìŠµì´ë‚˜ Gradient accumulationì„ ë„£ì§€ ì•Šê³  ê°€ì¥ ê¸°ë³¸ì ì¸ êµ¬í˜„ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”.
# lrì€ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ í•™ìŠµë¥  alphaì— í•´ë‹¹í•©ë‹ˆë‹¤.
# epochsëŠ” ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ ì „ì²´ ë°˜ë³µíšŸìˆ˜ iterationsì— í•´ë‹¹í•©ë‹ˆë‹¤.
# ì‹¤ë¬´ì—ì„œ ë” ë§ì´ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤.
def train_logistic_regression(
    X: np.ndarray,
    y: np.ndarray,
    lr: float = 0.1,
    epochs: int = 300,
    eps: float = 1e-12
  ) -> Tuple[np.ndarray, np.ndarray, List[float]]:
    # ë§¤ê°œë³€ìˆ˜ ì´ˆê¸°í™”
    m, n = X.shape
    W = np.zeros(n)
    b = 0.0

    losses = []
    for ep in range(0, epochs):
        # TODO: ì˜ˆì¸¡ê°’ ê³„ì‚° ë° ì†ì‹¤í•¨ìˆ˜ ê³„ì‚°í•˜ê¸°
        # ì˜ˆì¸¡ê°’ pë¥¼ ìœ„ì—ì„œ êµ¬í˜„í•œ `predict_proba`ë¥¼ ì‚¬ìš©í•´ êµ¬í˜„í•´ì£¼ì„¸ìš”.
        # pë¥¼ í†µí•´ Binary-cross-entropyë¥¼ `loss`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
        # logëŠ” ì–‘ìˆ˜ê°’ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # pê°€ ì™„ì „íˆ 0ì´ ë˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¯¸ëŸ‰ì˜ ê°’ epsë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.
        p = predict_proba(W, b, X)
        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))
        losses.append(loss)

        # TODO: ë¯¸ë¶„ ê³„ì‚°í•˜ê¸°
        # ìœ„ì—ì„œ ì„¤ëª…í•œ ìˆ˜ì‹ì— ë§ì¶° Wì™€ bì— í•´ë‹¹í•˜ëŠ” ë¯¸ë¶„ gW, gbë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.
        diff = p - y  # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´
        gW = (1 / m) * (X.T @ diff)  # Wì˜ ê·¸ë˜ë””ì–¸íŠ¸: (1/m) * X^T Â· (p - y)
        gb = (1 / m) * np.sum(diff)  # bì˜ ê·¸ë˜ë””ì–¸íŠ¸: (1/m) * 1^T Â· (p - y)

        # ìœ„ì—ì„œ ê³„ì‚°í•œ ë¯¸ë¶„ì„ í†µí•´ W, bë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”.
        W = W - lr * gW  # ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ W ì—…ë°ì´íŠ¸
        b = b - lr * gb  # ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ b ì—…ë°ì´íŠ¸

    return W, b, losses

# ìœ„ì—ì„œ ì§  ì½”ë“œëŠ” ì˜ ì‘ë™í• ê¹Œìš”? í•œ ë²ˆ ê²€ì¦í•´ë´…ì‹œë‹¤!
W, b, losses = train_logistic_regression(X_train, y_train, lr=0.1, epochs=300)

def accuracy(W, b, X, y):
    prediction = predict_proba(W, b, X) > 0.5
    return (prediction == y).mean()

print(f"Train Acc: {accuracy(W, b, X_train, y_train):.4f}")
print(f"Valid Acc: {accuracy(W, b, X_valid,  y_valid):.4f}")

plt.figure(figsize=(7,4))
plt.plot(losses, linewidth=2)
plt.xlabel("Epochs")
plt.ylabel("Train Loss")
plt.title("Convergence of Binary Cross-Entropy")
plt.show()

"""#### 3. ìš”ì†Œë¥¼ ì¶”ê°€í•˜ê¸°
ìš°ë¦¬ëŠ” ì„±ê³µì ìœ¼ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ êµ¬í˜„ ë° í•™ìŠµì„ í•´ëƒˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— ì•ì„œ ë°°ìš´ ë‚´ìš©ë“¤ê³¼ ìƒˆë¡œìš´ ë‚´ìš© í•œ ê°€ì§€ë¥¼ ì¶”ê°€í•´ë´…ì‹œë‹¤.

1. **Mini-Batch Training**: ì•ì„  ì‹¤ìŠµì—ì„œ ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë¯¸ë¶„ ê³„ì‚°ì´ ì•„ë‹ˆë¼, mini-batchì— ëŒ€í•´ì„œ ê³„ì‚°ì„ ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ êµ¬í˜„í•´ë´…ì‹œë‹¤.
2. **Gradient Accumulation**: ë©”ëª¨ë¦¬ë‚˜ í•˜ë“œì›¨ì–´ ì œì•½ìœ¼ë¡œ í•œ ë²ˆì— í° ë°°ì¹˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì´ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ëŠ” ì ê¸° ë•Œë¬¸ì— ë‘ ê¸°ë²• ëª¨ë‘ êµ³ì´ ì‚¬ìš©í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, ì—°ìŠµì°¨ì›ì—ì„œ ì§„í–‰í•´ë´…ì‹œë‹¤.
3. **Early Stopping**: MSEì˜ ê²½ìš° 0ì— ê°€ê¹Œì›Œì§€ë©´ ì¡°ê¸° ì¢…ë£Œë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. BCEë„ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìˆ˜ë ´í•˜ì§€ë§Œ, ë³´í†µ 0ê¹Œì§€ ê°€ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì ˆëŒ€ì ì¸ ê°’ì´ ì•„ë‹ˆë¼ ìƒëŒ€ì ì¸ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¡°ê¸°ì¢…ë£Œ ì¡°ê±´ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤.
4. **L2-Regularization**: í•™ìŠµí•˜ë‹¤ë³´ë©´ ê°€ì¤‘ì¹˜ì˜ ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ ë°œì‚°í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í•™ìŠµë¥ ì„ í†µí•´ ì¡°ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì†ì‹¤í•¨ìˆ˜ì— ì¡°ì¹˜ë¥¼ ì·¨í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
  - ëŒ€í‘œì ì¸ ë°©ë²•ì´ L2-ì •ê·œí™”ì…ë‹ˆë‹¤. ì‰½ê²Œ ì„¤ëª…í•˜ë©´ ë§¤ê°œë³€ìˆ˜ì˜ L2-norm í¬ê¸°ì— ì œí•œì„ ë‘ì–´ ì´ë¥¼ ì†ì‹¤í•¨ìˆ˜ì— ì¶”ê°€í•˜ì—¬ ì¡°ì ˆí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
  - ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµì—ì„œëŠ” BCE í•¨ìˆ˜ì— ì•„ë˜ ìˆ˜ì‹ì— ìˆëŠ” L2-normì„ ì¶”ê°€í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  $$
  \frac{\lambda}{2} |\mathbf{w}|^2 = \frac{\lambda}{2} \sum_j w_j^2
  $$
  - $\lambda$ëŠ” ê°€ì¤‘ì¹˜ì˜ ì œê³±í•© í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ëŠ” ì´ˆë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.
  - ì´ë¥¼ ì¶”ê°€í•œ ì†ì‹¤í•¨ìˆ˜ì™€ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  $$L(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\hat{p}_i) + (1 - y_i)\log(1 - \hat{p}_i) \right] + \frac{\lambda}{2} \|\mathbf{w}\|_2^2$$
  $$\nabla_{\mathbf{w}} = \frac{1}{m} \mathbf{X}^\top (\hat{\mathbf{p}} - \mathbf{y}) + \lambda \mathbf{w}$$
  $$\nabla_b = \frac{1}{m} \sum_{i=1}^m (\hat{p}_i - y_i)$$
  - ì†ì‹¤í•¨ìˆ˜ì™€ Wì— ëŒ€í•œ ë¯¸ë¶„í•­ì— ì‚´ì§ ë³€í™”ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¸í•˜ì—¬ ì½”ë“œì— ë°˜ì˜í•´ì£¼ì„¸ìš”.
"""

# TODO
from typing import Optional

def train_logistic_regression_improve(
    X: np.ndarray,
    y: np.ndarray,
    lr: float = 0.1,
    epochs: int = 300,
    eps: float = 1e-12,
    # ìƒˆë¡­ê²Œ ì¶”ê°€ëœ arguments
    batch_size: Optional[int] = None,   # None ì´ë©´ full batch
    l2: float = 0.0,                    # L2 ê°•ë„ (0.0 ì´ë©´ off)
    accumulate_n: int = 1,              # ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
    patience: int = 0                   # early stopping ìœˆë„ìš°, 0 ì´ë©´ off
) -> Tuple[np.ndarray, float, List[float]]:
    m, n = X.shape
    W = np.zeros(n)
    b = 0.0
    losses: List[float] = []

    best_loss = np.inf
    no_improve = 0

    # full-batch ëª¨ë“œ ì„¤ì •
    if batch_size is None or batch_size > m:
        batch_size = m

    for ep in range(epochs):
        # shuffle
        idx = np.random.permutation(m)
        Xs, ys = X[idx], y[idx]

        # ëˆ„ì  ë²„í¼
        gW_acc = np.zeros_like(W)
        gb_acc = 0.0
        cnt = 0

        # mini-batch loop
        for start in range(0, m, batch_size):
            xb = Xs[start:start+batch_size]
            yb = ys[start:start+batch_size]

            # TODO: ì˜ˆì¸¡ê°’ ê³„ì‚° ë° ì†ì‹¤í•¨ìˆ˜ ê³„ì‚°í•˜ê¸°
            # ì˜ˆì¸¡ê°’ pë¥¼ ìœ„ì—ì„œ êµ¬í˜„í•œ `predict_proba`ë¥¼ ì‚¬ìš©í•´ êµ¬í˜„í•´ì£¼ì„¸ìš”.
            # pë¥¼ í†µí•´ Binary-cross-entropyë¥¼ `loss`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
            # ì—¬ê¸°ì„œ l2-normì„ ì¶”ê°€í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
            # ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ëŠ” gWì—ë§Œ ê³„ì‚°í•´ì£¼ë©´ ë©ë‹ˆë‹¤.
            p = predict_proba(W, b, xb)
            p = np.clip(p, eps, 1 - eps)
            loss_b = -np.mean(yb * np.log(p) + (1 - yb) * np.log(1 - p))

            # TODO: ë¯¸ë¶„ ê³„ì‚°í•˜ê¸°
            # ìœ„ì—ì„œ ì„¤ëª…í•œ ìˆ˜ì‹ì— ë§ì¶° Wì™€ bì— í•´ë‹¹í•˜ëŠ” ë¯¸ë¶„ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”.
            # L2-normì„ ì¶”ê°€í•´ì£¼ì„¸ìš”
            diff = p - yb
            gW = (xb.T @ diff) / batch_size + l2 * W
            gb = np.sum(diff) / batch_size


            # TODO: Gradient Accumulation êµ¬í˜„í•˜ê¸°
            # gW_acc, gb_accì— gW, gbë¥¼ ëˆ„ì í•˜ê³ 
            # ì§€ì •ëœ `accumulate_n`ì— ë„ë‹¬í•˜ë©´ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”.
            gW_acc += gW
            gb_acc += gb
            cnt += 1
            if cnt == accumulate_n:
                W -= lr * (gW_acc / cnt)
                b -= lr * (gb_acc / cnt)


        # ë‚¨ì€ ëˆ„ì ë¶„ ì—…ë°ì´íŠ¸
        if cnt > 0:
            W -= lr * (gW_acc / cnt)
            b -= lr * (gb_acc / cnt)

        # TODO:
        # í•œ epoch ëë‚œ ë’¤ ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ loss ê³„ì‚°í•´ì£¼ì„¸ìš”.
        # ì´ëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•˜ì—¬ ê¸°ë¡í•´ë„ ë˜ê³ 
        # mini-batch loopë¥¼ ëŒ ë•Œ ê°’ì„ ëª¨ì•„ì„œ í‰ê· ì„ ë‚´ë„ ë©ë‹ˆë‹¤.
        p_all = predict_proba(W, b, X)
        p_all = np.clip(p_all, eps, 1 - eps)
        epoch_loss = -np.mean(y * np.log(p_all) + (1 - y) * np.log(1 - p_all))

        if l2 > 0:
            # Epoch ì¤‘ì— L2-normì„ lossì— ì¶”ê°€í•˜ì§€ ì•Šê³  ë¯¸ë¶„ê³„ì‚°ì—ì„œë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
            # ë¶ˆí•„ìš”í•œ ê³„ì‚°ì„ ì¤„ì¸ ê²ƒì¸ë°ìš”, ì‹¤ì œë¡œ ë¡œê¹…í•  ë•ŒëŠ” L2-normì„ í•©í•´ì„œ ê¸°ë¡í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
            epoch_loss += (l2 / 2) * np.sum(W ** 2)

        losses.append(epoch_loss)

        if patience > 0:
            # TODO: Early Stopping (train-loss ê¸°ì¤€)
            # `patience`ê°€ ì£¼ì–´ì§€ë©´ lossê°€ patienceë§Œí¼ì˜ epochë¥¼ ì§„í–‰í•˜ëŠ” ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì¢…ë£Œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
            # ë³´í†µ Local-minimaì— ë¹ ì§€ê±°ë‚˜ í•™ìŠµì´ ë” ì´ìƒ ì˜ë˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ì— í•´ë‹¹í•©ë‹ˆë‹¤.
            # ì•ì—ì„œ ì •ì˜ëœ `no_improve` ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•´ì£¼ì„¸ìš”.
            pass

    return W, b, losses

"""êµ¬í˜„ì´ ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³¼ê¹Œìš”? ì•„ë˜ ì½”ë“œë¡œ ìˆ˜í–‰í•´ë³´ì„¸ìš”."""

# ìœ„ì—ì„œ ì§  ì½”ë“œëŠ” ì˜ ì‘ë™í• ê¹Œìš”? í•œ ë²ˆ ê²€ì¦í•´ë´…ì‹œë‹¤!
W, b, losses = train_logistic_regression_improve(X_train, y_train,
                                                 lr=0.02,
                                                 epochs=100,
                                                 l2=1, accumulate_n=5)

def accuracy(W, b, X, y):
    prediction = predict_proba(W, b, X) > 0.5
    return (prediction == y).mean()

print(f"Train Acc: {accuracy(W, b, X_train, y_train):.4f}")
print(f"Valid Acc: {accuracy(W, b, X_valid,  y_valid):.4f}")

plt.figure(figsize=(7,4))
plt.plot(losses, linewidth=2)
plt.xlabel("Epochs")
plt.ylabel("Train Loss")
plt.title("Convergence of Binary Cross-Entropy")
plt.show()

"""ì´ˆë§¤ê°œë³€ìˆ˜ë¥¼ ë°”ê¿”ê°€ë©´ì„œ lossê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤!"""

from itertools import product

# ì‹¤í—˜ ì¡°í•© ì„¤ì • (í•™ìŠµë¥ , L2, ëˆ„ì  ìŠ¤í…)
lrs = [0.1, 0.01]
l2s = [0.0, 1.0]
accs = [1, 5]
experiments = list(product(lrs, l2s, accs))

plt.figure(figsize=(10, 6))

for lr, l2, acc_n in experiments:
    W, b, losses = train_logistic_regression_improve(
        X_train, y_train,
        lr=lr,
        l2=l2,
        accumulate_n=acc_n,
        epochs=300,
        patience=0
    )
    # Validation dataì— ëŒ€í•œ ì •í™•ë„ ê³„ì‚°
    acc = accuracy(W, b, X_valid, y_valid)
    label = f"lr={lr:.2f}, l2={l2:.2f}, acc_n={acc_n}, Accuracy={acc*100:3.1f}%"
    plt.plot(losses, label=label, linewidth=2)

plt.xlabel("Epochs", size="large")
plt.ylabel("Train Loss", size="large")
plt.title("Loss Curve across different hyperparameters", size="x-large")
plt.legend(title="Hyperparameter Configuration", bbox_to_anchor=(1, 1))
plt.show()

"""# ë§ˆì¹˜ë©° ...

ì˜¤ëŠ˜ì€ ì„±ê³µì ìœ¼ë¡œ Logistc Regressionì„ í†µí•œ ë¶„ë¥˜ê³¼ì œë¥¼ ì§„í–‰í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ ì˜¤ëŠ˜ ë°°ìš´ ê³¼ì •ë“¤ì€ High-level frameworkë¡œ ëª‡ ì¤„ë¡œë„ êµ¬í˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì½”ë“œ í•œ ì¤„ì—ì„œ ëŒì•„ê°€ëŠ” ë‚´ìš©ì„ ì•Œê³  ì“°ëŠ” ê²ƒê³¼ ëª¨ë¥´ê³  ì“°ëŠ” ê²ƒì€ ì‘ìš©ë ¥ì—ì„œ í° ì°¨ì´ê°€ ìƒê¹ë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤ :)

### Further Readings
- Logistic Regression From Scratch in Python NumPy: ë²¡í„° ì—°ì‚°ë§Œìœ¼ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. êµ¬í˜„ ì„¸ë¶€ ì‚¬í•­ì„ í•˜ë‚˜í•˜ë‚˜ ì§šì–´ ë³´ë©°, ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°ê³¼ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ì–´ë–»ê²Œ ì´ë¤„ì§€ëŠ”ì§€ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2/

- Gradient Descent Cheatsheet & Learning Rate Schedules: ë‹¤ì–‘í•œ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„(ê³ ì •, ë‹¨ê³„ì  ê°ì†Œ, ì§€ìˆ˜ ê°ì†Œ ë“±)ê³¼ ê·¸ íš¨ê³¼ë¥¼ ì •ë¦¬í•œ ê°€ì´ë“œì…ë‹ˆë‹¤. Mini-batch Gradient Descent ì™¸ì— ë‹¤ë¥¸ ìµœì í™” ë°©ë²•ì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ì•Œì•„ë´…ì‹œë‹¤.
  - https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
  

### Open-ended Mission

- **Multi-Class Softmax Regression êµ¬í˜„**: ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì™€ êµì°¨ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ë„ì…í•´ Irisì˜ ì„¸ ê°€ì§€ ì¢…(ì…‹) ëª¨ë‘ ë¶„ë¥˜í•˜ê¸°
- **2D ì˜ì‚¬ê²°ì • ê²½ê³„(Decision Boundary) ì‹œê°í™”**: ë‘ ê°œ íŠ¹ì„±(ì˜ˆ: petal_length vs petal_width)ë§Œ ì„ íƒí•´, ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ë³„ ì˜ˆì¸¡ í™•ë¥ ì„ ê³„ì‚°. ë“±ê³ ì„ (contour)ìœ¼ë¡œ ë¶„ë¥˜ ê²½ê³„ë¥¼ ê·¸ë ¤ ë³´ê³ , ëª¨ë¸ì´ ì–´ë–»ê²Œ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ í™•ì¸
"""