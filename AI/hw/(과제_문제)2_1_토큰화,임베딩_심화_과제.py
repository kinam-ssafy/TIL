# -*- coding: utf-8 -*-
"""(ê³¼ì œ-ë¬¸ì œ)2-1_í† í°í™”,ì„ë² ë”©_ì‹¬í™”_ê³¼ì œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G_DhRH2lRy2FCK3isjdS4qYMFijOa1dg

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**

1. ê³¼ì œëª…: í† í°í™”/ì„ë² ë”© ì‹¬í™” ì‹¤ìŠµ
2. í•µì‹¬ ì£¼ì œ
    1) tokenizerë¥¼ ì´ìš©í•˜ì—¬ ë‹¨ì–´ë“¤ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´
    2) í† í°í™”ëœ í† í°ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´
    3) GPT ì§ì ‘ êµ¬í˜„í•´ë³´ê¸°
3. í•™ìŠµ ëª©í‘œ
    1) í† í¬ë‚˜ì´ì €ê°€ ë¬´ì—‡ì´ê³  í† í°í™”ê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
    2) í† í°í™”ë¥¼ ì™œ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
    3) í† í°í™”ëœ í† í°ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.
    4) ì„ë² ë”© ë²¡í„°ë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–¤ ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
    5) GPTë¥¼ ì§ì ‘ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

4. í•™ìŠµ ê°œë…
    1) í† í°í™”
    2) ì„ë² ë”© ë²¡í„°
    3) GPT
  
5. í•™ìŠµ ë°©í–¥
    - ì‹¤ìŠµì€ ì•„ë˜ ë‚´ìš©ë“¤ì„ ê¹Šê²Œ íŒŒì•…í•˜ì—¬ ê°œë…ì„ ì´í•´í•˜ë©°, ì‘ìš©ë ¥ì„ ê°–ì¶”ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
      - í† í°í™”
      - ì„ë² ë”©
      - ë””ì½”ë”
    - ì‹¤ìŠµ ì½”ë“œëŠ” ì¡°êµê°€ ì§ì ‘ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ë©° í•™ìŠµí•©ë‹ˆë‹¤.
    - ìì—°ìŠ¤ëŸ½ê²Œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ì„œ ê¹Šì€ ì´í•´ë¥¼ í†µí•´ ê°œë…ì„ ì´ìš©í•˜ì—¬ í™œìš©í•  ìˆ˜ ìˆì„ ì •ë„ì˜ ê¹Šì´ ìˆëŠ” í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤.

6. ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´
    - ë°ì´í„°ì…‹ ëª… : NSMC(Naver Sentiment Movie Corpus)
    - ë°ì´í„°ì…‹ ê°œìš” : ë„¤ì´ë²„ ì˜í™” ê°ì •ë¶„ì„ ë°ì´í„°ì…‹
    - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ : CC0 1.0
    - ë°ì´í„°ì…‹ ëª… : tinyshakespeare
    - ë°ì´í„°ì…‹ ê°œìš” : ì…°ìµìŠ¤í”¼ì–´ì˜ ì‘í’ˆì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ ë°ì´í„°ì…‹
    - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ : MIT

### **Prerequisites**
```
numpy==2.0.2
pandas==2.2.2
tokenizers==0.21.4
transformers==4.55.2
plotly==6.2.0
nltk==3.9.1
sklearn==1.2.2
torch==2.8.0+cu126
```

- ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ë³µì‚¬í•´ì„œ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy>=2.0.2 pandas>=2.2.2 tokenizers>=0.21.4 transformers>=4.55.2 plotly>=6.2.0 nltk>=3.9.1 scikit-learn>=1.2.2 torch==2.8.0+cu126 --index-url https://download.pytorch.org/whl

import torch
import torch.nn as nn
import numpy as np

# ì‹œë“œ ì„¤ì •
np.random.seed(1234)
torch.manual_seed(1234)
torch.cuda.manual_seed(1234)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# 1. í† í¬ë‚˜ì´ì €

- í•™ìŠµ ëª©í‘œ
  1. í† í¬ë‚˜ì´ì €ê°€ ë¬´ì—‡ì´ê³  í† í°í™”ê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
  2. í† í°í™”ë¥¼ ì™œ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë…
  1. í† í¬ë‚˜ì´ì €
  2. í† í°í™”
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. BPE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•˜ëŠ”ì§€ êµ¬í˜„
  2. BPE í† í¬ë‚˜ì´ì €ë¥¼ ì§ì ‘ í•™ìŠµ

## 1.1. Tokenizer

<blockquote>
<b>ğŸ§  Tokenizerë€?</b><br>
Tokenizerë€ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ <strong>í† í° ë‹¨ìœ„ í˜¹ì€ ì„œë¸Œì›Œë“œ ë‹¨ìœ„</strong>ë¡œ
ìª¼ê°œì£¼ëŠ” ì—­í• ì„ í•´ì£¼ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
</blockquote>

`I'm a student of SSAFY!` ë¼ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ê³  í•´ë´…ì‹œë‹¤.

  1. ë‹¨ì–´(word) ë‹¨ìœ„ë¡œ ìª¼ê°œë©´ `["I", "'", "m", "a", "student", "of", "SSAFY", "!"]`ê°€ ë©ë‹ˆë‹¤.
  2. ë°˜ë©´ì— í† í°(token) ë‹¨ìœ„ë¡œ ìª¼ê°œë©´ `["I", "'", "m", "a", "student", "of", "SS", "##AF", "##Y", "!"]`ê°€ ë©ë‹ˆë‹¤.
  3. `student`ê°™ì€ ë‹¨ì–´ë“¤ì€ í† í°(token) ë‹¨ìœ„ë¡œ ìª¼ê°œì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
  4. ë°˜ë©´ì—, `SSAFY`ëŠ” `SS, ##AF, ##Y` ì²˜ëŸ¼ í† í° ë‹¨ìœ„ë¡œ ìª¼ê°œì¡ŒìŠµë‹ˆë‹¤.
        - ì—¬ê¸°ì„œ `##`ëŠ” **ê°™ì€ ë‹¨ì–´ ì•ˆì—ì„œ ë‚˜ì™”ë‹¤**ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
        - `SS`ëŠ” í•œ ë‹¨ì–´ì˜ **ì‹œì‘ í† í°**, `##AF`ê³¼ `##Y`ëŠ” ë‹¨ì–´ì˜ ì‹œì‘ í† í° ì´í›„ì— ë‹¨ì–´ë¥¼ ì™„ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í† í°ë“¤ì…ë‹ˆë‹¤.
        - `["SS", "##AF", "##Y"]` ì´í›„ì— `.` í† í°ì´ ë‚˜íƒ€ë‚¬ê³ , `##.`ì´ ì•„ë‹ˆë¯€ë¡œ í˜„ì¬ ë‹¨ì–´ì˜ **ë í† í°**ì€ `##Y`ì…ë‹ˆë‹¤.
        <blockquote>
        <b>ğŸ¤” ìª¼ê°œëŠ” ê¸°ì¤€ì´ ëª¨í˜¸í•œê±° ê°™ì•„ìš”!</b><br>
        <b>í† í¬ë‚˜ì´ì €ë¥¼ ì–´ë–»ê²Œ ë§Œë“œëƒ(í•™ìŠµì‹œí‚¤ëƒ)</b>ì— ë”°ë¼ì„œ ìª¼ê°œì§€ëŠ” ë°©ì‹ë„ ë‹¤ì–‘í•˜ê²Œ ìª¼ê°œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤! <br>ìì„¸í•œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì— ëŒ€í•´ì„œëŠ” ë‚˜ì¤‘ì— ê³¼ì œì—ì„œ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤!</br>
        </blockquote>
  5. ì´ëŸ¬í•œ í† í°(token) í˜¹ì€ ì„œë¸Œì›Œë“œ(sub-word) ë‹¨ìœ„ëŠ” ë‹¨ì–´(word)ë³´ë‹¤ ì‘ê±°ë‚˜ ê¸€ì(character)ë³´ë‹¤ëŠ” í° ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
        - ë°˜ë©´ì—, `student`ì²˜ëŸ¼ ë‹¨ì–´ê°€ ê³§ í† í°ì´ ë˜ëŠ” ê²½ìš°ë„ ì¡´ì¬í•©ë‹ˆë‹¤.
        - ì´ëŸ¬í•œ ì°¨ì´ì ì€ ì™œ ë°œìƒí•˜ëŠ” ê±¸ê¹Œìš”? - ì´ê²ƒì€ í† í°í™”ë¥¼ í•˜ëŠ” ì´ìœ ì™€ë„ ì—°ê²°ë©ë‹ˆë‹¤.

<blockquote>
<b>ğŸ“˜ ê·¸ë ‡ë‹¤ë©´ ì‹¤ì œë¡œ í•œë²ˆ ì§„í–‰í•´ë³¼ê¹Œìš”?</b><br>
sub-word ë‹¨ìœ„ì˜ í† í¬ë‚˜ì´ì €ëŠ” BERTì˜ WordPiece Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë©ë‹ˆë‹¤.
</blockquote>
"""

# ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”

# 1. ë‹¨ì–´ ë‹¨ìœ„ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì‹¤ìŠµì„ ìœ„í•´ ê°„ë‹¨í•˜ê²Œ ì§ì ‘ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.
import re
def word_tokenize(text: str):
    lines = text.strip().split()
    tokens = []
    for line in lines:
        # ë‹¨ì–´ ë˜ëŠ” íŠ¹ìˆ˜ê¸°í˜¸(.,!? ë“±)ëŠ” ë”°ë¡œ ë¶„ë¦¬
        line_tokens = re.findall(r"\w+|[^\w\s]", line)
        tokens.extend(line_tokens)

    return tokens

# 2. í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
text = "I'm a student of SSAFY!"
tokens = word_tokenize(text)

# 3. í† í°í™”ëœ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(tokens)

from transformers import BertTokenizer

# 1. í•™ìŠµì´ ì™„ë£Œê°€ ëœ í† í¬ë‚˜ì´ì €(WordPiece Tokenizer)ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

# 2. í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
text = "I'm a student of SSAFY!"
tokens = tokenizer.tokenize(text)

# 3. í† í°í™”ëœ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(tokens)

"""## 1.2. í† í°í™”ë¥¼ í•˜ëŠ” ì´ìœ 
<blockquote>
<b>ğŸ˜² ì™œ í† í°í™”ë¥¼ í•´ì•¼ í• ê¹Œìš”?</b><br>
ê°€ì¥ í° ì´ìœ ëŠ” Out of Vocabulary (OOV) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
</blockquote>

í† í°í™”ë¥¼ í•˜ëŠ” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë“¤ë¡œ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.
1. Out of Vocabulary (OOV) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
2. ë‹¨ì–´ì˜ ì ‘ë‘ì‚¬, ì ‘ë¯¸ì‚¬ ë“± ë³€í˜•ì„ ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•´
3. ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€

ì´ëŸ¬í•œ ì´ìœ ë“¤ë¡œ ì¸í•´ í† í°í™”ë¥¼ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì¢€ë” ìì„¸íˆ ì•Œì•„ë´…ì‹œë‹¤!

### 1.2.1. Out of Vocabulary (OOV) ë¬¸ì œ

<blockquote>
<b>ğŸ“˜ Out of Vocabulary (OOV)</b><br>
OOVëŠ” í•™ìŠµëœ ë‹¨ì–´ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ë¬¸ì œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
</blockquote>

ì˜ˆë¥¼ ë“¤ì–´ë´…ì‹œë‹¤!
1. word ë‹¨ìœ„ì˜ tokenizerì—ì„œ SSAFYë¼ëŠ” ë‹¨ì–´ê°€ í•™ìŠµëœ ë‹¨ì–´ ì‚¬ì „ì— ì—†ì„ ê²½ìš° SSAFYëŠ” `[UNK]`ë¼ëŠ” special tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    - special tokenì€ ì¼ë°˜ì ì¸ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ ì•„ë‹Œ íŠ¹ì • ìƒí™©ì— ëŒ€í•œ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•œ tokenì…ë‹ˆë‹¤.
    - ì˜ˆë¥¼ ë“¤ì–´, `[CLS]`ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” token, `[SEP]`ëŠ” ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” tokenì…ë‹ˆë‹¤.
    - `[UNK]`ëŠ” unknown tokenì˜ ì•½ìì…ë‹ˆë‹¤. ì¦‰, í•™ìŠµë˜ì§€ ì•Šì€ ë‹¨ì–´ê°€ ë“±ì¥í•˜ì—¬ í•´ë‹¹ ì˜ë¯¸ë¥¼ ëª¨ë¥¸ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
    - SSAFYëŠ” ìµœê·¼ì— ìƒê¸´ ë‹¨ì–´ì´ê±°ë‚˜ ë°ˆì´ê±°ë‚˜ ë“±ë“±ì˜ ì´ìœ ë¡œ í•™ìŠµëœ ë‹¨ì–´ ì‚¬ì „ì— ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ë°˜ë©´ì—, subword ë‹¨ìœ„ì˜ tokenizerì—ì„œ SSAFYë¼ëŠ” ë‹¨ì–´ê°€ í•™ìŠµëœ ë‹¨ì–´ ì‚¬ì „ì— ì—†ì„ ê²½ìš° SSAFYëŠ” `SS, ##AF, ##Y`ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    - SSAFYëŠ” `SS, ##AF, ##Y`ë¡œ ë³€í™˜ëœ ì´ìœ ëŠ” SSAFYë¥¼ ì´ë£¨ëŠ” sub-word ë‹¨ìœ„ë¡œ ë³€í™˜í•¨ìœ¼ë¡œì¨ `[UNK]`ì´ ì•„ë‹Œ ì •ìƒì ì¸ tokenìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

### 1.2.2. ë‹¨ì–´ì˜ ì ‘ë‘ì‚¬, ì ‘ë¯¸ì‚¬ ë“± ë³€í˜•ì„ ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•´

<blockquote>
<b>ğŸ“˜ í•œêµ­ì–´ ë“±ì˜ êµì°©ì–´ íŠ¹ì§•ì„ ê°€ì§€ëŠ” ê²½ìš°</b><br>
êµì°©ì–´(agglutinative language)ë€ ë‹¨ì–´ë¥¼ êµ¬ì„±í•  ë•Œ ì–´ê·¼ì— ì ‘ë‘ì‚¬(prefix), ì ‘ë¯¸ì‚¬(suffix), ì ‘ì¤‘ì‚¬(infix) ë“±ì„ ì°¨ë¡€ëŒ€ë¡œ ì„ í˜•ì ìœ¼ë¡œ ë¶™ì—¬ ë‚˜ê°€ë©°, ê° í˜•íƒœì†Œê°€ í•˜ë‚˜ì˜ ëœ»ì´ë‚˜ ë¬¸ë²• ë²”ì£¼(ì‹œì œÂ·ê²©Â·ìˆ˜Â·ì¸ì¹­ ë“±)ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì–¸ì–´ ìœ í˜•ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
</blockquote>

ì˜ˆë¥¼ ë“¤ì–´, í•œêµ­ì–´ì˜ ê²½ìš° `ë¨¹ë‹¤`, `ë¨¹ê³ `, `ë¨¹ìœ¼ë‹ˆ` ë“± `ë¨¹-`ì´ë¼ëŠ” ë™ì‚¬ì™€ `ë‹¤`, `ê³ `, `ë‹ˆ` ë“±ì˜ ì ‘ë¯¸ì‚¬ê°€ ì°¨ë¡€ëŒ€ë¡œ ë¶™ì—¬ ë‚˜ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ íŠ¹ì§•ì„ ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•´ sub-word ë‹¨ìœ„ì˜ tokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 1.2.3. ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€
<blockquote>
<b>ğŸ“˜ ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€</b><br>
sub-word ë‹¨ìœ„ì˜ tokenizerëŠ” ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ <b>ë™ì‹œì—</b> ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</blockquote>

1. ê¸€ì(character) ë‹¨ìœ„ì˜ tokenizerëŠ” <b>ë„ˆë¬´ ë§ì€ tokenì„ ìƒì„±</b>í•˜ì—¬ ì¶”ë¡  ì†ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì˜ˆë¥¼ ë“¤ì–´, `I'm a student of SSAFY!`ë¥¼ ë³€í™˜í•˜ë©´ `I, 'm, a, s, t, u, d, e, n, t, o, f, S, S, A, F, Y, !`ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    - ì´ëŠ” ë„ˆë¬´ ë§ì€ tokenì„ ìƒì„±í•˜ì—¬ ì¶”ë¡  ì†ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ë°˜ë©´ì—, ë‹¨ì–´(word) ë‹¨ìœ„ì˜ tokenizerëŠ” <b>ë„ˆë¬´ ë§ì€ ë‹¨ì–´ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥</b>í•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì˜ˆë¥¼ ë“¤ì–´, `I'm a student of SSAFY!`ë¥¼ ë³€í™˜í•˜ë©´ `I, 'm, a, student, of, SSAFY, !`ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    - ê¸€ì(character) ë‹¨ìœ„ë³´ë‹¤ëŠ” ì¶”ë¡  ì†ë„ ì¸¡ë©´ì—ì„œ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
    - í•˜ì§€ë§Œ, ì„¸ìƒì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ë“¤(vocab table)ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ì´ëŸ¬í•œ trade-off ê´€ê³„ë¥¼ ì ì ˆíˆ í•´ê²°í•˜ê¸° ìœ„í•´ sub-word ë‹¨ìœ„ì˜ tokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ê¸€ì ë‹¨ìœ„ë³´ë‹¤ëŠ” ì¶”ë¡  ì†ë„ ì¸¡ë©´ì—ì„œ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
    - ë˜í•œ, word ë‹¨ìœ„ë³´ë‹¤ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì•¼ í•  tokenì˜ ìˆ˜ê°€ ì ì–´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
    - ë”°ë¼ì„œ, ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë™ì‹œì— ê°œì„ í•  ìˆ˜ ìˆëŠ” sub-word ë‹¨ìœ„ì˜ tokenizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ [UNK] í† í°ì´ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.
"""

# í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
text = "ë†”ë‰¸ ì´ì´ ì»„ì²Œì•ºëŠ¬ë "
tokens = tokenizer.tokenize(text)

# í† í°í™”ëœ í† í°ë“¤ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
print(tokens)

"""## 1.3. BPE ì•Œê³ ë¦¬ì¦˜
<blockquote>
<b>ğŸ§  í† í¬ë‚˜ì´ì €ëŠ” ì–´ë–¤ ì›ë¦¬ë¡œ í•™ìŠµë ê¹Œìš”?</b><br>
Byte Pair Encoding (BPE) ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
</blockquote>

ì´ˆê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°œë°œëœ í›„, GPT ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•  ë•Œ í† í°í™”ë¥¼ ìœ„í•´ OpenAIì—ì„œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. GPT, GPT-2, RoBERTa, BART ë° DeBERTaë¥¼ í¬í•¨í•œ ë§ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

ê°„ë‹¨í•œ BPE ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ë³´ë©´ì„œ í† í¬ë‚˜ì´ì €ê°€ ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ì§€ ì´í•´í•´ë³´ê² ìŠµë‹ˆë‹¤.

1. BPE í•™ìŠµì€ ì •ê·œí™” ë° ì‚¬ì „ í† í°í™” ë‹¨ê³„ê°€ ì™„ë£Œëœ í›„, ë§ë­‰ì¹˜ì— ì‚¬ìš©ëœ ê³ ìœ í•œ ë‹¨ì–´ ì§‘í•©ì„ ê³„ì‚°í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì„ êµ¬ì„±í•˜ëŠ”ë° ì‚¬ìš©ëœ ëª¨ë“  ê¸°í˜¸(ê¸€ì)ë¥¼ ë°”íƒ•ìœ¼ë¡œ vocabularyë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. ì•„ì£¼ ê°„ë‹¨í•œ ì˜ˆë¡œì„œ ë§ë­‰ì¹˜ê°€ ë‹¤ìŒ ë„¤ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤:

- í•™ìŠµí•  ë‹¨ì–´ë“¤ : `low`, `lower`, `newest`, `widest`

- ê¸°ë³¸ vocabularyëŠ” ["l", "o", "w", "e", "r", "n", "t", "s", "i", "d"]ê°€ ë©ë‹ˆë‹¤.

2. BPE ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œ ìš°ì„  ê¸€ì(í† í°)ìŒì„ ë§Œë“­ë‹ˆë‹¤. ì—¬ê¸°ì„œ "ìŒ"ì€ í•œ ë‹¨ì–´ì—ì„œ ë‘ ê°œì˜ ì—°ì†ëœ ê¸€ìì„ ì˜ë¯¸í•˜ê³  ê¸€ìëŠ” ì²˜ìŒì—ëŠ” ë‹¨ì¼ ë¬¸ìì…ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ê¸€ì ìŒì„ ë§Œë“œëŠ” ì½”ë“œë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.
"""

def get_stats(corpus):
    """ê° ë¬¸ì ìŒì˜ ë¹ˆë„ë¥¼ ê³„ì‚°"""
    pairs = {}
    for word, freq in corpus.items():
        # TODO: ë„ì–´ì“°ê¸° ë° ê³„í–‰ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìª¼ê°œëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        symbols = word.split()
        for i in range(len(symbols) - 1):
            # TODO: ê¸€ì ìŒì„ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ê¸€ììŒì€ tupleë¡œ ì €ì¥ë©ë‹ˆë‹¤.
            pair = (symbols[i], symbols[i+1])
            if pair not in pairs:
                pairs[pair] = 1
            else:
                pairs[pair] += freq

    return pairs

"""3. í† í¬ë‚˜ì´ì € í•™ìŠµ ê³¼ì •ì—ì„œ ì–´ë–¤ ë‹¨ê³„ì—ì„œë“  BPE ì•Œê³ ë¦¬ì¦˜ì€ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ì¶œí˜„í•˜ëŠ” ê¸€ì ìŒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ë¹ˆë„ ê¸€ì ìŒì´ ë³‘í•©ë˜ë©° ì´ëŸ¬í•œ ê³¼ì •ì´ ê³„ì† ë°˜ë³µë©ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ BPE ì•Œê³ ë¦¬ì¦˜ì„ ì™„ì„±í•©ë‹ˆë‹¤.
"""

def merge_vocab(pair, corpus):
    """ê°€ì¥ ë¹ˆë„ ë†’ì€ pairë¥¼ ë³‘í•©"""
    new_corpus = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)
    for word in corpus:
        new_word = word.replace(bigram, replacement)
        new_corpus[new_word] = corpus[word]

    return new_corpus

def learn_bpe(corpus, num_merges):
    """BPE ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ"""
    for i in range(num_merges):
        pairs = get_stats(corpus)
        # pairë“¤ì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´ print(pairs)ë¥¼ ì¶”ê°€í•˜ì—¬ ê·¸ ê³¼ì •ì„ í™•ì¸í•´ë³´ì„¸ìš”!

        if not pairs:
            break

        # TODO: ê°€ì¥ ë¹ˆë„ ë†’ì€ pairë¥¼ ì„ íƒí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        best = max(pairs, key=pairs.get)
        print(f"ë³‘í•© {i+1}: {best} => {''.join(best)} ({pairs[best]}ê°œ)")
        corpus = merge_vocab(best, corpus)

    return corpus

"""BPE ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ì‹¤ì œ ë³‘í•©ë˜ëŠ” ê³¼ì •ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."""

# ì´ˆê¸° corpus: ë¹ˆë„ ê¸°ë°˜, ë‹¨ì–´ëŠ” ë¬¸ì ì‚¬ì´ì— ê³µë°± ì‚½ì…
corpus = {
    'l o w': 5,
    'l o w e r': 2,
    'n e w e s t': 6,
    'w i d e s t': 3
}

# BPE ë³‘í•© ì‹¤í–‰
learned_vocab = learn_bpe(corpus, num_merges=5)

print("\nFinal Vocabulary:")
for word in learned_vocab:
    print(f"{word}: {learned_vocab[word]}")

"""ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì›í•˜ëŠ” vocabulary í¬ê¸°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì´ ì‘ì—…ì„ ê³„ì†í•©ë‹ˆë‹¤.

## 1.4. Tokenizer í•™ìŠµ
<blockquote>
<b>ğŸ“˜ Tokenizer í•™ìŠµ</b><br>
ì‹¤ì œ ì˜ˆì œ ì½”ë“œë¡œ Tokenizer í•™ìŠµì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.
</blockquote>

ìœ„ì—ì„œ BPE ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ í† í¬ë‚˜ì´ì €ê°€ ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ê¸€ìë“¤ì„ ë³‘í•©í•´ë‚˜ê°€ë©´ì„œ í•™ìŠµì„ ì§„í–‰í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

í•™ìŠµí•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” NSMC(Naver Sentiment Movie Corpus) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.
(ì‹¤ìŠµì½”ë“œë¥¼ ì§„í–‰í•˜ì…¨ë‹¤ë©´ ì´ë¯¸ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë°›ì€ ìƒíƒœì…ë‹ˆë‹¤.)
"""

!wget https://github.com/e9t/nsmc/raw/master/ratings.txt

"""ë°ì´í„°ì…‹ì„ í™•ì¸í•´ë´…ë‹ˆë‹¤."""

import pandas as pd
import os

file_list = os.listdir()
for file in file_list:
    if "ratings.txt" == file:
        print('í•™ìŠµì— í•„ìš”í•œ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤!', file)
        df = pd.read_table( (os.getcwd() + '/' + file), encoding='utf-8') # ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³´ê¸° í¸í•˜ê²Œ ë°”ê¿”ì¤ì‹œë‹¤!
        df = df.dropna(how = 'any') # ë„ê°’ì„ ì—†ì• ì¤ë‹ˆë‹¤!
        print('ë¦¬ë·° ê°¯ìˆ˜ :', len(df))
df.head()

"""í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆëŠ” 'document'ì—´ë§Œì„ ê°€ì ¸ì˜¤ê³ 

í•´ë‹¹ ë°ì´í„°ë¥¼ txt íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

with open((os.getcwd() + '/' + 'naver_review.txt'), 'w', encoding='utf8') as f:
    f.write('\n'.join(df['document']))

"""í•™ìŠµì´ ë˜ì–´ ìˆì§€ ì•Šì€ ë¹ˆ tokenizer ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ì´í•´ë„ë¥¼ ë•ê¸° ìœ„í•´ CharBPETokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (OpenAIëŠ” ByteLevelBPETokenizerì…ë‹ˆë‹¤.)
CharBPETokenizerë¥¼ ì´ˆê¸°í™”í•  ë•Œ íŒŒë¼ë¯¸í„°ë“¤ì„ ì„¤ì •í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.

#### ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°
- `suffix (default : </w>)`: `</w>`ë¼ëŠ” ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ì„ í‘œì‹œí•˜ëŠ” special tokenì„ ì¶”ê°€í•©ë‹ˆë‹¤
- `split_on_whitespace_only (default: False)` : BPEëŠ” '</w>'ë¥¼ ëœ»í•˜ëŠ” ë‹¨ì–´ì˜ êµ¬ë³„ì„ ë„ì–´ì“°ê¸°ì™€ ë§ˆì¹¨í‘œ'.'ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë³„í•©ë‹ˆë‹¤. ì´ ì„¤ì •ì„ Trueë¡œ í•˜ë©´ ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ '</w>'ì„ êµ¬ë³„í•´ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.

#### í•™ìŠµ íŒŒë¼ë¯¸í„°
- `vocab_size (default: 30000)` : ë‹¨ì–´ì‚¬ì „ í¬ê¸°ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë– í•œ ê°’ì´ ê°€ì¥ ì¢‹ë‹¤ëŠ” ê²ƒì€ ì—†ì§€ë§Œ, ê°’ì´ í´ìˆ˜ë¡ ë§ì€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‹´ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `min_frequency (default: 2)` : ìµœì†Œ ë¹ˆë„ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ ì–´ë–¤ ë‹¨ì–´ê°€ 1ë²ˆ ë‚˜ì˜¤ë©´ vocabì— ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""

from tokenizers import CharBPETokenizer

tokenizer = CharBPETokenizer(
    suffix='</w>',
    split_on_whitespace_only = True
)
tokenizer.train(
    files = 'naver_review.txt',
    vocab_size = 30000,
    min_frequency = 1,
)

vocab = tokenizer.get_vocab()
print(sorted(vocab, key=lambda x: vocab[x]))

"""í•™ìŠµí•œ í† í¬ë‚˜ì´ì €ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ë–»ê²Œ í† í°í™”ë¥¼ í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""

text = "I'm a student of SSAFY!"

encoded = tokenizer.encode(text)
print('ğŸŒ±í† í°í™” ê²°ê³¼ :',encoded.tokens)
print('ğŸŒ±ì •ìˆ˜ ì¸ì½”ë”© :',encoded.ids)
print('ğŸŒˆë””ì½”ë”© :',tokenizer.decode(encoded.ids))

"""## 2. ì„ë² ë”©
- í•™ìŠµ ëª©í‘œ
  1. í† í°í™”ëœ í† í°ë“¤ì„ ì›Œë“œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë…
  1. ì›Œë“œ ì„ë² ë”©
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. nn.Embedding ë ˆì´ì–´(í˜¹ì€ ê°„ë‹¨í•œ dict lookup)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í† í° IDì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì¡°íšŒí•˜ëŠ” ì½”ë“œ
  2. ì„ë² ë”© ì‹œê°í™”

<blockquote>
<b>ğŸ§  ì„ë² ë”©ì´ë€?</b><br>
ì„ë² ë”©ì€ í† í°í™”ëœ í† í°ë“¤ì„ ì˜ë¯¸ë¥¼ ë³´ì¡´í•œ ì—°ì†ì ì¸ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜(ë§¤í•‘)í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
</blockquote>

ì„ë² ë”©, íŠ¹íˆ ì›Œë“œ ì„ë² ë”©(word embeddings)ì€ ë‹¨ì–´(ì—¬ê¸°ì„œëŠ” í† í°)ë¥¼ ì˜ë¯¸ë¥¼ ë³´ì¡´í•œ ì—°ì†ì ì¸ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì¢€ë” êµ¬ì²´ì ìœ¼ë¡œ ë§í•˜ë©´, ë‹¨ì–´ë“¤ì„ ë°€ì§‘ ë²¡í„°(dense vector)ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### 2.1. ë°€ì§‘ ë²¡í„°(dense vector)

ë°€ì§‘ ë²¡í„°ëŠ” ëª¨ë“  ì°¨ì›ì„ ì—°ì†ì ì¸ ì‹¤ìˆ˜ê°’ìœ¼ë¡œ í‘œí˜„í•œ ë²¡í„°ì…ë‹ˆë‹¤.

ë°€ì§‘ ë²¡í„°ì™€ ë°˜ëŒ€ë˜ëŠ” í¬ì†Œ ë²¡í„°(Sparse Vector)ì™€ ë¹„êµí•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<table>
  <thead>
    <tr>
      <th>ê°œë…</th>
      <th>ì„¤ëª…</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Dense vector</td>
      <td>ëª¨ë“  ì°¨ì›ì— ì‹¤ìˆ˜ê°’ì´ ê³¨ê³ ë£¨ ì¡´ì¬ ([0.23, -1.5, 0.88, 0.01, ...])</td>
    </tr>
    <tr>
      <td>Sparse vector</td>
      <td>ëŒ€ë¶€ë¶„ì´ 0ì´ê³ , ì¼ë¶€ ìœ„ì¹˜ì—ë§Œ ê°’ì´ ìˆìŒ (ì˜ˆ: one-hot [0, 0, 1, 0, 0, ...])</td>
    </tr>
  </tbody>
</table>


<blockquote>
<b>ğŸ§  ë°€ì§‘ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” í¬ì†Œ ë²¡í„°ë³´ë‹¤ ë” ì ì€ ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</b><br>
ì¦‰, ë°€ì§‘ ë²¡í„°ëŠ” í¬ì†Œ ë²¡í„°ë³´ë‹¤ ë” ë§ì€ ì •ë³´ë¥¼ ì ì€ ì°¨ì›ì— ë‹´ëŠ” ë²¡í„°ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</blockquote>

ê·¸ë ‡ë‹¤ë©´ ì›Œë“œ ì„ë² ë”©ìœ¼ë¡œ ì–´ë–»ê²Œ ì •ë³´ë¥¼ ë‹´ê²Œ ë˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### 2.2. ì›Œë“œ ì„ë² ë”©

ì›Œë“œ ì„ë² ë”©ì´ë€ ë‹¨ì–´ë“¤ì„ ë²¡í„°í™”í•˜ì—¬ í‘œí˜„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
ì‚¬ì „ì— í•™ìŠµëœ ì›Œë“œ ì„ë² ë”©ì„ í†µí•´ ì›Œë“œ ì„ë² ë”©ì˜ ì°¨ì›ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ë¥¼ íŒŒì•…í•´ë´…ì‹œë‹¤!

ì•„ë˜ ì½”ë“œëŠ” ë‹¤ì–‘í•œ ì˜ë¯¸ë¡œ ê·¸ë£¹ì„ ì§€ì„ ìˆ˜ ìˆëŠ” ë‹¨ì–´ë“¤ì„ ë‚˜ì—´í–ˆìŠµë‹ˆë‹¤.

- Age group : "infant", "child", "girl", "boy"
- Adult group : "man", "woman", "grandfather"
- Royal group : "prince", "princess", "king", "queen", "monarch"

ìœ„ì˜ 3ê°€ì§€ ì˜ë¯¸ë¡œ ê° ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ë¥¼ ë¬¶ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ë‹¨ì–´ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì—ˆë‹¤ëŠ” ê²ƒì€ ì•„ë˜ì™€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  ê·¸ë£¹ ë‚´ì˜ ë‹¨ì–´ë“¤ì€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</b><br>
ê° ê·¸ë£¹ì„ ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë¬¶ì–´ë‚´ê³  ê·¸ ê·¸ë£¹ë“¤ì´ ì‹¤ì œë¡œ ì„ë² íŒ… ê³µê°„ì— ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.
</blockquote>

íŠ¹íˆ ì„ë² ë”© ê³µê°„ì—ì„œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì€ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ë¥¼ ìœ ì‹¬í•˜ê²Œ í™•ì¸í•´ë³´ì„¸ìš”.
"""

words = [
    "infant", "child", "girl", "boy",       # ğŸ‘¶ Age group
    "man", "woman", "grandfather",          # ğŸ§‘ Adult group
    "prince", "princess", "king", "queen", "monarch"  # ğŸ‘‘ Royal group
]

"""<blockquote>
<b>ğŸ§  ë¨¼ì € í•™ìŠµì´ ì™„ë£Œëœ word embeddingsì˜ ê²½ìš° ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.</b><br>
ì´ë¯¸ ì˜ í•™ìŠµëœ(ì‚¬ì „í•™ìŠµëœ) word embeddingsì„ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.
</blockquote>
"""

from transformers import BertModel, BertTokenizer

# ì‚¬ì „í•™ìŠµëœ BERT í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

print("í† í¬ë‚˜ì´ì €ì˜ vocab size : ", tokenizer.vocab_size)

# word embedding êº¼ë‚´ê¸°
word_embeddings = model.embeddings.word_embeddings
print("ì›Œë“œ ì„ë² ë”©ì˜ ì°¨ì› : ", word_embeddings.weight.shape)

"""ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³¸ ê²°ê³¼ `word_embeddings`ì˜ ì°¨ì›ì€ `30522 x 768` ì…ë‹ˆë‹¤.
- í–‰ì¸ `30522`ëŠ” vocabì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
    - ì´ì „ì— ì €í¬ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•´ë³´ë©´ì„œ `vocab_size`ë¥¼ 30000ìœ¼ë¡œ ì„¤ì •í•˜ê³  í•™ìŠµì„ í–ˆìŠµë‹ˆë‹¤.
    - ì´ë²ˆì— ë¶ˆëŸ¬ì˜¨ í† í¬ë‚˜ì´ì €ëŠ” `vocab_size=30522`ì¸ í† í¬ë‚˜ì´ì €ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
    - ê·¸ë¦¬ê³  word_embeddingsì˜ í–‰ì€ vocab_sizeì™€ ë™ì¼í•©ë‹ˆë‹¤.
- ì—´ì¸ `768`ì€ ë°€ì§‘ ë²¡í„°ì—ì„œ í‘œí˜„í•˜ëŠ” ì°¨ì› ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    - ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ ê°’ëŠ” ë§ˆìŒëŒ€ë¡œ ì„¤ì •í•´ë„ ë©ë‹ˆë‹¤.
    - ì—¬ê¸°ì„œëŠ” 256 * 3 = 768ì˜ ê°’ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
    - ë³´í†µ 768, 1024, 2048 ë“± 2ì˜ ì œê³±ìˆ˜ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  ì¦‰, word_embeddingsì˜ ì°¨ì›ì€ </b><br>
ëª¨ë¸ì´ í‘œí˜„ ê°€ëŠ¥í•œ ë‹¨ì–´(í† í°)ì˜ ìˆ˜ x ë°€ì§‘ ë²¡í„°ì˜ ì°¨ì›ì…ë‹ˆë‹¤.
</blockquote>

ì°¨ì›ì€ ì •í™•íˆ ì–´ë–¤ ê°’ì´ ì¢‹ë‹¤ëŠ” ê°’ì€ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•„ë˜ì™€ ê°™ì€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤.

|  | ì°¨ì›ì´ í´ ë•Œ (embedding_dim â†‘) | ì°¨ì›ì´ ì‘ì„ ë•Œ (embedding_dim â†“) |
| --- | --- | --- |
| **í‘œí˜„ë ¥** | ë” ì •êµí•˜ê³  ë¯¸ë¬˜í•œ ì˜ë¯¸ë¥¼ í‘œí˜„ ê°€ëŠ¥ | í‘œí˜„ë ¥ ë¶€ì¡± ê°€ëŠ¥ì„± ìˆìŒ |
| **ê³„ì‚° ì†ë„** | ê³„ì‚°ì´ ëŠë¦¬ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ìŒ | ê³„ì‚°ì´ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì  |
| **ê³¼ì í•©** | ê³¼ì í•© ê°€ëŠ¥ì„± ì¦ê°€ | ê³¼ì í•© ê°€ëŠ¥ì„± ê°ì†Œ |

ê·¸ëŸ¬ë©´ ì´ì œ ì‹¤ì œ ë‹¨ì–´ë“¤ì´ ì„ë² ë”© ê³µê°„ ìƒì—ì„œ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹¨ì–´ë“¤ì„ token idë¡œ ë§¤í•‘í•œ í›„ ë§¤í•‘í•œ í† í°ë“¤ì„ ì„ë² ë”© ê³µê°„ ìƒì—ì„œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•´ë³´ì„¸ìš”.
"""

import torch

words_to_ids = {word: tokenizer.convert_tokens_to_ids(word) for i, word in enumerate(words)}
print("ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í† í°ë“¤ê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” id ê°’ë“¤ : ", words_to_ids)

input_tensor = torch.tensor([words_to_ids[word] for word in words])
print("ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í† í°ë“¤ì˜ id ê°’ë“¤ : ", input_tensor)
print("ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í…ì„œì˜ ì°¨ì› : ", input_tensor.shape)
embeddings_vector = word_embeddings(input_tensor)
print("ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› : ", embeddings_vector.shape)

"""ê²°ê³¼ë¥¼ ë³´ì‹œë©´ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì´ `30522 X 768`ì—ì„œ `12 X 768`ìœ¼ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤.
ì´ëŠ” ê° ì…ë ¥ í† í°ì— í•´ë‹¹í•˜ëŠ” `id` ê°’ì´ ì¸ë±ìŠ¤ê°€ ë˜ì–´ í•´ë‹¹ ì¸ë±ìŠ¤ í–‰ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, `10527` idë¥¼ ê°€ì§€ëŠ” í† í° `infant`ì€ `10527`ë²ˆì§¸ í–‰ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë©ë‹ˆë‹¤.

### 2.3. ì„ë² ë”© ì‹œê°í™”

ê·¸ëŸ¬ë©´ ì„ë² ë”©ì„ ì‹œê°í™”í•´ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œëŠ” ì„ë² ë”©ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.
ì½”ë“œì— ëŒ€í•œ ë¶„ì„ì„ ì§„í–‰í•˜ì§€ ì•Šì•„ë„ ì¢‹ìŠµë‹ˆë‹¤!
"""

def visualize_embeddings(embeddings_vector, words, word_colors):
    import plotly.graph_objects as go
    from sklearn.decomposition import PCA
    import numpy as np

    """
        ë‹¨ì–´ ì„ë² ë”©ì„ 3ì°¨ì› êµ¬ì˜ í˜•íƒœë¡œ ì‹œê°í™”
        ì´ ì½”ë“œëŠ” êµ³ì´ ì´í•´í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
    """

    if isinstance(embeddings_vector, torch.Tensor):
        embeddings_vector = embeddings_vector.detach().numpy()


    # 3ì°¨ì› êµ¬ì˜ í˜•íƒœë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ PCAë¡œ ì°¨ì›ì¶•ì†Œ
    reduced = PCA(n_components=3).fit_transform(embeddings_vector)
    normed = reduced / np.linalg.norm(reduced, axis=1, keepdims=True)

    x, y, z = normed[:, 0], normed[:, 1], normed[:, 2]

    # êµ¬ í‘œë©´ ì¢Œí‘œ ìƒì„±
    u = np.linspace(0, 2 * np.pi, 50)
    v = np.linspace(0, np.pi, 50)
    x_sphere = np.outer(np.cos(u), np.sin(v))
    y_sphere = np.outer(np.sin(u), np.sin(v))
    z_sphere = np.outer(np.ones(np.size(u)), np.cos(v))

    # êµ¬í˜• í‘œë©´
    sphere = go.Surface(
        x=x_sphere, y=y_sphere, z=z_sphere,
        colorscale='Greys', opacity=0.1, showscale=False
    )

    # ì  + í…ìŠ¤íŠ¸ (ìƒ‰ìƒ ë°˜ì˜)
    points = go.Scatter3d(
        x=x, y=y, z=z,
        mode='markers+text',
        marker=dict(size=6, color=word_colors),
        text=words,
        textposition='top center'
    )

    # ì›ì  â†’ ë‹¨ì–´ ë²¡í„° ì—°ê²° ì„ 
    lines = [
        go.Scatter3d(
            x=[0, x[i]], y=[0, y[i]], z=[0, z[i]],
            mode='lines',
            line=dict(width=2, color=word_colors[i]),
            showlegend=False
        )
        for i in range(len(words))
    ]

    # ë ˆì´ì•„ì›ƒ
    layout = go.Layout(
        title="3D Word Embeddings with Semantic Coloring",
        margin=dict(l=0, r=0, t=40, b=0),
        scene=dict(
            xaxis=dict(visible=False),
            yaxis=dict(visible=False),
            zaxis=dict(visible=False),
            aspectmode='cube'
        )
    )

    fig = go.Figure(data=[sphere, points] + lines, layout=layout)
    fig.show()

"""ì´ì œ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ì„ë² ë”© ê³µê°„ìƒì— í† í°ë“¤ì„ ìœ„ì¹˜ì‹œì¼œë³´ê³  ê·¸ ì˜ë¯¸ë¥¼ íŒŒì•…í•´ë´…ì‹œë‹¤!"""

# ë‹¨ì–´ë³„ ê·¸ë£¹ ë§¤í•‘
group_map = {
    "age": ["infant", "child", "girl", "boy"],
    "adult": ["man", "woman", "grandfather"],
    "royalty": ["prince", "princess", "king", "queen", "monarch"]
}

# ê° ë‹¨ì–´ì— ê·¸ë£¹ë³„ ìƒ‰ìƒ ì§€ì •
color_map = {
    "age": "red",
    "adult": "blue",
    "royalty": "green"
}

word_colors = []
for w in words:
    for group, word_list in group_map.items():
        if w in word_list:
            word_colors.append(color_map[group])
            break

visualize_embeddings(embeddings_vector, words, word_colors)

"""ìœ„ì˜ ì„ë² ë”© ê³µê°„ì„ í™•ì¸í•´ë³´ì‹œë©´ ê° ìƒ‰ê¹”ë“¤ì´ ê·¸ë£¹ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ê° ê·¸ë£¹ë“¤ì˜ ë‹¨ì–´ë“¤ì´ ë¹„ìŠ·í•œ ìœ„ì¹˜ì— ìˆëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ì„ë² ë”© ê³µê°„ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ì›Œë“œ ì„ë² ë”©ì˜ í•µì‹¬ì ì¸ ê°œë…ì…ë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  ì¢€ë” ì •í™•íˆ í‘œí˜„í•˜ìë©´, ìœ ì‚¬í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë°©í–¥ì´ ë¹„ìŠ·í•˜ê³  ë²¡í„° ê°„ ê°ë„ê°€ ì‘ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. </b><br>
ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(cosine similarity)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì—ì„œì˜ í•µì‹¬ì€ ë²¡í„° ê°„ ê°ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
</blockquote>

ìš°ë¦¬ ëˆˆì— ë³´ì´ëŠ” ê²ƒì€ ê°€ê¹Œìš´ ê±°ë¦¬ìƒì— ì¡´ì¬í•œë‹¤ê³  ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ, ë²¡í„° ìƒì—ì„œëŠ” ê±°ë¦¬ë³´ë‹¤ëŠ” ë°©í–¥ê³¼ ê°ë„ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ê¼­ ê¸°ì–µí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ìœ„ì—ì„œì˜ ì˜ˆì‹œëŠ” ì´ë¯¸ ì˜ í•™ìŠµì´ ë˜ì–´ ìˆëŠ” ì›Œë“œ ì„ë² ë”©ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

ë°˜ë©´ì— í•™ìŠµì´ ë˜ì–´ìˆì§€ ì•Šì€ ì›Œë“œ ì„ë² ë”©ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

`nn.Embedding()` ë§¤ì„œë“œë¥¼ í†µí•´ ì„ë² ë”©ì„ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë˜í•œ, í•™ìŠµëœ ì„ë² ë”©ì´ ì•„ë‹ˆë¯€ë¡œ `vocab_size` ë˜í•œ ì €í¬ê°€ ì •ì˜í•œ 12ê°œì˜ ë‹¨ì–´ë§Œ ìˆë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.
"""

# TODO: embeddings layerë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
import torch.nn as nn
embedding_layer = nn.Embedding(len(words), 768)

words_to_ids = {word: i for i, word in enumerate(words)}
print("ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í† í°ë“¤ê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” id ê°’ë“¤ : ", words_to_ids)

input_tensor = torch.tensor([words_to_ids[word] for word in words])
print("ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í† í°ë“¤ì˜ id ê°’ë“¤ : ", input_tensor)
print("ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” í…ì„œì˜ ì°¨ì› : ", input_tensor.shape)
embeddings_vector = embedding_layer(input_tensor)
print("ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› : ", embeddings_vector.shape)

"""ê²°ê³¼ë¥¼ ë³´ì‹œë©´ ì´ì „ì— í–ˆë˜ ê²ƒê³¼ ë™ì¼í•˜ê²Œ `12 X 768`ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ì´ ì„ë² ë”© ë²¡í„°ë¥¼ ì‹œê°í™”í•´ë³´ê² ìŠµë‹ˆë‹¤.
"""

# ë‹¨ì–´ë³„ ê·¸ë£¹ ë§¤í•‘
group_map = {
    "age": ["infant", "child", "girl", "boy"],
    "adult": ["man", "woman", "grandfather"],
    "royalty": ["prince", "princess", "king", "queen", "monarch"]
}

# ê° ë‹¨ì–´ì— ê·¸ë£¹ë³„ ìƒ‰ìƒ ì§€ì •
color_map = {
    "age": "red",
    "adult": "blue",
    "royalty": "green"
}

word_colors = []
for w in words:
    for group, word_list in group_map.items():
        if w in word_list:
            word_colors.append(color_map[group])
            break

visualize_embeddings(embeddings_vector, words, word_colors)

"""ì˜ í•™ìŠµëœ ì›Œë“œ ì„ë² ë”©ê³¼ ë‹¤ë¥´ê²Œ í•™ìŠµë˜ì§€ ì•Šì€ ì›Œë“œ ì„ë² ë”©ì€ ìƒëŒ€ì ìœ¼ë¡œ ê·¸ë£¹ë¼ë¦¬ ëª¨ì—¬ìˆì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì²˜ëŸ¼ í•™ìŠµë˜ì§€ ì•Šì€ ì›Œë“œ ì„ë² ë”©ìœ¼ë¡œëŠ” ê·¸ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ëª…í™•í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë˜í•œ **ì˜ í•™ìŠµëœ ì›Œë“œ ì„ë² ë”©ì€ ê° ë‹¨ì–´ë“¤ê°„ì˜ ê´€ê³„(ìœ ì‚¬í•œ ì •ë„)ë¥¼ ëª…í™•í•˜ê²Œ í‘œí˜„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì „ ì§ˆë¬¸ìœ¼ë¡œ ëŒì•„ê°€ë³´ìë©´,

<blockquote>
<b>ğŸ§  ì›Œë“œ ì„ë² ë”©ìœ¼ë¡œ ì–´ë–»ê²Œ ì •ë³´ë¥¼ ë‹´ê²Œ ë ê¹Œìš”? </b><br>
ê° ë‹¨ì–´ë“¤ì„ ë²¡í„° ìƒì˜ ì¢Œí‘œë¡œ í‘œí˜„í•˜ê³  ê·¸ ì¢Œí‘œëŠ” ë°©í–¥ê³¼ ê°ë„ë¥¼ ê°€ì§€ê²Œ ë˜ë©°, ë°©í–¥ê³¼ ê°ë„ë¡œ ë‹¨ì–´ë“¤ê°„ì˜ ê´€ê³„ ì •ë³´ë¥¼ ë‹´ê²Œ ë©ë‹ˆë‹¤.
</blockquote>

# 3. GPT êµ¬í˜„
- í•™ìŠµ ëª©í‘œ
  1. GPTë¥¼ êµ¬í˜„í•˜ì—¬ ì–´í…ì…˜ì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ íŒŒì•…í•©ë‹ˆë‹¤.
- í•™ìŠµ ê°œë…
  1. GPT
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. GPT ì‚¬ì „í•™ìŠµì„ ì‹¤ì œë¡œ ì§„í–‰í•´ë´…ë‹ˆë‹¤.

ê°„ë‹¨í•œ GPTë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ì„œ ëª¨ë¸ ì•„í‚¤í…ì³ë¥¼ ì´í•´í•´ë³´ê² ìŠµë‹ˆë‹¤.

ìµœê·¼ì— ë‚˜ì˜¤ëŠ” OpenAIì˜ GPT, Googleì˜ Gemini, Metaì˜ Llama ë“±ì€ ëª¨ë‘ ì´ëŸ¬í•œ GPTì˜ ì•„í‚¤í…ì³ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•˜ê²Œ ë³€í˜•í•˜ê±°ë‚˜ í•™ìŠµë“¤ ë‹¤ì–‘í•˜ê²Œ í•œ ëª¨ë¸ë“¤ì…ë‹ˆë‹¤.

ì¦‰, ì´ë²ˆ GPT êµ¬í˜„ ì‹¤ìŠµì„ í•´ë³¸ë‹¤ë©´ ëª¨ë¸ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë„ë¥¼ ê°€ì ¸ê°€ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì— ëŒ€í•œ ì•„í‚¤í…ì³ë¥¼ ì–´ëŠì •ë„ ì´í•´í•œ ë‹¤ìŒì•  ëª¨ë¸ì˜ íŠ¹ì„±ì„ íŒŒì•…í•œë‹¤ë©´ ë”ë”ìš± ì´í•´ë„ê°€ ë†’ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤.

configë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
"""

from dataclasses import dataclass

@dataclass
class Config:
    # The size of hidden state in transformer, also called d_model
    hidden_size: int = 512
    # The size of hidden state in MLP in the decoder block
    ff_hidden_size: int = 4 * 512
    # The number of decoder blocks
    num_hidden_layers: int = 2
    # Dropout rates for all modules that need dropout
    dropout_rate: float = 0.1
    vocab_size: int = 10000
    max_seq_len: int = 128
    num_heads: int = 1

config = Config()

"""GPT-2ëŠ” ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

![image](https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/GPT-2.png)

ì˜¤ë¥¸ìª½ attention Headì—ì„œë¶€í„° ì™¼ìª½ GPT2ìœ¼ë¡œ ì°¨ë¡€ëŒ€ë¡œ êµ¬í˜„í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

### Attention Head

![image](https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/MultiHeadAttention.png)


GPT-2 ìŠ¤íƒ€ì¼ì˜ compact multi-head causal self-attention ë¸”ë¡:
- ì…ì¶œë ¥ í˜•íƒœ: ì…ë ¥ê³¼ ì¶œë ¥ì€ ëª¨ë‘ ê°™ì€ í¬ê¸° $(B, T, H)$ì…ë‹ˆë‹¤. (ì—¬ê¸°ì„œ $B$ëŠ” ë°°ì¹˜ í¬ê¸°, $T$ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´, $H$ëŠ” hidden dimension)
- í—¤ë“œ ë¶„í• : $n_h$ê°œì˜ attention headë¡œ ìª¼ê°  ë’¤, ê° head ì°¨ì›ì€ $d = H/n_h$ê°€ ë©ë‹ˆë‹¤.
- êµ¬ì„± ìš”ì†Œ:
	1.	í•˜ë‚˜ì˜ ì„ í˜•ì¸µì—ì„œ Q/K/Vë¥¼ ë™ì‹œì— ìƒì„±
	2.	Qì™€ Kë¡œ dot-productë¥¼ ê³„ì‚°í•˜ê³  í¬ê¸°ë¥¼ $d$ë¡œ ë‚˜ëˆˆ ë’¤ softmax (attention score ê³„ì‚°)
	3.	ë¯¸ë˜ í† í°ì„ ëª» ë³´ê²Œ causal mask ì ìš©
	4.	softmax í›„ dropoutìœ¼ë¡œ ì •ê·œí™”
	5.	attention scoreë¡œ Vë¥¼ ê°€ì¤‘í•©í•˜ì—¬ context ìƒì„±
	6.	ì—¬ëŸ¬ headë¥¼ ë‹¤ì‹œ í•©ì³ concat
	7.	ìµœì¢…ì ìœ¼ë¡œ projectionê³¼ dropoutìœ¼ë¡œ ì¶œë ¥

ì¦‰, â€œí˜„ì¬ í† í°ì´ ê³¼ê±° ì •ë³´ ì¤‘ ì–´ë””ë¥¼ ì§‘ì¤‘í•´ì„œ ë³¼ì§€(attendí• ì§€)â€œë¥¼ ê³„ì‚°í•˜ê³ , ê·¸ ê°€ì¤‘ì¹˜ë¥¼ ì´ìš©í•´ Valueë¥¼ ì„ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

í‘œê¸°ë²• & í…ì„œ í¬ê¸°
- $X \in \mathbb{R}^{B\times T\times H}$: ì…ë ¥ ì‹œí€€ìŠ¤ ì„ë² ë”©
- ì´ë¥¼ $n_h$ê°œì˜ headë¡œ ë‚˜ëˆ„ë©´ ê° headëŠ” $d = H/n_h$ ì°¨ì›ì„ ê°€ì§
- head ë‹¨ìœ„ë¡œ ë³´ë©´ $Q,K,V \in \mathbb{R}^{B\times n_h\times T\times d}$

ì¦‰, ì…ë ¥ì´ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ â€œì£¼ì˜ ì§‘ì¤‘ ì¥ì¹˜(head)â€ë¡œ ë‚˜ë‰˜ì–´ ë³‘ë ¬ ê³„ì‚°ë©ë‹ˆë‹¤.

1) Q/K/V íˆ¬ì˜

```python
self.c_attn = nn.Linear(H, 3H, bias=False)
qkv = self.c_attn(x)
q, k, v = qkv.split(H, dim=-1)
```

ì…ë ¥ì„ ë°”ë¡œ Q/K/Vë¡œ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼,
í•˜ë‚˜ì˜ í° ì„ í˜•ì¸µì´ ì…ë ¥ì„ ë°›ì•„ ì„¸ ê°œì˜ í–‰ë ¬ë¡œ ë™ì‹œì— ë³€í™˜í•©ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
[Qâ€™, Kâ€™, Vâ€™] = X W_{qkv}
$$
- ì´ê²ƒì€ ê³§ $Qâ€™,Kâ€™,Vâ€™$ê°€ ê°ê° $X$ì— ëŒ€í•´ ë‹¤ë¥¸ projection($W_Q,W_K,W_V$)ì„ ì ìš©í•œ ê²°ê³¼ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.

ì´í›„ head ë‹¨ìœ„ë¡œ reshape í•´ì£¼ì–´ $Q,K,V$ë¥¼ $(B, n_h, T, d)$ í¬ê¸°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
â†’ ì´ë ‡ê²Œ í•˜ë©´ â€œê° headê°€ ìê¸°ë§Œì˜ ê´€ì ìœ¼ë¡œ ì…ë ¥ì„ ë³¸ë‹¤â€ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2) Scaled dot-product attention score

ê° headì—ì„œ, Queryì™€ Keyì˜ ë‚´ì ì„ êµ¬í•´ â€œì„œë¡œ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€â€ ì¸¡ì •í•©ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
S_h = \frac{Q_h K_h^\top}{\sqrt{d}}
$$

QueryëŠ” â€œë‚´ê°€ ì§€ê¸ˆ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤â€ë¼ëŠ” ì§ˆë¬¸ì´ê³ ,

KeyëŠ” â€œë‚´ê°€ ê°€ì§„ ì •ë³´ì˜ íŠ¹ì§•â€ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‘˜ì˜ ë‚´ì ì´ í¬ë©´, ê·¸ Keyì— í•´ë‹¹í•˜ëŠ” Valueë¥¼ ë” ë§ì´ ì°¸ê³ í•˜ê²Œ ë©ë‹ˆë‹¤.

$\sqrt{d}$ë¡œ ë‚˜ëˆ ì£¼ëŠ” ì´ìœ ëŠ” ì°¨ì›ì´ ì»¤ì§ˆìˆ˜ë¡ ë‚´ì  ê°’ì´ ì»¤ì ¸ softmaxê°€ ë¶ˆì•ˆì •í•´ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

3) Causal ë§ˆìŠ¤í¬

```python
causal_mask = tril(ones(T,T)).view(1,1,T,T)
attn_scores = attn_scores.masked_fill(causal_mask==0, -inf)
```

ì—¬ê¸°ì„œëŠ” GPT ê°™ì€ autoregressive ëª¨ë¸ì´ë¯€ë¡œ,
í˜„ì¬ í† í°ì€ ë¯¸ë˜ í† í°ì„ ì°¸ì¡°í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- ë§ˆìŠ¤í¬ ìˆ˜ì‹:
$$
M_{ij} = \begin{cases}
1, & i \ge j \
0, & i < j
\end{cases}
$$

ì¦‰, ië²ˆì§¸ ìœ„ì¹˜ëŠ” ìê¸° ìì‹ ê³¼ ê³¼ê±°ê¹Œì§€ë§Œ ë³¼ ìˆ˜ ìˆê³ , ë¯¸ë˜ jëŠ” ì°¨ë‹¨ë©ë‹ˆë‹¤.
ì½”ë“œì—ì„œëŠ” ë§ˆìŠ¤í¬ëœ ê³³ì— $-\infty$ë¥¼ ì±„ì›Œ softmax ì´í›„ í™•ë¥ ì´ 0ì´ ë˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.

4) Softmaxì™€ Dropout

```python
attn_probs = F.softmax(attn_scores, dim=-1)
attn_probs = self.attn_dropout(attn_probs)
```

ë§ˆìŠ¤í¬ëœ attention scoreì— softmaxë¥¼ ì ìš©í•˜ë©´,
ê° Queryê°€ â€œì–´ë–¤ Keyì— ì–¼ë§ˆë§Œí¼ ì§‘ì¤‘í• ì§€â€ í™•ë¥  ë¶„í¬ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
A_h = \text{softmax}(S_h^{\text{masked}})
$$

ì¦‰, í•œ í–‰(row)ì€ íŠ¹ì • í† í°ì´ ê³¼ê±° í† í°ë“¤ì„ ì–¼ë§ˆë‚˜ ì£¼ëª©í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ì´ë•Œ dropoutì„ ì ìš©í•˜ì—¬ ì¼ë¶€ attention ì—°ê²°ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í•˜ë©´ ëª¨ë¸ì´ ê³¼ì í•©ì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

5) ê°’(Value)ì˜ ê°€ì¤‘í•©

```python
attn_out = attn_probs @ v
```

ì–»ì€ attention í™•ë¥ ë¡œ Valueë¥¼ ê°€ì¤‘í•©í•˜ì—¬ context ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
Z_h = A_h V_h
$$

ì§ê´€ì ìœ¼ë¡œ ë§í•˜ë©´, â€œí˜„ì¬ ë‹¨ì–´ê°€ í•„ìš”í•œ ì •ë³´â€ë¥¼ ê³¼ê±° ë‹¨ì–´ë“¤ì˜ Valueì—ì„œ ê³¨ë¼ ì„ì–´ì˜¨ ê²°ê³¼ì…ë‹ˆë‹¤.

6) ì—¬ëŸ¬ Head ê²°í•©

```python
attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, H)
```

ê° headì—ì„œ ì–»ì€ ê²°ê³¼ $Z_h$ë“¤ì„ ë‹¤ì‹œ í•©ì³ $(B, T, H)$ í¬ê¸°ë¡œ ë§Œë“­ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
Z = \text{Concat}(Z_1, Z_2, \ldots, Z_{n_h})
$$

ì¦‰, ì—¬ëŸ¬ ê´€ì (í—¤ë“œ)ì´ ëª¨ì—¬ ìµœì¢…ì ì¸ ë§¥ë½(context)ì„ í˜•ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

7) ìµœì¢… Projectionê³¼ Dropout

```python
proj_out = self.c_proj(attn_out)
proj_out = self.proj_dropout(proj_out)
```

Concatëœ ê²°ê³¼ $Z$ëŠ” ì„ í˜•ì¸µ $W_O$ë¥¼ ê±°ì³ ë‹¤ì‹œ ëª¨ë¸ hidden dimensionìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
Y = Z W_O,\quad \tilde{Y} = \text{Dropout}(Y)
$$

ì´ ë‹¨ê³„ëŠ” â€œì—¬ëŸ¬ headì˜ ì •ë³´ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ë¡œ ì˜ ì„ëŠ” ê³¼ì •â€ì…ë‹ˆë‹¤.
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, hidden_size: int, num_heads: int, dropout_rate: float = 0.1):
        super().__init__()
        assert hidden_size % num_heads == 0, "hidden_sizeëŠ” num_headsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤."
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads

        # Q, K, Vë¥¼ í•œ ë²ˆì— ìƒì„±: (H) -> (3H)
        self.c_attn = nn.Linear(hidden_size, 3 * hidden_size, bias=False)
        # head concat í›„ ìµœì¢… íˆ¬ì˜
        self.c_proj = nn.Linear(hidden_size, hidden_size, bias=False)

        # ë“œë¡­ì•„ì›ƒ
        self.attn_dropout = nn.Dropout(dropout_rate)   # ì–´í…ì…˜ í™•ë¥ ì— ì ìš©
        self.proj_dropout = nn.Dropout(dropout_rate)   # ìµœì¢… ì¶œë ¥ì— ì ìš©

    def forward(self, x: torch.Tensor, return_attn: bool = False):
        """
        x: (B, T, H)
        return_attn=Trueì´ë©´ ì–´í…ì…˜ ë§µ(í—¤ë“œë³„)ì„ ë°˜í™˜í•©ë‹ˆë‹¤: (B, num_heads, T, T)
        """
        B, T, H = x.shape
        assert H == self.hidden_size, f"hidden_size ë¶ˆì¼ì¹˜: config={self.hidden_size}, input={H}"

        # 1) Q/K/V ìƒì„±
        qkv = self.c_attn(x)                                      # (B, T, 3H)
        q, k, v = qkv.split(self.hidden_size, dim=-1)             # ê° (B, T, H)

        # 2) ë©€í‹°í—¤ë“œë¡œ reshape â†’ (B, T, num_heads, head_dim) â†’ (B, num_heads, T, head_dim)
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2).contiguous() # (B, nh, T, hd)
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2).contiguous() # (B, nh, T, hd)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2).contiguous() # (B, nh, T, hd)

        # 3) ìŠ¤ì¼€ì¼ë“œ ë‹·í”„ë¡œë•íŠ¸ ì–´í…ì…˜
        #    ì ìˆ˜: (B, nh, T, T)
        # TODO: attention scoreë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”!
        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # 4) Causal Mask (ë¯¸ë˜ í† í° ì°¨ë‹¨) : (1, 1, T, T) ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        causal_mask = torch.tril(torch.ones(T, T, device=x.device)).view(1, 1, T, T)
        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))

        # 5) Softmax â†’ Dropout
        attn_probs = F.softmax(attn_scores, dim=-1)                # (B, nh, T, T)
        attn_probs = self.attn_dropout(attn_probs)

        # 6) ê°€ì¤‘í•©: (B, nh, T, T) @ (B, nh, T, hd) -> (B, nh, T, hd)
        attn_out = attn_probs @ v

        # 7) multi í—¤ë“œë¥¼ concat: (B, nh, T, hd) -> (B, T, nh*hd=H)
        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, H)

        # 8) ìµœì¢… íˆ¬ì˜ + ë“œë¡­ì•„ì›ƒ
        proj_out = self.c_proj(attn_out)                           # (B, T, H)
        proj_out = self.proj_dropout(proj_out)

        if return_attn:
            return proj_out, attn_probs
        return proj_out

"""#### Layer Normalization

LayerNormì€ ì…ë ¥ ë²¡í„°ì˜ ë§ˆì§€ë§‰ ì°¨ì›(= hidden dimension) ë‹¨ìœ„ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ì •ê·œí™”(normalization)í•œ ë’¤ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(gamma, beta)ë¡œ ë‹¤ì‹œ ìŠ¤ì¼€ì¼ê³¼ ì‹œí”„íŠ¸ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

ì¦‰, í•œ ì‹œì ì˜ hidden state ì „ì²´ë¥¼ í‰ê·  0, ë¶„ì‚° 1ë¡œ ë§Œë“¤ê³ , ì´í›„ ì ì ˆíˆ ë‹¤ì‹œ ì¡°ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

1) ì…ë ¥ê³¼ ì¶œë ¥ í¬ê¸°
- ì…ë ¥: $x \in \mathbb{R}^{B \times T \times H}$ ($B$: batch í¬ê¸°, $T$: ì‹œí€€ìŠ¤ ê¸¸ì´, $H$: hidden dimension)
- ì¶œë ¥: ë™ì¼í•œ í¬ê¸° $\mathbb{R}^{B \times T \times H}$

2) í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°

ê° hidden ë²¡í„°($H$ì°¨ì›)ì— ëŒ€í•´ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•©ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
\mu = \frac{1}{H} \sum_{i=1}^{H} x_i, \qquad
\sigma = \sqrt{\frac{1}{H}\sum_{i=1}^H (x_i - \mu)^2}
$$

```python
mean = x.mean(dim=-1, keepdim=True) # (B, T, 1)
std  = x.std(dim=-1, keepdim=True)  # (B, T, 1)
```

3) ì •ê·œí™”

í‰ê· ì„ ë¹¼ê³  í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ hidden ë²¡í„°ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
- ìˆ˜ì‹:
$$
\hat{x} = \frac{x - \mu}{\sigma + \epsilon}
$$

ì—¬ê¸°ì„œ $\epsilon$ì€ ë¶„ëª¨ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²½ìš°ë¥¼ ë§‰ê¸° ìœ„í•œ ì‘ì€ ê°’ì…ë‹ˆë‹¤.

4) í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼($\gamma$)ê³¼ ì‹œí”„íŠ¸($\beta$)

ì •ê·œí™”ë§Œ í•˜ë©´ í‘œí˜„ë ¥ì´ ì œí•œë˜ë¯€ë¡œ, í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° $\gamma, \beta \in \mathbb{R}^H$ë¥¼ ë„ì…í•©ë‹ˆë‹¤.
- ìµœì¢… ì¶œë ¥:
$$
y = \hat{x} \cdot \gamma + \beta
$$

"""

class LayerNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-5):
        super().__init__()

        self.gamma = nn.Parameter(torch.ones(hidden_size)) # (hidden_size,)
        self.beta = nn.Parameter(torch.zeros(hidden_size)) # (hidden_size,)
        self.eps = eps

    def forward(self, x):
        # x: (batch_size, seq_len, hidden_size)
        mean = x.mean(dim=-1, keepdim=True) # (batch_size, seq_len, 1)
        std = x.std(dim=-1, keepdim=True) # (batch_size, seq_len, 1)
        return (x - mean) / (std + self.eps) * self.gamma + self.beta # (batch_size, seq_len, hidden_size)


# Test layer norm
layer_norm = LayerNorm(config.hidden_size)
x = torch.randn(1, 3, config.hidden_size)
layer_norm_output = layer_norm(x)
print("The size of layer norm output is (batch_size, seq_len, hidden_size):", layer_norm_output.shape)

"""#### MLP

![](https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/MLP.png)

Transformer ë¸”ë¡ ì•ˆì—ì„œ self-attention ë‹¤ìŒì— ë¶™ëŠ” í¬ì§€ì…˜ë³„(feed-forward) MLPì…ë‹ˆë‹¤.
ì¦‰, ê° í† í°(hidden state)ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤.

ì…ë ¥ê³¼ ì¶œë ¥ í¬ê¸°

- ì…ë ¥: $x \in \mathbb{R}^{B \times T \times H}$
- ì¶œë ¥: ë™ì¼í•œ í¬ê¸° $\mathbb{R}^{B \times T \times H}$

ì—¬ê¸°ì„œ $B$ëŠ” ë°°ì¹˜ í¬ê¸°, $T$ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´, $H$ëŠ” ëª¨ë¸ hidden dimensionì…ë‹ˆë‹¤.

ë‚´ë¶€ êµ¬ì¡°

MLPëŠ” ë‘ ê°œì˜ ì„ í˜• ë³€í™˜ê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
1. í™•ì¥ ë‹¨ê³„ (c_fc)
- ì…ë ¥ ì°¨ì› $H$ë¥¼ ë” í° ì°¨ì› $H_{ff}$ë¡œ í™•ì¥
- ìˆ˜ì‹:
$$
z = x W_1 + b_1 \quad \in \mathbb{R}^{B \times T \times H_{ff}}
$$

2. ë¹„ì„ í˜• í™œì„±í™” (GELU)
- $z$ì— ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€
- ìˆ˜ì‹ (GELU):
$$
\text{GELU}(z) = z \cdot \Phi(z)
$$

ì—¬ê¸°ì„œ $\Phi(z)$ëŠ” Gaussian CDF â†’ ReLU, SwiGLU ë“± ë‹¤ë¥¸ í•¨ìˆ˜ë¡œë„ ëŒ€ì²´ ê°€ëŠ¥

3.	ì¶•ì†Œ ë‹¨ê³„ (c_proj)
- ë‹¤ì‹œ ì›ë˜ hidden dimension $H$ë¡œ ì¤„ì„
- ìˆ˜ì‹:
$$
h = zâ€™ W_2 + b_2 \quad \in \mathbb{R}^{B \times T \times H}
$$

4.	Dropout
- í•™ìŠµ ì‹œ ì¼ë¶€ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í•˜ì—¬ ê³¼ì í•© ë°©ì§€

"""

class MLP(nn.Module):
    def __init__(self, hidden_size, ff_hidden_size, dropout_rate=0.1):
        super().__init__()

        self.hidden_size = hidden_size
        self.ff_hidden_size = ff_hidden_size

        self.c_fc = nn.Linear(self.hidden_size, self.ff_hidden_size)
        self.act = nn.GELU() # Or other activation functions
        self.c_proj = nn.Linear(self.ff_hidden_size, self.hidden_size)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # x: (batch_size, seq_len, hidden_size)
        # TODO: MLP layerë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
        # x = FIXME
        # x = FIXME
        # x = FIXME
        # x = FIXME

        return x

# Test MLP
x = torch.randn(1, 3, config.hidden_size)
mlp = MLP(config.hidden_size, config.ff_hidden_size)
output = mlp(x)
print("Output shape:", output.shape)

"""#### Decoder block

<img src="https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/TransformerBlock.png">

DecoderBlock (GPT-style, Pre-Norm)

- ìˆœì„œ: LayerNorm â†’ Self-Attention â†’ Residual â†’ LayerNorm â†’ MLP â†’ Residual
- Self-Attentionì€ í† í° ê°„ ìƒí˜¸ì‘ìš©ì„, MLPëŠ” ê° í† í°ì˜ í‘œí˜„ë ¥ ê°•í™”ë¥¼ ë‹´ë‹¹
- Pre-Norm êµ¬ì¡°ëŠ” í•™ìŠµ ì•ˆì •ì„±(gradient íë¦„)ì„ ë†’ì´ëŠ” ì¥ì ì´ í¼

ì…ì¶œë ¥ê³¼ í¬ê¸°
- ì…ë ¥/ì¶œë ¥: (B, T, H) \to (B, T, H)
- B: ë°°ì¹˜, T: ê¸¸ì´, H: hidden size


ë‹¨ê³„ë³„ ë™ì‘
1.	LNâ‚ â†’ Self-Attention â†’ Residual
ë¨¼ì € ì…ë ¥ì„ ì •ê·œí™”(LNâ‚)í•´ì„œ attentionì— ë„£ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ attentionì´ â€œì •ëˆëœ ìŠ¤ì¼€ì¼â€ì˜ ë²¡í„°ë¥¼ ë°›ì•„ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ê³„ì‚°ëœ ê²°ê³¼ë¥¼ **ì›ë˜ ì…ë ¥(residual)**ì— ë”í•´ ì •ë³´ ì†ì‹¤ ì—†ì´ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
2.	LNâ‚‚ â†’ MLP â†’ Residual
Self-Attentionì„ í†µê³¼í•´ ê°±ì‹ ëœ í‘œí˜„ì„ ë‹¤ì‹œ ì •ê·œí™”(LNâ‚‚)í•˜ê³ , í¬ì§€ì…˜ë³„ MLPë¡œ ë¹„ì„ í˜• ë³€í™˜ì„ ê°€í•œ ë’¤ ë‹¤ì‹œ residual addë¡œ í•©ì¹©ë‹ˆë‹¤. ì´ë¡œì¨ í† í° ê°„ ì •ë³´êµí™˜(Attention)ê³¼ í† í° ë‚´ í‘œí˜„ê°•í™”(MLP)ê°€ í•œ ë¸”ë¡ì—ì„œ ì™„ê²°ë©ë‹ˆë‹¤.

ê°„ë‹¨ ìˆ˜ì‹(ê°œë…ìš©)
- $$\text{AttnOut} = \text{MHA}(\text{LN}_1(X))$$
- $$Xâ€™ = X + \text{AttnOut}$$
- $$\text{MLPOut} = \text{MLP}(\text{LN}_2(Xâ€™))$$
- $$Y = Xâ€™ + \text{MLPOut}$$

ì—¬ê¸°ì„œ MHAëŠ” causal maskë¥¼ ì‚¬ìš©í•œ multi-head self-attentionì…ë‹ˆë‹¤.
"""

class DecoderBlock(nn.Module):
    def __init__(self, hidden_size, ff_hidden_size, num_heads, dropout_rate=0.1):
        super().__init__()
        self.ln_1 = LayerNorm(hidden_size)
        self.attn = MultiHeadSelfAttention(hidden_size, num_heads, dropout_rate)
        self.ln_2 = LayerNorm(hidden_size)
        self.mlp = MLP(hidden_size, ff_hidden_size, dropout_rate)

    def forward(self, x):
        # x: (batch_size, seq_len, hidden_size)
        # TODO: ì§ì ‘ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ë³´ì„¸ìš”!
        # Layer norm 1 :
        x = self.ln_1(x)
        # Self attention + residual
        x = x + self.attn(x)
        # Layer norm 2
        x = self.ln_2(x)
        # MLP + residual
        x = x + self.mlp(x)

        return x

# Test Decoder
x = torch.randn(1, 3, config.hidden_size)
decoder = DecoderBlock(config.hidden_size, config.ff_hidden_size, config.num_heads)
x = decoder(x)
print(x.shape)

"""#### GPT-2

![image](https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/GPT-2.png)

- ì…ë ¥ í† í° ID â†’ í† í° ì„ë² ë”© + ìœ„ì¹˜ ì„ë² ë”© â†’ Nê°œì˜ DecoderBlock(Pre-Norm) â†’ ìµœì¢… LayerNorm â†’ ì–¸ì–´ í—¤ë“œ(Linear to vocab) â†’ ë¡œì§“(logits)
- Causal self-attentionì„ í†µí•´ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ì•Šë„ë¡ í•™ìŠµ/ì¶”ë¡ 
- ì¶œë ¥ì€ (B, T, V) í˜•íƒœì˜ ë‹¤ìŒ í† í° ë¶„í¬ì˜ ë¡œì§“(softmax ì´ì „ ê°’)

êµ¬ì„±ìš”ì†Œ ì„¤ëª…

1) ì„ë² ë”© ê³„ì¸µ

```python
self.token_embed = nn.Embedding(vocab_size, hidden_size)
self.position_embed = nn.Embedding(max_seq_len, hidden_size)
self.embed_dropout = nn.Dropout(dropout_rate)
```

- í† í° ì„ë² ë”©: ì •ìˆ˜ í† í° IDë¥¼ Hì°¨ì›ì˜ ì—°ì† ë²¡í„°ë¡œ ë§¤í•‘
- í¬ì§€ì…˜ ì„ë² ë”©: ê° ìœ„ì¹˜(0..T-1)ì— ëŒ€ì‘í•˜ëŠ” Hì°¨ì› ìœ„ì¹˜ ë²¡í„°ë¥¼ ë”í•´ ìˆœì„œ ì •ë³´ë¥¼ ì£¼ì…
- ë“œë¡­ì•„ì›ƒ: ì„ë² ë”© í›„ ê³¼ì í•© ë°©ì§€

ìˆ˜ì‹(ê°œë…):
ì…ë ¥ $X_{\text{ids}} \in {0,\dots,V-1}^{B\times T}$ì— ëŒ€í•´
$$E = \text{TokenEmb}(X_{\text{ids}}) + \text{PosEmb}([0,\dots,T-1]) \in \mathbb{R}^{B\times T\times H}$$

2) ë³¸ë¬¸: Nê°œì˜ DecoderBlock
```python
self.hidden_layers = nn.ModuleList([
    DecoderBlock(hidden_size, ff_hidden_size, num_heads, dropout_rate)
    for _ in range(num_hidden_layers)
])
```

- ê° ë¸”ë¡ì€ Pre-Norm â†’ Self-Attention(+Residual) â†’ Pre-Norm â†’ MLP(+Residual)
- Self-Attentionì€ causal maskë¥¼ ì‚¬ìš© (DecoderBlock ë‚´ë¶€ì˜ MultiHeadSelfAttentionê°€ ìˆ˜í–‰)

ë¸”ë¡ë³„ ì¶œë ¥ shapeëŠ” ì…ë ¥ê³¼ ë™ì¼: (B, T, H) ìœ ì§€

3) ìµœì¢… ì •ê·œí™” + ì–¸ì–´ í—¤ë“œ
```python
self.ln_f = nn.LayerNorm(hidden_size)
self.language_head = nn.Linear(hidden_size, vocab_size)
```

- ìµœì¢… LayerNorm: ê¹Šê²Œ ìŒ“ì˜€ì„ ë•Œ ë¶„í¬ë¥¼ ì•ˆì •í™”
- ì–¸ì–´ í—¤ë“œ: ê° ìœ„ì¹˜ì˜ hidden stateë¥¼ ì–´íœ˜ ë¶„í¬ ì°¨ì›ìœ¼ë¡œ ì‚¬ìƒ
- ìµœì¢… ì¶œë ¥: logits âˆˆ â„^{BÃ—TÃ—V}

ê°œë… ìˆ˜ì‹:
$$H_T = \text{DecoderBlocks}(E) \quad,\quad \tilde{H}_T = \text{LN}_f(H_T) \quad,\quad \text{Logits} = \tilde{H}T W{\text{LM}} + b$$

forward íë¦„(ë§ë¡œ ì„¤ëª…)
1.	í¬ì§€ì…˜ ID ìƒì„±: 0..T-1ì„ ë§Œë“¤ì–´ ë°°ì¹˜ì— broadcast
2.	ì„ë² ë”© í•©: í† í° ì„ë² ë”© + ìœ„ì¹˜ ì„ë² ë”© â†’ ë“œë¡­ì•„ì›ƒ
3.	Nê°œì˜ ë””ì½”ë” ë¸”ë¡ í†µê³¼: ê° ë¸”ë¡ì—ì„œ
    - LN â†’ Causal Self-Attention â†’ Residual
    - LN â†’ MLP â†’ Residual
4.	ìµœì¢… LN í›„ ì–¸ì–´ í—¤ë“œë¡œ íˆ¬ì˜ â†’ ê° ìœ„ì¹˜ì˜ ë‹¤ìŒ í† í° ë¡œì§“

"""

class GPT2(nn.Module):
    def __init__(
        self,
        hidden_size,
        ff_hidden_size,
        num_heads,
        vocab_size,
        max_seq_len,
        num_hidden_layers,
        dropout_rate=0.1
    ):
        super().__init__()

        self.token_embed = nn.Embedding(vocab_size, hidden_size)
        self.position_embed = nn.Embedding(max_seq_len, hidden_size)
        self.embed_dropout = nn.Dropout(dropout_rate)

        self.hidden_layers = nn.ModuleList([DecoderBlock(hidden_size, ff_hidden_size, num_heads, dropout_rate) for _ in range(num_hidden_layers)])
        # The final layer norm
        self.ln_f = nn.LayerNorm(hidden_size)

        # The final language head, which maps the last hidden state to logits
        self.language_head = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_ids):
        # input_ids: (batch_size, seq_len)
        batch_size, seq_len = input_ids.shape

        # Create position ids (0, 1, 2, ..., seq_len-1)
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) # (1, seq_len)

        # Embed tokens and positions, apply dropout
        x = self.token_embed(input_ids) + self.position_embed(position_ids)
        x = self.embed_dropout(x)

        # Pass through decoder blocks
        for layer in self.hidden_layers:
            x = layer(x)

        # Final layer norm
        x = self.ln_f(x)

        # Project to vocabulary
        logits = self.language_head(x) # (batch_size, seq_len, vocab_size)

        return logits


# Test GPT2
input_ids = torch.randint(0, config.vocab_size, (1, 3))
print("Input IDs shape:", input_ids.shape)

model = GPT2(
    hidden_size=config.hidden_size,
    ff_hidden_size=config.ff_hidden_size,
    num_heads=config.num_heads,
    vocab_size=config.vocab_size,
    max_seq_len=config.max_seq_len,
    num_hidden_layers=config.num_hidden_layers,
    dropout_rate=config.dropout_rate,
)
print("Our GPT2 model:\n", model)

logits = model(input_ids)
print("Logits shape:", logits.shape)