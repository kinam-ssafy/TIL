# -*- coding: utf-8 -*-
"""(ê³¼ì œ-ë¬¸ì œ) 1-1_ë°ì´í„° EDA ë° ëª¨ë¸ í•™ìŠµ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AcySKOb676mSi-UnSDhjffrKvctmBEYS

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**


1. **ê³¼ì œëª…**: ë°ì´í„° EDA ë° ëª¨ë¸ í•™ìŠµ

2. **í•µì‹¬ ì£¼ì œ**
  - íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)ì„ í†µí•œ ë°ì´í„° ì´í•´
  - ì „ì²˜ë¦¬(ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±°, ì¸ì½”ë”©, ìŠ¤ì¼€ì¼ë§) ì›Œí¬í”Œë¡œìš°
  - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµÂ·í‰ê°€

3. **í•™ìŠµ ëª©í‘œ**
  - `pandas`ì™€ `seaborn`ì„ ì´ìš©í•´ ë°ì´í„°ì…‹ì„ ë¡œë”©í•˜ê³  ê²°ì¸¡ì¹˜Â·ì´ìƒì¹˜ë¥¼ íƒìƒ‰í•œë‹¤.
  - `SimpleImputer`, `OneHotEncoder`, `StandardScaler` ë“±ì„ í™œìš©í•´ ìˆ˜ì¹˜í˜•Â·ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì ì ˆíˆ ì „ì²˜ë¦¬í•œë‹¤.
  - train/test ë¶„í• ê³¼ êµì°¨ê²€ì¦ì„ í†µí•´ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.
  - `LinearRegression` ëª¨ë¸ì„ í•™ìŠµí•˜ê³  RMSE, MAE, RÂ² ì§€í‘œë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.

4. **í•™ìŠµ ê°œë…**
  - **IQR(Interquartile Range)** : ì‚¬ë¶„ìœ„ ë²”ìœ„ë¥¼ ì´ìš©í•´ ì´ìƒì¹˜ ê²½ê³„ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•
  - **One-hot Encoding** : ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì´ì§„ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•
  - **SimpleImputer** : í‰ê· Â·ìµœë¹ˆê°’ ë“±ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ìë™ ëŒ€ì²´í•˜ëŠ” ì „ì²˜ë¦¬ í´ë˜ìŠ¤
  - **StandardScaler** : í‰ê·  0, ë¶„ì‚° 1ë¡œ íŠ¹ì„± ê°’ì„ í‘œì¤€í™”í•˜ëŠ” ìŠ¤ì¼€ì¼ëŸ¬
  
5. **í•™ìŠµ ë°©í–¥**
  - ë°ì´í„° ë¡œë”© ë° EDA: seabornì˜ ë‚´ì¥ mpg ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€ pandasë¡œ ê¸°ì´ˆ í†µê³„ì™€ ì‹œê°í™” ìˆ˜í–‰
  - ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸: ê²°ì¸¡ì¹˜ ëŒ€ì²´(SimpleImputer), ì´ìƒì¹˜ í•„í„°ë§(IQR), ë²”ì£¼í˜• ì¸ì½”ë”©(OneHotEncoder), ìŠ¤ì¼€ì¼ë§(StandardScaler)
  - ëª¨ë¸ í•™ìŠµÂ·í‰ê°€: train/test ë¶„í• , êµì°¨ê²€ì¦(cross_val_score) ê¸°ë°˜ LinearRegression í•™ìŠµ ë° ì„±ëŠ¥ ì§€í‘œ ì‚°ì¶œ

6. **ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´**
  - ë°ì´í„°ì…‹ ëª…: Auto MPG dataset (mpg)
  - ë°ì´í„°ì…‹ ê°œìš”: 1970~1982ë…„ ë¯¸êµ­ ì¶œì‹œ ì°¨ëŸ‰ 398ëŒ€ì˜ ì—°ë¹„(MPG)ì™€ ì‹¤ë¦°ë” ìˆ˜, ë°°ê¸°ëŸ‰, ë§ˆë ¥, ì¤‘ëŸ‰, ê°€ì†ë„, ëª¨ë¸ ì—°ë„, ì œì¡°êµ­(origin), ëª¨ë¸ëª…(name) ë“± 9ê°œ íŠ¹ì„±ìœ¼ë¡œ êµ¬ì„±ëœ ê³µê°œ ë°ì´í„°ì…ë‹ˆë‹¤.
  - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ: ì›ë³¸ ë°ì´í„°ëŠ” UCI Machine Learning Repositoryì—ì„œ í¼ë¸”ë¦­ ë„ë©”ì¸(public domain)ìœ¼ë¡œ ì œê³µë˜ë©°, seaborn ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë‚´ì¥ëœ í˜•íƒœë¡œ ì¬ë°°í¬ ë° í•™ìŠµìš© ì‚¬ìš©ì´ í—ˆìš©ë©ë‹ˆë‹¤.

### **Prerequisites**
```
numpy>=1.26
pandas>=2.0
scikit-learn>=1.4
seaborn>=0.12
matplotlib>=3.8
```

# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

**í•™ìŠµ ëª©í‘œ**
  - seabornê³¼ pandasë¥¼ ì´ìš©í•´ mpg ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
  - ê²°ì¸¡ì¹˜ì˜ ìœ„ì¹˜ì™€ ê°œìˆ˜ë¥¼ íƒì§€í•˜ì—¬ ì ì ˆíˆ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.
  - IQR ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì¹˜í˜• ì´ìƒì¹˜ë¥¼ ê²€ì¶œí•˜ê³  train/testì— ì¼ê´€ë˜ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤.
  - ë²”ì£¼í˜• ë³€ìˆ˜(origin)ë¥¼ One-Hot Encodingí•˜ê³  ë¶ˆí•„ìš”í•œ ì‹ë³„ì(name)ë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤.

**í•™ìŠµ ê°œë…**
  - `pandas.isna` & `DataFrame.sum`: ê²°ì¸¡ì¹˜ ì—¬ë¶€ë¥¼ í–‰Â·ì—´ ë‹¨ìœ„ë¡œ ì§‘ê³„í•˜ëŠ” ë©”ì„œë“œ
  - ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ: `df.corr()` + `seaborn.heatmap`
  - ì‹œê°í™” í•¨ìˆ˜
    - `sns.histplot`ìœ¼ë¡œ íƒ€ê¹ƒ ë¶„í¬
    - `sns.regplot`ìœ¼ë¡œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„
    - `sns.lineplot`
  - IQR(Interquartile Range): ì‚¬ë¶„ìœ„ìˆ˜ ì°¨ì´ë¥¼ ì´ìš©í•´ ì´ìƒì¹˜ ê²½ê³„ë¥¼ ì„¤ì •í•˜ëŠ” í†µê³„ ê¸°ë²•
  - OneHotEncoder: sklearn ê¸°ë°˜ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤
  - train/test split: ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì¦ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ëŠ” í‘œì¤€ ì ˆì°¨

**ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½**
  - IQRì„ ì´ìš©í•´ ê° ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ì´ìƒì¹˜ ê²½ê³„ ì‚°ì¶œ â†’ trainì—ì„œ í•„í„°ë§ â†’ ë™ì¼ ê²½ê³„ë¥¼ testì— ì ìš©
  - `SimpleImputer`ë¡œ ìˆ˜ì¹˜í˜• ê²°ì¸¡ì¹˜ ëŒ€ì²´
  - `OneHotEncoder`ë¡œ origin ë³€í™˜, name ì»¬ëŸ¼ ë“œë¡­
  - ì „ì²˜ë¦¬ í›„ train/test DataFrame ì¤€ë¹„ ì™„ë£Œ

ì´ë²ˆ ì‹œê°„ì—ëŠ” seaborn ë‚´ì¥ mpg ë°ì´í„°ì…‹ì„ pandasë¡œ ë¡œë“œí•œ ë’¤, ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  í•µì‹¬ ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ì™€ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•˜ë©° ë°ì´í„°ì˜ ì „ë°˜ì  íŠ¹ì„±ì„ ì´í•´í•©ë‹ˆë‹¤. ì´ì–´ì„œ SimpleImputer, IQR ê¸°ë°˜ í•„í„°ë§, OneHotEncoderë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ê²°ì¸¡ì¹˜ ëŒ€ì²´Â·ì´ìƒì¹˜ ì œê±°Â·ë²”ì£¼í˜• ì¸ì½”ë”©ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì— ìµœì í™”ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

# ë°ì´í„° ë¡œë“œ
df = sns.load_dataset("mpg")

"""### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: ë°ì´í„° ë¶„ì„í•˜ê¸°

`mpg` ë°ì´í„°ì…‹ì— ëŒ€í•´ ì•Œì•„ê°€ëŠ” ì‹œê°„ì„ ê°€ì ¸ë´…ì‹œë‹¤. ê¸°ë³¸ì ì¸ ë°ì´í„°ì˜ ì •ë³´ë¥¼ ë‹¤ìŒ TODOë¥¼ ë”°ë¼ê°€ë©° ì§„í–‰í•´ì£¼ì„¸ìš”.

"""

# TODO
# 1. ì´ ë°ì´í„°ì…‹ì˜ ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ êµ¬í•´ `sample_count`ì— ë‹´ì•„ì£¼ì„¸ìš”!
sample_count = len(df)

EXPECTED_SAMPLE_COUNT = 398
assert sample_count == EXPECTED_SAMPLE_COUNT, f"ì´ ìƒ˜í”Œ ìˆ˜ëŠ” {EXPECTED_SAMPLE_COUNT}ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (got {sample_count})"

# 2. ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œ Seriesë¥¼ `missing_counts`ì— ë‹´ì•„ì£¼ì„¸ìš”!
missing_counts = df.isna().sum().sort_values(ascending=False)

EXPECTED_MISSING_TOP_COL = "horsepower"
EXPECTED_MISSING_TOP_CNT = 6
assert missing_counts.index[0] == EXPECTED_MISSING_TOP_COL, "ê²°ì¸¡ ìµœë‹¤ ì»¬ëŸ¼ì€ horsepowerì—¬ì•¼ í•©ë‹ˆë‹¤."
assert int(missing_counts.iloc[0]) == EXPECTED_MISSING_TOP_CNT, "horsepower ê²°ì¸¡ ê°œìˆ˜ëŠ” 6ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
assert int(missing_counts.drop(index=[EXPECTED_MISSING_TOP_COL]).sum()) == 0, "horsepower ì™¸ ê²°ì¸¡ì€ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤."

# 3. ê²°ì¸¡ì¹˜ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ í–‰ì˜ ìˆ˜ë¥¼ `rows_with_na_count`ì— ë‹´ì•„ì£¼ì„¸ìš”!
rows_with_na_count = df.isna().any(axis=1).sum()

EXPECTED_NA_ROWS = 6
assert rows_with_na_count == EXPECTED_NA_ROWS, f"ê²°ì¸¡ í¬í•¨ í–‰ ìˆ˜ëŠ” {EXPECTED_NA_ROWS}ì—¬ì•¼ í•©ë‹ˆë‹¤. (got {rows_with_na_count})"

# 4. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ëª©ë¡ì—ì„œ íƒ€ê¹ƒ `mpg`ë¥¼ ì œì™¸í•œ ì˜ˆì¸¡ í›„ë³´ ì»¬ëŸ¼ ëª©ë¡ì„
#    `numeric_predictors`ì—, ê·¸ ê°œìˆ˜ë¥¼ `numeric_feature_count`ì— ë‹´ì•„ì£¼ì„¸ìš”!
numeric_predictors = df.select_dtypes(include=[np.number]).columns.drop('mpg').tolist()
numeric_feature_count = len(numeric_predictors)

EXPECTED_NUMERIC_PREDICTORS = ['cylinders','displacement','horsepower','weight','acceleration','model_year']
assert numeric_predictors == EXPECTED_NUMERIC_PREDICTORS, f"ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ í›„ë³´ëŠ” {EXPECTED_NUMERIC_PREDICTORS}ì—¬ì•¼ í•©ë‹ˆë‹¤. (got {numeric_predictors})"
assert numeric_feature_count == len(EXPECTED_NUMERIC_PREDICTORS), "ìˆ˜ì¹˜í˜• ì˜ˆì¸¡ í›„ë³´ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# 5. ë²”ì£¼í˜•(ë¬¸ì/ì¹´í…Œê³ ë¦¬) ì»¬ëŸ¼ë“¤ì˜ ê³ ìœ ê°’ ê°œìˆ˜ë¥¼ ì§‘ê³„í•œ Seriesë¥¼
#    ì¸ë±ìŠ¤ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•´ `cat_unique_counts`ì— ë‹´ì•„ì£¼ì„¸ìš”!
cat_unique_counts = df.select_dtypes(include=['object', 'category']).nunique().sort_index()

EXPECTED_ORIGIN_LEVELS = 3
EXPECTED_NAME_UNIQUE = 305
assert "origin" in cat_unique_counts.index and "name" in cat_unique_counts.index, "origin, name ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
assert int(cat_unique_counts["origin"]) == EXPECTED_ORIGIN_LEVELS, "origin ê³ ìœ ê°’ ê°œìˆ˜ëŠ” 3ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
assert int(cat_unique_counts["name"]) == EXPECTED_NAME_UNIQUE, "name ê³ ìœ ê°’ ê°œìˆ˜ëŠ” 305ì´ì–´ì•¼ í•©ë‹ˆë‹¤."

# 6. íƒ€ê¹ƒ `mpg`ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰(í‰ê· , ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨, ì™œë„)ì„ ê°ê°
#    `mpg_mean`, `mpg_median`, `mpg_std`, `mpg_skew`ì— ë‹´ì•„ì£¼ì„¸ìš”!
mpg_mean = df['mpg'].mean()
mpg_median = df['mpg'].median()
mpg_std = df['mpg'].std()
mpg_skew = df['mpg'].skew()

EXP_MEAN, EXP_MEDIAN, EXP_STD, EXP_SKEW = 23.514572864, 23.0, 7.815984, 0.457
assert abs(mpg_mean - EXP_MEAN) < 1e-6, f"mpg í‰ê· ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_mean})"
assert abs(mpg_median - EXP_MEDIAN) < 1e-6, f"mpg ì¤‘ì•™ê°’ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_median})"
assert abs(mpg_std - EXP_STD) < 1e-6, f"mpg í‘œì¤€í¸ì°¨ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_std})"
assert abs(mpg_skew - EXP_SKEW) < 1e-3, f"mpg ì™œë„ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_skew})"

# 7. IQR ë°©ì‹ìœ¼ë¡œ `mpg`ì˜ ì´ìƒì¹˜ í•˜í•œ/ìƒí•œì„ ê³„ì‚°í•´ ê°ê° `mpg_outlier_lower`, `mpg_outlier_upper`ì— ë‹´ê³ ,
#    ì´ êµ¬ê°„ ë°– ìƒ˜í”Œ ë¹„ìœ¨(%)ì„ `mpg_outlier_ratio`ì— ë‹´ì•„ì£¼ì„¸ìš”!
Q1 = df['mpg'].quantile(0.25)
Q3 = df['mpg'].quantile(0.75)
IQR = Q3 - Q1
mpg_outlier_lower = Q1 - 1.5 * IQR
mpg_outlier_upper = Q3 + 1.5 * IQR
mpg_outlier_ratio = ((df['mpg'] < mpg_outlier_lower) | (df['mpg'] > mpg_outlier_upper)).mean() * 100

EXP_LOWER, EXP_UPPER, EXP_RATIO = 0.25, 46.25, 0.2512562
assert abs(mpg_outlier_lower - EXP_LOWER) < 1e-6, f"IQR í•˜í•œì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_outlier_lower})"
assert abs(mpg_outlier_upper - EXP_UPPER) < 1e-6, f"IQR ìƒí•œì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_outlier_upper})"
assert abs(mpg_outlier_ratio - EXP_RATIO) < 1e-6, f"IQR ì´ìƒì¹˜ ë¹„ìœ¨ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {mpg_outlier_ratio})"

# 8. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì— ëŒ€í•´ íƒ€ê¹ƒ `mpg`ì™€ì˜ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ 
#    (ìê¸° ìì‹  ì œì™¸) ì ˆëŒ€ê°’ ê¸°ì¤€ ìµœëŒ“ê°’ íŠ¹ì„±ëª…ì„ `top_corr_with_mpg`,
#    ê·¸ ìƒê´€ê³„ìˆ˜ ê°’ì„ `top_corr_value`ì— ë‹´ì•„ì£¼ì„¸ìš”!
corr_with_mpg = df[['mpg'] + numeric_predictors].corr()['mpg'].drop('mpg')
top_corr_with_mpg = corr_with_mpg.abs().idxmax()
top_corr_value = corr_with_mpg[top_corr_with_mpg]

EXP_TOP_CORR_FEATURE = "weight"
EXP_TOP_CORR_VALUE = -0.8317409332443347
assert top_corr_with_mpg == EXP_TOP_CORR_FEATURE, f"mpgì™€ ì ˆëŒ€ ìƒê´€ ìµœëŒ“ê°’ íŠ¹ì„±ì€ '{EXP_TOP_CORR_FEATURE}'ì—¬ì•¼ í•©ë‹ˆë‹¤."
assert abs(top_corr_value - EXP_TOP_CORR_VALUE) < 1e-12, f"í•´ë‹¹ ìƒê´€ê³„ìˆ˜ ê°’ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (got {top_corr_value})"

# 9. ìˆ˜ì¹˜í˜• ì „ì²´ ìƒê´€í–‰ë ¬ì„ `corr_mat`ì— ë‹´ê³ ,
#    |corr| >= 0.7 ì¸ (i, j) ìŒ(ëŒ€ê°/ì¤‘ë³µ ì œì™¸)ì„ (ì´ë¦„ìŒ ë¦¬ìŠ¤íŠ¸) `strong_corr_pairs`ì— ë‹´ì•„ì£¼ì„¸ìš”!
corr_mat = df[['mpg'] + numeric_predictors].corr()
strong_corr_pairs = []
for i in range(len(corr_mat.columns)):
    for j in range(i+1, len(corr_mat.columns)):
        if abs(corr_mat.iloc[i, j]) >= 0.7:
            pair = tuple(sorted([corr_mat.columns[i], corr_mat.columns[j]]))
            strong_corr_pairs.append(pair)
strong_corr_pairs = sorted(strong_corr_pairs)

REQUIRED_PAIRS = sorted([
    ("cylinders", "displacement"),
    ("cylinders", "weight"),
    ("displacement", "horsepower"),
    ("displacement", "weight"),
    ("horsepower", "weight"),
    ("mpg", "weight"),
])
for p in REQUIRED_PAIRS:
    assert p in strong_corr_pairs, f"|corr|>=0.7 ê°•í•œ ìƒê´€ìŒì— {p} ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."

# 10. ì‹¤ë¦°ë” ìˆ˜(`cylinders`) ë³„ í‰ê·  `mpg`ë¥¼ êµ¬í•´ ì˜¤ë¦„ì°¨ìˆœ ì¸ë±ìŠ¤ë¡œ ì •ë ¬í•œ
#     Seriesë¥¼ `mpg_by_cylinders`ì— ë‹´ì•„ì£¼ì„¸ìš”!
mpg_by_cylinders = df.groupby('cylinders')['mpg'].mean().sort_index()

EXPECTED_CYL_INDEX = [3, 4, 5, 6, 8]
assert list(mpg_by_cylinders.index) == EXPECTED_CYL_INDEX, "ì‹¤ë¦°ë” ì¸ë±ìŠ¤ëŠ” [3,4,5,6,8] ì´ì–´ì•¼ í•©ë‹ˆë‹¤."


# 11. `model_year`ë¥¼ ì—°ë„(1900+ì—°ë„)ë¡œ í•´ì„í•´ 10ë…„ ë‹¨ìœ„ë¡œ êµ¬ê°„í™”í•œ ë’¤,
#     ê° 10ë…„ëŒ€ë³„ í‰ê·  `mpg`ë¥¼ `mpg_by_decade`ì— ë‹´ì•„ì£¼ì„¸ìš”!
df['year'] = 1900 + df['model_year']
df['decade'] = (df['year'] // 10) * 10
mpg_by_decade = df.groupby('decade')['mpg'].mean()

DECADES_EXPECTED = [1970, 1980]
assert list(mpg_by_decade.index) == DECADES_EXPECTED, "10ë…„ëŒ€ ì¸ë±ìŠ¤ëŠ” [1970, 1980]ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
assert mpg_by_decade.loc[1980] > mpg_by_decade.loc[1970], "1980ë…„ëŒ€ í‰ê·  mpgê°€ 1970ë…„ëŒ€ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤."


print("âœ… ëª¨ë“  EDA í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")

"""### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: í”Œë¡¯ì„ í†µí•œ EDA

ìœ„ì—ì„œ ì•Œê²Œëœ ìˆ˜ì¹˜ì  ë°ì´í„° ì™¸ì— ì‹œê°ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì´í•´í•´ë´…ì‹œë‹¤. ì•„ë˜ ì§€ì‹œì‚¬í•­ì„ ì½ìœ¼ë©´ì„œ ë”°ë¼ê°€ì£¼ì„¸ìš”.

"""

# TODO: ì§€ë‚œ ì‹¤ìŠµê³¼ ë§ˆì°¬ê°€ì§€ë¡œ correlationì„ ê·¸ë ¤ë´…ì‹œë‹¤. ì•„ë˜ ìš”êµ¬ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”.
# ì•„ë˜ ì²¨ë¶€í•œ ê·¸ë¦¼ì²˜ëŸ¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”!
"""
ìš”êµ¬ì‚¬í•­
- ë³€ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. Figure sizeë¥¼ í‚¤ì›Œì£¼ì„¸ìš”.
- Heatmapì€ í•œ ëˆˆì— ìƒê´€ê´€ê³„ì„±ì„ ë³´ê¸°ì— ì¢‹ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì™€ë‹¿ê¸° ìœ„í•´ì„œ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•  ìˆ˜ ìˆì„ê¹Œìš”?
  - hint: annot=True
- í¬ë§·íŒ…ì„ ì†Œìˆ˜ì  ë‘ìë¦¬ê¹Œì§€ë§Œ ë‚˜ì˜¤ê²Œ í•´ì£¼ì„¸ìš”.
- colormapì€ coolwarmì„ ì‚¬ìš©í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.
  - ì°¸ê³ : https://seaborn.pydata.org/tutorial/color_palettes.html
- ëŒ€ê°í–‰ë ¬ì—ëŠ” ìê°€ ìƒê´€ê³„ìˆ˜ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì¦‰ ì–´ì°¨í”¼ 1ì…ë‹ˆë‹¤. ì œì™¸í•˜ê³  ë³´ì—¬ì£¼ì„¸ìš”.
- ìƒê´€ê³„ìˆ˜ í–‰ë ¬ì€ ëŒ€ì¹­í–‰ë ¬ì…ë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš° ì „ì²´ í–‰ë ¬ì„ ì‹œê°í™”í•˜ê¸°ë„ í•˜ì§€ë§Œ, ì˜¤ëŠ˜ì€ ì•„ë˜ ë¶€ë¶„ë§Œ ë³´ê³  ì‹¶ë„¤ìš”. ëŒ€ê°í–‰ë ¬ì„ ê¸°ì¤€ìœ¼ë¡œ ìœ—ë¶€ë¶„ ì œì™¸í•˜ê³  ì•„ë«ë¶€ë¶„ë§Œ ë³´ì—¬ì£¼ì„¸ìš”.
- ìœ„ë¥¼ ë‚ ë¦¬ê³ ë‚˜ë‹ˆ ë’¤ì— ìˆëŠ” gridê°€ ê±°ìŠ¬ë¦½ë‹ˆë‹¤. ì—†ì• ì£¼ì„¸ìš”.
- ìƒ‰ê¹” ìŠ¤ì¼€ì¼ì˜ ë²”ìœ„ë¥¼ -1ë¶€í„° 1ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚°
corr_mat = df[['mpg'] + numeric_predictors].corr()

# ëŒ€ê°ì„  ìœ„ìª½ì„ ë§ˆìŠ¤í‚¹ (í•˜ì‚¼ê°í–‰ë ¬ë§Œ í‘œì‹œ)
mask = np.triu(np.ones_like(corr_mat, dtype=bool))

# Figure í¬ê¸° ì„¤ì •
plt.figure(figsize=(10, 8))

# íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
sns.heatmap(
    corr_mat,
    annot=True,              # ìˆ˜ì¹˜ í‘œì‹œ
    fmt='.2f',               # ì†Œìˆ˜ì  2ìë¦¬ í¬ë§·
    cmap='coolwarm',         # coolwarm ì»¬ëŸ¬ë§µ
    mask=mask,               # ìƒì‚¼ê°í–‰ë ¬ ë§ˆìŠ¤í‚¹
    square=True,             # ì •ì‚¬ê°í˜• ì…€
    vmin=-1,                 # ìµœì†Œê°’ -1
    vmax=1,                  # ìµœëŒ€ê°’ 1
    cbar_kws={'shrink': 0.8} # ì»¬ëŸ¬ë°” í¬ê¸° ì¡°ì •
)

# ê·¸ë¦¬ë“œ ì œê±°ë¥¼ ìœ„í•´ ë°°ê²½ ìŠ¤íƒ€ì¼ ì„¤ì •
plt.gca().set_facecolor('white')

# ì œëª© ì„¤ì •
plt.title('Correlation between features', fontsize=14, pad=20)

# ë ˆì´ì•„ì›ƒ ì¡°ì •
plt.tight_layout()
plt.show()

"""ì°¸ê³ ìš© Correlation Heatmap   
![image](https://github.com/1pha/image-repo/blob/main/module3/corr_hw.png?raw=true)
"""

# TODO: ë‹¤ìŒ ë„¤ ê°€ì§€ í”Œë¡¯ì„ ê°ê° ax[0], ax[1], ax[2], ax[3]ì— ë„£ì–´ì£¼ì„¸ìš” :)
# https://seaborn.pydata.org/generated/seaborn.histplot.html
fig, ax = plt.subplots(figsize=(9, 9), ncols=2, nrows=2)
ax = ax.ravel()

# 1. target ë³€ìˆ˜ì¸ mpgì˜ ë„ìˆ˜ë¶„í¬í‘œë¥¼ ê·¸ë ¤ë³´ì„¸ìš”.
sns.histplot(data=df, x='mpg', kde=True, ax=ax[0])
ax[0].set_title('MPG Distribution')
ax[0].set_xlabel('mpg')
ax[0].set_ylabel('Count')

# 2. weightì™€ mpgëŠ” ì–´ë–¤ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§ˆê¹Œìš”?
sns.regplot(data=df, x='weight', y='mpg', ax=ax[1], scatter_kws={'alpha':0.5})
ax[1].set_title('Correlation between weight and MPG')
ax[1].set_xlabel('weight')
ax[1].set_ylabel('mpg')

# 3. model_yearì— ë”°ë¼ mpgëŠ” ì–´ë–»ê²Œ ë³€í™”í•˜ë‚˜ìš”?
sns.lineplot(data=df, x='model_year', y='mpg', ax=ax[2], marker='o')
ax[2].set_title('Average MPG by Model Year')
ax[2].set_xlabel('model_year')
ax[2].set_ylabel('mpg')

# 4. origin ë³„ë¡œ mpgì˜ ë¶„í¬ì—ëŠ” ì°¨ì´ê°€ ìˆì„ê¹Œìš”? boxplotìœ¼ë¡œ ê·¸ë ¤ì£¼ì„¸ìš”.
sns.boxplot(data=df, x='origin', y='mpg', ax=ax[3])
ax[3].set_title('MPG by Origin')
ax[3].set_xlabel('origin')
ax[3].set_ylabel('mpg')

fig.tight_layout()
plt.show()

"""ì°¸ê³ ìš© Plot.     
![image](https://github.com/1pha/image-repo/blob/main/module3/histograms_hw.png?raw=true)

### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: ë°ì´í„° ì „ì²˜ë¦¬

ë°ì´í„°ë¥¼ ì˜ ì‚´í´ë´¤ë‹¤ë©´ ë¨¼ì € í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¥¼ ë¶„í• í•œ í›„, í•™ìŠµë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ë¥¼ íƒì§€í•œ í›„ ì²˜ë¦¬í•´ë´…ì‹œë‹¤. ê·¸ë¦¬ê³  í‘œì¤€í™”ë¥¼ ì§„í–‰í•˜ì—¬ ëª¨ë¸ë§ì„ ì§„í–‰í•´ë´…ì‹œë‹¤.

ì´ë²ˆì—ëŠ” ê²°ì¸¡ì¹˜ë¥¼ ì‹¤ìŠµê³¼ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì œê±°í•´ë´…ì‹œë‹¤. `sklearn.impute` ëª¨ë“ˆì—ëŠ” `SimpleImputer`ë¼ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ API Referenceë¥¼ í•œ ë²ˆ ì½ì–´ë³´ê³  ì§„í–‰í•´ë´…ì‹œë‹¤.
- https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html

`SimpleImputer`ë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ì¥ì ì´ ìˆì„ê¹Œìš”? ì§€ë‚œ ì‹œê°„ì— `sklearn.preprocessing.StandardScaler`ë¥¼ ì´ìš©í•˜ë©´ í•™ìŠµë°ì´í„°ì˜ í‰ê· /í‘œì¤€í¸ì°¨ë¥¼ ë”°ë¡œ ê³„ì‚°í•˜ê³  ì €ì¥í•˜ì§€ ì•Šì•„ë„ ë©”ì†Œë“œë¥¼ í™œìš©í•´ ë°”ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤. `SimpleImputer`ë„ ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµë°ì´í„°ì— ë“±ì¥í•œ ìµœë¹ˆê°’ì„ ì´ìš©í•´ í…ŒìŠ¤íŠ¸ë°ì´í„°ì— ì ìš©í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

# TODO:
# 1. Train/Test ë°ì´í„° ë¶„í• 
# ì´ë²ˆì—ëŠ” yì—ë„ ê²°ì¸¡ì¹˜ê°€ ìˆì„ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ë¶„í• ì„ ì§„í–‰í•´ë´…ì‹œë‹¤.
# df_train, df_testì— í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„í• í•´ì£¼ì„¸ìš”
""" ìš”êµ¬ì‚¬í•­
- test_sizeëŠ” ì „ì²´ ë°ì´í„°ì˜ 30%ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.
- ì§€ë‚œ ë²ˆê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì¬í˜„ê°€ëŠ¥ì„±ì„ ìœ„í•´ ë‚œìˆ˜ë¥¼ ê³ ì •í•´ì¤ì‹œë‹¤.
- stratifyë¥¼ ì§„í–‰í•˜ê³  ì‹¶ì€ë°, ì´ëŠ” ì—°ì†í˜• ë³€ìˆ˜ì—ì„œëŠ” ì–´ë µìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ì§„í–‰í•˜ë©´ ì¢‹ì„ê¹Œìš”?
  - hint: pd.qcut
"""
from sklearn.model_selection import train_test_split

df['mpg_bin'] = pd.qcut(df['mpg'], q=4, labels=False, duplicates='drop')


df_train, df_test = train_test_split(
    df,
    test_size=0.3,
    random_state=42,
    stratify=df['mpg_bin']
)

# ì„ì‹œë¡œ ë§Œë“  mpg_bin ì»¬ëŸ¼ ì œê±°
df_train = df_train.drop(columns=['mpg_bin'])
df_test = df_test.drop(columns=['mpg_bin'])

# ì¸ë±ìŠ¤ ë¦¬ì…‹
df_train = df_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)


# 2. ê²°ì¸¡ì¹˜ì œê±°
# `sklearn.impute.SimpleImputer`ë¥¼ ì´ìš©í•´ì„œ ê²°ì¸¡ì¹˜ë¥¼ ì±„ì›Œë´…ì‹œë‹¤.
# SimpleImputerëŠ” ë²”ì£¼í˜•ê³¼ ìˆ˜ì¹˜í˜• ë°ì´í„° ë³„ë¡œ ë³„ë„ë¡œ ì œì‘í•´ì•¼í•©ë‹ˆë‹¤.
# ê·¸ë¦¬ê³  í•™ìŠµë°ì´í„°ì— ëŒ€í•´ì„œ ì œì‘í•´ì•¼í•©ë‹ˆë‹¤.
from sklearn.impute import SimpleImputer

# 2-1. ìˆ˜ì¹˜í˜• ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°: `num_imputer`ì— `SimpleImputer` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œì‘í•´ì£¼ì„¸ìš”.
# ìˆ˜ì¹˜í˜• ë°ì´í„°ëŠ” í‰ê· ê°’ìœ¼ë¡œ ì±„ì›Œë´…ì‹œë‹¤.
# ê·¸ë¦¬ê³  `fit_transform`ì„ í†µí•´ í•™ìŠµë°ì´í„°ì— ëŒ€í•œ ë³€í™˜ì„ ì§„í–‰í•´ì¤ë‹ˆë‹¤.
# `transform`ì„ í†µí•´ í…ŒìŠ¤íŠ¸ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜ë„ ì§„í–‰í•©ë‹ˆë‹¤.
numeric_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()
num_imputer = SimpleImputer(strategy='mean')
df_train[numeric_cols] = num_imputer.fit_transform(df_train[numeric_cols])
df_test[numeric_cols] = num_imputer.transform(df_test[numeric_cols])

# 2-2. ë²”ì£¼í˜• ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°: `cat_imputer`ì— `SimpleImputer` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œì‘í•´ì£¼ì„¸ìš”.
# ë²”ì£¼í˜• ë°ì´í„°ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ì›Œë´…ì‹œë‹¤.
# ê·¸ë¦¬ê³  `fit_transform`ì„ í†µí•´ í•™ìŠµë°ì´í„°ì— ëŒ€í•œ ë³€í™˜ì„ ì§„í–‰í•´ì¤ë‹ˆë‹¤.
# `transform`ì„ í†µí•´ í…ŒìŠ¤íŠ¸ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜ë„ ì§„í–‰í•©ë‹ˆë‹¤.
categorical_cols = df_train.select_dtypes(include=['object', 'category']).columns.tolist()
cat_imputer = SimpleImputer(strategy='most_frequent')
df_train[categorical_cols] = cat_imputer.fit_transform(df_train[categorical_cols])
df_test[categorical_cols] = cat_imputer.transform(df_test[categorical_cols])


# 3. IQR ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê¸°ì¤€)
# ë§ˆì°¬ê°€ì§€ë¡œ ë°ì´í„° ëˆ„ìˆ˜ì— ì£¼ì˜í•´ì£¼ì„¸ìš”.
# 3-1. í•™ìŠµë°ì´í„°ì…‹ì—ì„œì˜ IQR ë²”ìœ„ë¥¼ êµ¬í•´ì„œ `iqr_bounds`ì— ë„£ì–´ì£¼ì„¸ìš”.
iqr_bounds = {}
for col in numeric_cols:
    Q1 = df_train[col].quantile(0.25)
    Q3 = df_train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    iqr_bounds[col] = (lower_bound, upper_bound)

# 3-2. ìœ„ì—ì„œ êµ¬í•œ IQR bounds ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë°ì´í„°ì—ì„œì˜ IQRì„ ì œê±°í•´ì£¼ì„¸ìš”.
mask_train = pd.Series([True] * len(df_train))
for col in numeric_cols:
    lower, upper = iqr_bounds[col]
    mask_train &= (df_train[col] >= lower) & (df_train[col] <= upper)
df_train = df_train[mask_train].reset_index(drop=True)

# 3-3. ìœ„ì—ì„œ êµ¬í•œ IQR bounds ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œì˜ IQRì„ ì œê±°í•´ì£¼ì„¸ìš”.
mask_test = pd.Series([True] * len(df_test))
for col in numeric_cols:
    lower, upper = iqr_bounds[col]
    mask_test &= (df_test[col] >= lower) & (df_test[col] <= upper)
df_test = df_test[mask_test].reset_index(drop=True)

# 4. í‘œì¤€í™” ì§„í–‰
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_train[numeric_cols] = scaler.fit_transform(df_train[numeric_cols])
df_test[numeric_cols] = scaler.transform(df_test[numeric_cols])

# 5. ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬
# ì§€ë‚œ ì‹œê°„ì— ë‹¤ë£¬ wine ë°ì´í„°ëŠ” ë²”ì£¼í˜• ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
# í•˜ì§€ë§Œ ì§€ê¸ˆì²˜ëŸ¼ `origin`ê³¼ ê°™ì€ ë²”ì£¼í˜• ë°ì´í„°ëŠ” ëª¨ë¸ì´ í•´ì„í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—
# ì´ë¥¼ ìˆ«ì í˜•íƒœë¡œ "ì¸ì½”ë”©"í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.

# 5-1. df_train.name
# nameì—ëŠ” ì´ ëª‡ ê°œì˜ ê³ ìœ í•œ ê°’ì´ ì¡´ì¬í•˜ë‚˜ìš”? `n_unique_name`ì— í• ë‹¹í•´ì£¼ì„¸ìš”
# ì „ì²´ ë°ì´í„°ìˆ˜ì™€ ë¹„êµí–ˆì„ ë•Œ ë¹„ìœ¨ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”? `pct_unique_name`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
n_unique_name = df_train['name'].nunique()
pct_unique_name = (n_unique_name / len(df_train)) * 100

# EXPECTED_NUNIQUE_NAME = 209
# assert n_unique_name == EXPECTED_NUNIQUE_NAME, (
#     f"Expected {EXPECTED_NUNIQUE_NAME} unique names, got {n_unique_name}"
# )
# print(f"Unique names: {n_unique_name} ({pct_unique_name:.1f}% of all samples)")

# 5-2. df_train.origin
# ë§ˆì°¬ê°€ì§€ë¡œ originì—ëŠ” ì´ ëª‡ ê°œì˜ ê³ ìœ í•œ ê°’ì´ ì¡´ì¬í•˜ë‚˜ìš”? `n_unique_origin`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
n_unique_origin = df_train['origin'].nunique()
NUNIQUE_ORIGIN = 3
assert n_unique_origin == NUNIQUE_ORIGIN, (
    f"Expected {NUNIQUE_ORIGIN} origin levels, got {n_unique_origin}"
)
print(f"Unique origin levels: {n_unique_origin}")

# 5-3. ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©
# ìˆ˜ì¹˜ë¡œ í™•ì¸í•´ë³¼ ê²°ê³¼ nameì˜ ê²½ìš° ë°ì´í„°ìˆ˜ì™€ ë¹„êµí–ˆì„ ë•Œ ê·¸ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„
# mpgë¥¼ ì˜ˆì¸¡í•˜ê¸°ì—ëŠ” ì¡°ê¸ˆ ì–´ë ¤ì›€ì´ ìˆì–´ë³´ì…ë‹ˆë‹¤.
# ë”°ë¼ì„œ ìš°ë¦¬ëŠ” nameì€ ì‚¬ìš©í•˜ì§€ ì•Šê³ 
# originë§Œ ìˆ«ìë¡œ ì¸ì½”ë”©í•˜ì—¬ ì‚¬ìš©í•©ì‹œë‹¤
# `OneHotEncoder`ë¥¼ í™œìš©í•´ì„œ í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ì— `origin` ì¹¼ëŸ¼ì„ ì¸ì½”ë”©í•´ì„œ ë„£ì–´ì£¼ì„¸ìš”.
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse_output=False)
origin_encoded_train = encoder.fit_transform(df_train[['origin']])
origin_encoded_test = encoder.transform(df_test[['origin']])

# ì¸ì½”ë”©ëœ ì»¬ëŸ¼ëª… ìƒì„±
origin_cols = [f'origin_{cat}' for cat in encoder.categories_[0][1:]]

# ê¸°ì¡´ origin ì»¬ëŸ¼ ì œê±°í•˜ê³  ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì¶”ê°€
df_train = df_train.drop(columns=['origin', 'name'])
df_test = df_test.drop(columns=['origin', 'name'])

df_train[origin_cols] = origin_encoded_train
df_test[origin_cols] = origin_encoded_test

"""### ğŸ“ ì°¸ê³ : ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ì²˜ë¦¬
ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ ì²˜ë¦¬ëŠ” ë‹¨ìˆœíˆ ê°’ë§Œ ì±„ìš°ê±°ë‚˜ ì œê±°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì‹œê°„ì  íë¦„ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤!

1. **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**
	-	Forward Fill (ffill): ì´ì „ ì‹œì ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ ì±„ì›€. ì£¼ê°€Â·ì˜¨ë„ì²˜ëŸ¼ ê°’ì´ ê¸‰ë³€í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì í•©.
	-	Backward Fill (bfill): ë‹¤ìŒ ì‹œì ì˜ ê°’ì„ ì‚¬ìš©.
	-	ë³´ê°„(Interpolation): ì„ í˜•Â·ë‹¤í•­Â·ìŠ¤í”Œë¼ì¸ ë³´ê°„ ë“±ìœ¼ë¡œ ì‹œê³„ì—´ì˜ ì¶”ì„¸ë¥¼ ë°˜ì˜í•´ ì±„ì›€.
	-	ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡: ARIMA, Prophet ë“± ì‹œê³„ì—´ ëª¨ë¸ë¡œ ê²°ì¸¡ êµ¬ê°„ì„ ì˜ˆì¸¡í•˜ì—¬ ì±„ì›€.

2. **ì´ìƒì¹˜ ì²˜ë¦¬**
	-	í†µê³„ì  ë°©ë²•: ì´ë™í‰ê· Â·ì´ë™í‘œì¤€í¸ì°¨ë¡œ ì •ìƒ ë²”ìœ„ë¥¼ ì„¤ì •í•˜ê³  ë²—ì–´ë‚œ ê°’ ì œê±°/ìˆ˜ì •.
	-	ê³„ì ˆì„± ê³ ë ¤: ê³„ì ˆ/ì£¼ê¸° íŒ¨í„´ì„ ë¶„ë¦¬í•œ ë’¤ ì”ì°¨ê°€ ì¼ì • ê¸°ì¤€ì„ ë„˜ìœ¼ë©´ ì´ìƒì¹˜ë¡œ íŒë‹¨.

í•µì‹¬ì€ ì‹œê°„ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ê²°ì¸¡ì¹˜Â·ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë©°, ë¬´ì‘ì • ì‚­ì œí•˜ë©´ ì‹œê³„ì—´ íŒ¨í„´ì´ ì™œê³¡ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

# 2: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµÂ·í‰ê°€ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
**í•™ìŠµ ëª©í‘œ**
  - train/testë¡œ ë¶„ë¦¬ëœ ë°ì´í„°ë¥¼ ì´ìš©í•´ `LinearRegression` ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  - RMSE, MAE, RÂ² ë“±ì˜ ì§€í‘œë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆë‹¤.

**í•™ìŠµ ê°œë…**
  - `LinearRegression`: ì•™ìƒë¸” ê¸°ë°˜ íšŒê·€ ëª¨ë¸ë¡œ ë‹¤ìˆ˜ì˜ ê²°ì •íŠ¸ë¦¬ë¥¼ í•™ìŠµí•´ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì¸ë‹¤
  - RMSE (Root Mean Squared Error): ì œê³±ì˜¤ì°¨ì˜ í‰ê· ì— ë£¨íŠ¸ë¥¼ ì”Œì›Œ ëª¨ë¸ ì˜¤ì°¨ë¥¼ í•´ì„í•˜ê¸° ì‰¬ìš´ ë‹¨ìœ„ë¡œ í‘œí˜„
  - MAE (Mean Absolute Error): ì ˆëŒ€ì˜¤ì°¨ í‰ê· ìœ¼ë¡œ ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•œ ì„±ëŠ¥ ì§€í‘œ
  - RÂ² (Coefficient of Determination): ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ ë³€ë™ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë¹„ìœ¨ë¡œ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ

**ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½**
  - ê¸°ë³¸ `LinearRegression` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„± í›„ `fit`, `predict` ìˆ˜í–‰
  - `mean_squared_error`, `mean_absolute_error`, `r2_score`ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥ ì‚°ì¶œ

### ğŸ§‘â€ğŸ’» **ê³¼ì œ**: ëª¨ë¸ í•™ìŠµ

ê¸°ì´ˆì ì¸ ì„ í˜•íšŒê·€ ëª¨ë¸ë¡œ í•´ë‹¹ ê³¼ì œë¥¼ ìˆ˜í–‰í•´ë´…ì‹œë‹¤!
"""

# TODO
# 1. X, y ë‚˜ëˆ„ê¸°
# X_train, y_train, X_test, y_testë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”
# ì˜ˆì¸¡í•´ì•¼í•˜ëŠ” ë³€ìˆ˜ëŠ” "mpg"ì…ë‹ˆë‹¤.
X_train = df_train.drop(columns=['mpg'])
y_train = df_train['mpg']
X_test = df_test.drop(columns=['mpg'])
y_test = df_test['mpg']

# 2. LinearRegressionì„ `reg`ì— í• ë‹¹í•˜ê³  í•™ìŠµë°ì´í„°ì— ëŒ€í•´ í•™ìŠµí•´ì£¼ì„¸ìš”.
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)

# 3. í…ŒìŠ¤íŠ¸ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ë§Œë“¤ì–´ì„œ `y_pred`ì— í• ë‹¹í•´ì£¼ì„¸ìš”.
y_pred = reg.predict(X_test)

# 4. íšŒê·€ ëª¨ë¸ í‰ê°€í•˜ê¸°
# ì•ì—ì„œ ì—¬ëŸ¬ë¶„ì´ í•™ìŠµí•œ ëª¨ë¸ì„ í† ëŒ€ë¡œ ì˜ˆì¸¡í•œ `y_pred`ì™€ `y_true`ë¥¼ í†µí•´ í…ŒìŠ¤íŠ¸ë°ì´í„° ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
# ìˆ˜ì •ì—†ì´ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

test_rmse = mean_squared_error(y_test, y_pred)
test_mae  = mean_absolute_error(y_test, y_pred)
test_r2   = r2_score(y_test, y_pred)

fig, ax = plt.subplots(figsize=(8, 6))
sns.regplot(x=y_test, y=y_pred)
ax.set_title(f"Prediction on Test Data: RMSE={test_rmse:.3f}, R2={test_r2:.3f}", size="large")
ax.set_xlabel("Ground Truth MPG")
ax.set_ylabel("Predicted MPG")

"""# ë§ˆì¹˜ë©°

ì‹¤ì œë¡œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´, ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë©‹ì§„ ë‚˜ì˜ ëª¨ìŠµì„ ë– ì˜¬ë¦¬ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ í”„ë¡œì íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ì†ì§ˆí•˜ê³  ìˆëŠ” ë³¸ì¸ì„ ë°œê²¬í•˜ì‹¤ ìˆ˜ ìˆì„ê²ë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ì‹¤ì œë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ì½”ë“œë³´ë‹¤, ë°ì´í„° íƒìƒ‰ê³¼ ì „ì²˜ë¦¬ì— ë§ì€ ì‹œê°„ì„ ìŸìœ¼ì…¨ì„í…ë°ìš”, ì•„ë¬´ë˜ë„ ë°ì´í„°ê°€ ì œì¼ ìœ„ì— ìˆë‹¤ë³´ë‹ˆ ì•ìœ¼ë¡œë„ ìì£¼ ê²ªê²Œë˜ì‹¤ í˜„ìƒì…ë‹ˆë‹¤ :) ì˜¤ëŠ˜ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤!

## Further Readings

- A Comprehensive Guide to Seaborn for EDA: `seaborn`ìœ¼ë¡œ ìˆ˜í–‰í•œ ë˜ë‹¤ë¥¸ EDA ì˜ˆì‹œì…ë‹ˆë‹¤. ì°¸ê³ í•˜ë©´ ë³µìŠµì— ìœ ìš©í•©ë‹ˆë‹¤.
  - https://towardsdatascience.com/exploratory-data-analysis-in-python-c9a77dfa39ce/
- Hyperparameter Optimization with Optuna and scikit-learn Pipeline: scikit-learn Pipelineê³¼ Optunaë¥¼ ê²°í•©í•˜ëŠ” ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. OptunaëŠ” ì ì ˆí•œ ì´ˆë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•œ ì‹¤í—˜ì„ ì„¤ê³„í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì•Œì•„ë‘ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤ :)
  - https://mlops.community/optuna-sklearn-pipeline

## Open-ended Mission
- **ë°ì´í„° ì „ì²˜ë¦¬ ë‹¤ë¥´ê²Œ ì§„í–‰í•˜ê¸°**: ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ëª¨ë¸ ì„±ëŠ¥ì—ì„œ êµ‰ì¥íˆ í•µì‹¬ì ì¸ ì—­í• ì…ë‹ˆë‹¤. ì¼ë¶€ íŒŒì´í”„ë¼ì¸ì„ ìˆ˜ì •í•˜ë©´ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ê¹Œìš”?
- **ë‹¤ë¥¸ íšŒê·€ ëª¨ë¸ ì ìš© ë° ì„±ëŠ¥ ë¹„êµí•˜ê¸°**: `sklearn.linear_model`ì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•´ë³´ì„¸ìš”. ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¤ë‚˜ìš”?

"""