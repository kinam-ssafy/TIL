# -*- coding: utf-8 -*-
"""(ì‹¤ìŠµ-ë¬¸ì œ) 1-2_MLP êµ¬í˜„.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wkkQU299Pr513PW-4gSsHr4WmdqYA6e4

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**

1. **ì‹¤ìŠµëª…**: MLP êµ¬í˜„

2. **í•µì‹¬ ì£¼ì œ**
  - PyTorch nn.Sequentialì„ ì´ìš©í•œ MLP ì•„í‚¤í…ì²˜ ì„¤ê³„
  - forwardì™€ \_\_call\_\_ ë§¤ì§ ë©”ì„œë“œì˜ ì—­í•  ì´í•´
  - í•™ìŠµ/ê²€ì¦ ë£¨í”„ êµ¬ì„± ë° Early Stoppingì„ í™œìš©í•œ ëª¨ë¸ í‰ê°€

3. **í•™ìŠµ ëª©í‘œ**
  - MLP ëª¨ë¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
  - PyTorchì˜ `forward` í˜¸ì¶œ ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
  - `DataLoader`ì™€ `Dataset`ì„ í™œìš©í•´ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.
  - í•™ìŠµ ì¤‘ `train()`/`eval()` ëª¨ë“œ ì „í™˜ê³¼ Dropout ë™ì‘ì„ ê²€ì¦í•  ìˆ˜ ìˆë‹¤.
  - ê²€ì¦ ì†ì‹¤ ê¸°ì¤€ìœ¼ë¡œ Early Stoppingì„ ì ìš©í•´ ìµœì ì˜ ëª¨ë¸ì„ ì €ì¥Â·ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.

4. **í•™ìŠµ ê°œë…**
  - **`nn.Module`**: PyTorch ëª¨ë¸ í´ë˜ìŠ¤ì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤ë¡œ, `forward`ì™€ ë§¤ì§ ë©”ì„œë“œë¥¼ ì§€ì›í•œë‹¤.
  - **`__call__`**: ê°ì²´ì— () ì—°ì‚°ì„ ê±¸ë©´ ë‚´ë¶€ì—ì„œ `forward` í˜¸ì¶œ íë¦„ì„ ê´€ë¦¬í•˜ëŠ” ë§¤ì§ ë©”ì„œë“œë‹¤.
  - **`DataLoader`**: `Dataset`ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½ì–´ì˜¤ê³ , shuffleÂ·ë³‘ë ¬ ë¡œë”©ì„ ì§€ì›í•˜ëŠ” í—¬í¼ í´ë˜ìŠ¤ë‹¤.

5. **í•™ìŠµ ë°©í–¥**
  - ì‹¤ìŠµì€ PyTorch ê¸°ë°˜ì˜ MLP ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì„¤ê³„Â·êµ¬í˜„í•©ë‹ˆë‹¤.
  - ë¨¼ì € `nn.Sequential`ì„ ì‚¬ìš©í•´ 3ê³„ì¸µ MLPë¥¼ ì •ì˜í•œ ë’¤, `forward`ì™€ `__call__` ë§¤ì»¤ë‹ˆì¦˜ì„ ê²€í† í•©ë‹ˆë‹¤.
  - `sklearn.datasets.load_digits`ë¡œ ì†ê¸€ì”¨ ìˆ«ì ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , `train_test_split`ìœ¼ë¡œ 80/10/10 ë¶„í•  í›„ `StandardScaler`ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
  - ë³€í™˜ëœ NumPy ë°°ì—´ì„ `TensorDataset`ê³¼ `DataLoader`ë¡œ ê°ì‹¸ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•˜ê³ , í•™ìŠµ/ê²€ì¦ ë£¨í”„ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
  - `train()`/`eval()` ëª¨ë“œë¥¼ ì „í™˜í•˜ë©° Dropout ë™ì‘ì„ í™•ì¸í•˜ê³ , ê²€ì¦ ì†ì‹¤ ê¸°ë°˜ Early Stoppingìœ¼ë¡œ ìµœì  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

6. **ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´**
  - ë°ì´í„°ì…‹ ëª…: sklearn `load_digits`
  - ë°ì´í„°ì…‹ ê°œìš”: 8Ã—8 í™”ì†Œë¡œ ì¶•ì†Œëœ ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€ 1,797ê°œì™€ ë ˆì´ë¸”(0â€“9)ë¡œ êµ¬ì„±ëœ ë¶„ë¥˜ìš© ë°ì´í„°ì…‹
  - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ: ì›ë³¸ì€ UCI Machine Learning Repository ê³µê°œ ë„ë©”ì¸ ë°ì´í„°ì´ë©°, scikit-learnì—ì„œ ì¬ë°°í¬ ë° ê°€ê³µí•˜ì—¬ ì œê³µë©ë‹ˆë‹¤.

### **Prerequisites**
```
numpy>=1.26
pandas>=2.0
scikit-learn>=1.4
seaborn>=0.12
torch>=2.2
matplotlib>=3.8
```

# ğŸ”¥ PyTorchë¥¼ í™œìš©í•œ MLP êµ¬í˜„í•˜ê¸°
**í•™ìŠµ ëª©í‘œ**
  - PyTorch nn.Moduleì—ì„œ `forward`ê°€ ì–´ë–»ê²Œ ì—°ë™ë˜ì–´ í•¨ìˆ˜ í˜¸ì¶œì²˜ëŸ¼ ë™ì‘í•˜ëŠ”ì§€ ì´í•´í•œë‹¤.
  - ëª¨ë¸ ê°ì²´ë¥¼ í˜¸ì¶œí•  ë•Œ ì‹¤í–‰ë˜ëŠ” ì „ì²˜ë¦¬(pre-forward hook), ìˆœì „íŒŒ(forward), í›„ì²˜ë¦¬(post-forward hook), ìë™ë¯¸ë¶„ ê·¸ë˜í”„ ë“±ë¡ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

**í•™ìŠµ ê°œë…**
  - `forward` : ì‚¬ìš©ìê°€ êµ¬í˜„í•˜ëŠ” ìˆœì „íŒŒ ë¡œì§ ë©”ì„œë“œë¡œ, `__call__`ì— ì˜í•´ ìë™ ì‹¤í–‰ë¨
  - Hook : ëª¨ë“ˆ ì‹¤í–‰ ì „(pre-forward)Â·í›„(post-forward) ì‹œì ì— ì¶”ê°€ ë¡œì§ì„ ì‚½ì…í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
  - Autograd : ì—°ì‚° ê·¸ë˜í”„ë¥¼ ë™ì ìœ¼ë¡œ êµ¬ì„±í•˜ê³  ì—­ì „íŒŒ(backpropagation)ë¥¼ ìˆ˜í–‰í•˜ëŠ” PyTorch ìë™ ë¯¸ë¶„ ì—”ì§„
  - nn.Module : íŒŒë¼ë¯¸í„° ê´€ë¦¬, hook ë“±ë¡ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” PyTorch ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤

**ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½**
  - ê°„ë‹¨í•œ `nn.Module` ì„œë¸Œí´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  `forward` ë©”ì„œë“œë¥¼ êµ¬í˜„í•œë‹¤.
  - ìë™ë¯¸ë¶„ì„ ìœ„í•œ ê·¸ë˜í”„ ë…¸ë“œ ë“±ë¡ ìˆœì„œë¡œ ë™ì‘í•˜ëŠ” ê²ƒì„ ì½”ë“œë¡œ ì§ì ‘ í™•ì¸í•œë‹¤.
  - ì†ìˆ˜ hook í•¨ìˆ˜ë¥¼ ë“±ë¡í•´ ì‹¤í–‰ ìˆœì„œë¥¼ ì¶œë ¥í•´ ë³´ê³ , ì‹¤ì œ `criterion(y_pred, y_true)`ë„ ë™ì¼í•œ `forward` íë¦„ì„ ì‚¬ìš©í•¨ì„ ì‹¤ìŠµí•œë‹¤.
  - ì´ ê³¼ì •ì„ í†µí•´ PyTorch ëª¨ë“ˆì´ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œë˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ëª…í™•íˆ ì´í•´í•˜ê³ , ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì„¤ê³„ ë¥¼ í•™ìŠµí•œë‹¤.

ì˜¤ëŠ˜ì€ ë“œë””ì–´ ê°€ì¥ ê¸°ì´ˆì ì¸ ë”¥ëŸ¬ë‹ ì‹¤ìŠµì„ ì§„í–‰í•  ì˜ˆì •ì…ë‹ˆë‹¤. PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê¸°ë³¸ì ì¸ Multi-layer Perceptronì„ êµ¬í˜„í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë°°ì›Œë³¼ ì˜ˆì •ì…ë‹ˆë‹¤! ì´ë²ˆ ì‹¤ìŠµì€ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤, PyTorchì™€ ì¹œí•´ì§€ëŠ” ê²ƒì— ì´ˆì ì„ ë‘¡ë‹ˆë‹¤.

**ëŸ°íƒ€ì„ ë³€ê²½**
- ë³¸ ì‹¤ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ì„¤ì •í•  ê²ƒì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ í•´ë‹¹ Colaboratory ì„¸ì…˜ì˜ ëŸ°íƒ€ì„ì„ ë³€ê²½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì€ í–‰ë ¬ê³±ì„ ë§ì´ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¨ìˆœí•œ ë³‘ë ¬ê³„ì‚°ì´ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì—¬ê¸°ì— ìœ ë¦¬í•œ ê²ƒì´ ë°”ë¡œ GPUì…ë‹ˆë‹¤. ìš°ì¸¡ ìƒë‹¨ì˜ â–¾ ëª¨ì–‘ì˜ ë“œë¡­ë‹¤ìš´ì„ ëˆŒëŸ¬ Change Runtime Type â¡ï¸ T4 GPU ì„ íƒ â¡ï¸ Disconnect and delet runtime íŒì—…ì´ ëœ¨ë©´ OK ì„ íƒ â¡ï¸ Save ë¥¼ í†µí•´ Runtimeì„ ë³€ê²½í•´ì¤ë‹ˆë‹¤.

- ì´ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ëŠ” ì´ìœ ëŠ” ëŸ°íƒ€ì„ì´ ë³€ê²½ë˜ë©´ í˜„ì¬ ì‘ì—… ì¤‘ì¸ ë³€ìˆ˜ë“¤ì´ ì´ˆê¸°í™”ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì‹œì‘ ì „ì— ë³€ê²½í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

- âš ï¸ GPUëŠ” ë¹„ì‹¼ ë¦¬ì†ŒìŠ¤ì…ë‹ˆë‹¤. í•œì •ëœ ìì›ì„ ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë‚˜ëˆ  ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, GPUë¥¼ í• ë‹¹ ë°›ì€ í›„ í™œë™ì´ ì—†ê±°ë‚˜, ì¼ì •ì‹œê°„ ì´ìƒ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ìë™ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ê°€ ë°˜í™˜ë˜ëŠ” ì  ìœ ì˜í•´ì£¼ì„¸ìš”. ë”°ë¼ì„œ ë‹¹ì¥ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì´ëŸ° ì €ëŸ° í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•´ë³´ëŠ” ì¤‘ì´ë¼ë©´ êµ³ì´ ëŸ°íƒ€ì„ì„ GPUë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤!
- ì°¸ê³ : [Colab FAQ](https://research.google.com/colaboratory/faq.html)

**ğŸ”¥ PyTorch**     
ë³¸ê²©ì ìœ¼ë¡œ PyTorchë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤. Colabì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ torchê°€ ì„¤ì¹˜ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— Colab í™˜ê²½ ì‚¬ìš©ì‹œ ë³„ë„ì˜ ì„¤ì¹˜ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë§Œì•½ ì¶”ê°€ë¡œ ì„¤ì¹˜í•´ì•¼í•œë‹¤ë©´ í™ˆí˜ì´ì§€ì— ë°©ë¬¸í•˜ì—¬ ë³¸ì¸ì˜ í™˜ê²½ì— ë§ì¶° ì„¤ì¹˜ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”.
- https://pytorch.org/get-started/locally/


PyTorchëŠ” NumPyì™€ ì‚¬ìš©ì´ ìœ ì‚¬í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ ì‚¬ìš©ìë“¤ì˜ í˜¼ë€ì„ ì¤„ì´ê¸° ìœ„í•´ NumPyì— ì¡´ì¬í•˜ëŠ” ê°™ì€ ê¸°ëŠ¥ì˜ í•¨ìˆ˜ë¥¼ ê°™ì€ ì´ë¦„ìœ¼ë¡œ êµ¬í˜„í•´ë‘” ê²ƒì´ ëŒ€ë¶€ë¶„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë¶€ ì°¨ì´ê°€ ì¡´ì¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì •í™•í•œ ë™ì‘ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” API Referenceë¥¼ í•­ìƒ ì°¸ê³ í•´ì£¼ì„¸ìš”.

NumPyì˜ ì‹œì‘ì´ np.arrayì˜€ë‹¤ë©´ PyTorchì˜ ì‹œì‘ì€ torch.Tensorì…ë‹ˆë‹¤. ëª¨ë“  ê²ƒì€ í…ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
"""

import torch

x = torch.rand(3, 3)   # 0~1 ì‚¬ì´ ëœë¤ ê°’
y = torch.ones(3, 3)   # ëª¨ë‘ 1ì¸ í…ì„œ
print("x:\n", x)
print("y:\n", y)
print("x + y:\n", x + y)
print("x @ y.T:\n", x @ y.T)  # í–‰ë ¬ê³±

"""ê°€ì†í™”ê°€ í•„ìˆ˜ë¼ë©´... êµ³ì´ PyTorchë¥¼ ì“°ì§€ ì•Šê³  NumPyë¥¼ CUDAì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“  CuPyë¥¼ ì“°ëŠ”ê²Œ ë‚«ì§€ ì•Šì•˜ì„ê¹Œìš”? PyTorchë¥¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ë°”ë¡œ AutoGrad ê¸°ëŠ¥ ë•Œë¬¸ì…ë‹ˆë‹¤.

**Pytorch Autograd**   
AutogradëŠ” PyTorchì—ì„œ ìë™ìœ¼ë¡œ ë¯¸ë¶„ì„ ê³„ì‚°í•´ì£¼ëŠ” ì—”ì§„ì…ë‹ˆë‹¤.
ìš°ë¦¬ê°€ í…ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´, PyTorchëŠ” ì—°ì‚° ê·¸ë˜í”„(computational graph) ë¥¼ ë§Œë“¤ê³ ,
.backward() í˜¸ì¶œ ì‹œ ê·¸ ê·¸ë˜í”„ë¥¼ ë”°ë¼ ì—­ì „íŒŒ(Backpropagation) ë¥¼ ì‹¤í–‰í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•©ë‹ˆë‹¤.

1. ê¸°ë³¸ ë™ì‘ ì›ë¦¬
  -	`requires_grad=True` ë¡œ ì„¤ì •ëœ í…ì„œì— ëŒ€í•´ ëª¨ë“  ì—°ì‚° ê³¼ì •ì„ ì¶”ì í•©ë‹ˆë‹¤.
  -	ì—°ì‚°ì´ ì¼ì–´ë‚  ë•Œë§ˆë‹¤ `grad_fn` ì´ë¼ëŠ” ì—°ì‚° ë…¸ë“œê°€ ì—°ê²°ë˜ì–´ ê·¸ë˜í”„ê°€ í™•ì¥ë©ë‹ˆë‹¤.
  -	`.backward()`ë¥¼ í˜¸ì¶œí•˜ë©´, ìŠ¤ì¹¼ë¼ ê°’ì—ì„œ ì‹œì‘í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ë”°ë¼ ê±°ê¾¸ë¡œ ê¸°ìš¸ê¸°ë¥¼ ì „íŒŒí•©ë‹ˆë‹¤.
  -	ê³„ì‚°ëœ ê¸°ìš¸ê¸°ëŠ” `.grad` ì†ì„±ì— ì €ì¥ë©ë‹ˆë‹¤.

2. ì˜ˆì‹œ:
$$ y = x^2 + 3x + 1, x, y \in R^2 $$
$$ z = y_1 + y_2 $$

ì—¬ê¸°ì„œ ì–´ë–¤ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ì¶œë ¥ê°’ $z$ëŠ” $x$ì˜ ë³€í™”ì— ì–´ë–»ê²Œ ë°˜ì‘í• ê¹Œìš”? ìš°ë¦¬ëŠ” ì´ë¥¼ $\frac{\partial z}{\partial x}$ë¼ê³  í‘œê¸°ë˜ëŠ” ë¯¸ë¶„ì´ë¼ í‘œí˜„í•˜ê³  ìš°ë¦¬ëŠ” ì—­ì „íŒŒë¥¼ ìœ„í•´ ì´ ê°’ì´ ëŠ˜ í•„ìš”í•©ë‹ˆë‹¤. ìˆ˜ì‹ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³¼ê¹Œìš”? $z$ë¼ëŠ” ë³€ìˆ˜ëŠ” $x$ë¼ëŠ” ë²¡í„°ì— ì˜í•´ì„œ ë³€í•˜ê¸° ë•Œë¬¸ì— ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•©ë‹ˆë‹¤.
$$
\frac{\partial z}{\partial x}=[ \frac{\partial z}{\partial x_1} , \frac{\partial z}{\partial x_2} ]
$$

$$
= [ \frac{\partial z}{\partial y_1} \frac{\partial y_1}{\partial x_1} ,  \frac{\partial z}{\partial y_2} \frac{\partial y_2}{\partial x_2} ]
$$

$\frac{\partial z}{\partial y}$ëŠ” ëª¨ë‘ 1ì´ê¸° ëŒ€ë¬¸ì— ì‹¤ì§ˆì ìœ¼ë¡œ $\frac{\partial y}{\partial x}$ë§Œ ê³„ì‚°í•´ì£¼ë©´ ë©ë‹ˆë‹¤. $x$ì— ëŒ€í•œ $y$ì˜ ë¯¸ë¶„ì€ ì†ì‰½ê²Œ $2x+3$ì´ ë˜ëŠ” ê²ƒì„ ì•Œê¸° ë•Œë¬¸ì— ìµœì¢…ì ìœ¼ë¡œ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

$$
\frac{\partial z}{\partial x}=[2x_1+3, 2x_2+3]
$$

ì‹¤ì œë¡œ ì´ë ‡ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ í™•ì¸í•´ë³¼ê¹Œìš”? ì•„ë˜ ì½”ë“œë¥¼ ì‚´í´ë´…ì‹œë‹¤. í˜„ì¬ $x=[2, 3]$ì¸ ë²¡í„°ë¥¼ ê°€ì •í•˜ê³  ì—°ì‚°ì„ í•´ë´…ì‹œë‹¤. ìˆ˜ì‹ì— ë”°ë¥´ë©´ $\frac{\partial z}{\partial x}=[7, 9]$ê°€ ê³„ì‚°ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
"""

x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x ** 2 + 3 * x + 1
z = y.sum()        # ìŠ¤ì¹¼ë¼

z.backward()       # ì—­ì „íŒŒ ì‹œì‘
print(x.grad)      # dz/dx ê°’ ì¶œë ¥: [7., 9.]

"""ì‹¤ì œë¡œ ì˜ ê³„ì‚°ë˜ì—ˆë‚˜ìš”? ì´ë¥¼ ì–´ë–»ê²Œ ì‘ìš©í•´ë³¼ ìˆ˜ ìˆì„ê¹Œìš”? ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ë‹¤ì‹œ ë– ì˜¬ë ¤ë´…ì‹œë‹¤. ì•„ì£¼ ê°„ë‹¨í•œ ëª¨ë¸ì„ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤.

$$ y = \theta x + b $$

ì—¬ê¸°ì„œ $\theta$ì™€ $b$ëŠ” í•™ìŠµí•´ì•¼í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ì´ê³ , ì‹¤ì œë¡œ ì˜ˆì¸¡í•´ì•¼í•˜ëŠ” íšŒê·€ íƒ€ê²Ÿë³€ìˆ˜ $y$, ê·¸ë¦¬ê³  ë…ë¦½ë³€ìˆ˜ $x$ê°€ ì¡´ì¬í•œë‹¤ê³  í•´ë´…ì‹œë‹¤. ì–´ë–»ê²Œ ê²½ì‚¬í•˜ê°•ë²•ì„ ì§„í–‰í–ˆì—ˆëŠ”ì§€ ë‹¤ì‹œ ë– ì˜¬ë ¤ë´…ì‹œë‹¤.

ë°ì´í„°ê°€ `x=2`, `y=4`ì¸ ìƒí™©ì´ë¼ê³  ê°€ì •í•©ì‹œë‹¤.
1. ì´ˆê¸° ë§¤ê°œë³€ìˆ˜ê°’ ì„¤ì • (e.g. $\theta=3$, $b=1$)
2. í˜„ì¬ ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•œ ì˜ˆì¸¡ê°’ ì¶œë ¥ (e.g. $\hat{y}=3\times2+1=7$)
3. MSE ê³„ì‚° (e.g. $L=(\hat{y}-y)^2=(7-4)^2=3$)
4. MSEë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¯¸ë¶„ í›„ ì—…ë°ì´íŠ¸
  - $\frac{\partial L}{\partial \theta}=\frac{1}{n}\sum{2(\hat{y}-y})x=2\times3\times2=12$
  - $\frac{\partial L}{\partial \theta}=\frac{1}{n}\sum{2(\hat{y}-y})=2\times3=6$

ë³µìŠµí•´ë³´ë‹ˆ ê¸°ì–µì´ ë‚˜ì‹œë‚˜ìš”? ì•„ë˜ ìˆ˜ì‹ìœ¼ë¡œ ì‹¤ì œë¡œ ê³„ì‚°ì´ ë˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.
"""

# íŒŒë¼ë¯¸í„° ì •ì˜ (requires_grad=True)
weight = torch.tensor([[3.0]], requires_grad=True)
bias   = torch.tensor([[1.0]], requires_grad=True)

# ì…ë ¥ê³¼ ëª©í‘œê°’
x = torch.tensor([[2.0]])
y_true = torch.tensor([[4.0]])

# ìˆœì „íŒŒ â†’ ì†ì‹¤ ê³„ì‚°
y_pred = x @ weight + bias              # ì„ í˜• ëª¨ë¸
loss = torch.mean((y_pred - y_true) ** 2)

# ì—­ì „íŒŒ
loss.backward()

weight.grad, bias.grad

"""ì˜¤ ì‹¤ì œë¡œ ë¯¸ë¶„ê°’ì´ ì˜ ë‚˜ì˜¤ë„¤ìš”. ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•´ì£¼ëŠ” ë°©ì‹ì€ í˜„ì¬ ë§¤ê°œë³€ìˆ˜ì—ì„œ í•™ìŠµë¥ ë§Œí¼ ê³±í•œ ë¯¸ë¶„ì„ ë¹¼ì£¼ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ë§ˆì € ë§ˆë¬´ë¦¬í•´ë³´ë©´"""

# íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
with torch.no_grad():
    # requires_grad=Trueì¸ í…ì„œëŠ” inplace operation (+=, -=ì™€ ê°™ì€)ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.
    # ì´ëŸ° ê²½ìš° torch.no_grad contextë¥¼ ì¼œì„œ ì‘ì—…í•´ì£¼ë©´
    weight -= 0.1 * weight.grad
    bias   -= 0.1 * bias.grad

    # ë¯¸ë¶„ì€ ë®ì–´ì“°ëŠ”ê²Œ ì•„ë‹ˆë¼ ëˆ„ì  ë˜ê¸° ë•Œë¬¸ì— ì—†ì• ì¤˜ì•¼í•¨.
    # weight / biasì˜ ë¯¸ë¶„ì„ ì¤‘ê°„ì¤‘ê°„ ì—†ì• ì¤˜ì•¼í•¨
    weight.grad.zero_()
    bias.grad.zero_()

print(weight, bias)

"""ì˜ ì—…ë°ì´íŠ¸ëœê±¸ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ :)   
í•˜ì§€ë§Œ ì•„ì§ì€ PyTorchì˜ ì¥ì ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„ í•™ìŠµì—ì„œ ì œì¼ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì´ì§€ë§Œ, ê·¸ ë°–ì— ë§ì€ ë¶€ë¶„ë“¤ì€ ì—¬ì „íˆ ìˆ˜ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œ ëª‡ ê°€ì§€ë¥¼ ìë™í™”í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. **Parameter ì„ ì–¸**: ì„ í˜•ëª¨ë¸ì—ì„œ Featureë§ˆë‹¤ ê³±í•´ì§€ëŠ” ì„ í˜•ê³„ìˆ˜ë¥¼ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¸í–¥í•­ê¹Œì§€ ë”°ë¡œ ì •ì˜í•˜ì§€ì•Šê³  ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ì†ì‹¤í•¨ìˆ˜ ì •ì˜**: MSEì™€ ê°™ì´ ë‹¨ìˆœí•œ ìˆ˜ì‹ì€ í•¨ìˆ˜ë¡œ ì •ì˜í•˜ë©´ ì¢‹ì§€ë§Œ, ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ê°€ ìƒê¸°ë©´ ê±°ê¸°ì— ë˜ë‹¤ì‹œ MSEë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì½”ë“œ ê´€ë¦¬ë¥¼ í•˜ëŠ”ë° ìˆì–´ ë¶ˆë¦¬í•œ ë©´ì´ ì‘ìš©í•©ë‹ˆë‹¤. PyTorchì—ëŠ” ìš°ë¦¬ê°€ ìì£¼ ì“°ëŠ” MSELoss ê°™ì€ ì†ì‹¤í•¨ìˆ˜ë“¤ì´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. import í›„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤ :)
2. **Parameter ì—…ë°ì´íŠ¸**: torch.optim ëª¨ë“ˆì€ ìš°ë¦¬ê°€ í•™ìŠµí•´ì•¼í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ë“±ë¡í•˜ë©´, ë¯¸ë¶„ê°’ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•´ì£¼ëŠ” ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì„¤ëª…ë§Œ ë“£ê³ ëŠ” ì´í•´ê°€ ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤.
"""

from torch import nn
from torch import optim

'''ë”ë¯¸ ë°ì´í„° ì¤€ë¹„: y = 2x + 1
0~10ê¹Œì§€ 100ê°œì˜ ê°„ê²©, PytorchëŠ” 2Dì…ë ¥ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ì°¨ì› ì¶”ê°€í•˜ì—¬ (100, 1)ëª¨ì–‘ë§Œë“¦'''
X = torch.linspace(0, 10, 100).unsqueeze(1)  # shape: (100, 1)

''' torch.randn_like(X)ë¥¼ í†µí•´
Xì™€ ê°™ì€ shapeì˜ ì •ê·œë¶„í¬ ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì ì¸ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜)'''
y_true = 2 * X + 1 + 0.5 * torch.randn_like(X)  # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€


# 1. Paramter ì„ ì–¸
# ì´ë ‡ê²Œ í•˜ë©´ weight, biasê°€ ìë™ìœ¼ë¡œ ì¤€ë¹„ë©ë‹ˆë‹¤.
# ìš°ë¦¬ëŠ” 1ì°¨ì› ë³€ìˆ˜ xë¡œ 1ì°¨ì› ë³€ìˆ˜ yë¥¼ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ì„ ì–¸í•˜ë©´ ë©ë‹ˆë‹¤.

'''ì„ í˜• íšŒê·€ ëª¨ë¸ ìƒì„±: y = wx + b
in_features=1: ì…ë ¥ ë³€ìˆ˜ê°€ 1ê°œ (x)
out_features=1: ì¶œë ¥ ë³€ìˆ˜ê°€ 1ê°œ (y)
ë‚´ë¶€ì ìœ¼ë¡œ weightì™€ biasê°€ ìë™ìœ¼ë¡œ ì´ˆê¸°í™”ë¨'''

model = nn.Linear(in_features=1, out_features=1)  # ì…ë ¥ 1ì°¨ì›, ì¶œë ¥ 1ì°¨ì›

# 2. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
# torch.nnì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'''Mean Squared Error (í‰ê·  ì œê³± ì˜¤ì°¨) ì†ì‹¤ í•¨ìˆ˜
ê³„ì‚°ì‹: MSE = (1/n) * Î£(ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’)Â²'''
criterion = nn.MSELoss()



# 3. Parameter ì—…ë°ì´íŠ¸
# í˜„ì¬ ì‚¬ìš©í•˜ê³  ìˆëŠ” ì—…ë°ì´íŠ¸ ë°©ì‹ì€ Mini-batch Gradient Descentì…ë‹ˆë‹¤.
# ì •í™•í•œ ì •ì˜ëŠ” batch_size=1ì¼ë•Œ Stochastic Gradient Descent, ì¤„ì—¬ì„œ SGDë¼ê³  ë¶€ë¥´ëŠ”ë°
# PyTorchì—ì„œëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë¯¸ë¶„ ê¸°ë°˜ì˜ ì—…ë°ì´íŠ¸ ë°©ì‹ì„ SGDë¥¼ ì„ ì–¸í•˜ì—¬ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì„ ì–¸í•  ë–„ëŠ” ì—…ë°ì´íŠ¸ ëŒ€ìƒì¸ ë§¤ê°œë³€ìˆ˜ì™€ í•™ìŠµë¥ ì„ ì…ë ¥í•´ì•¼í•©ë‹ˆë‹¤.

'''model.parameters(): ì—…ë°ì´íŠ¸í•  ëª¨ë¸ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜(weight, bias) ë“±ë¡
lr=0.01: í•™ìŠµë¥ (learning rate) ì„¤ì •'''

optimizer = optim.SGD(model.parameters(), lr=0.01)

# 4. í•™ìŠµ ë£¨í”„
epochs = 200
for epoch in range(epochs):
    # ê¸°ìš¸ê¸° ì´ˆê¸°í™”: ìœ„ì—ì„œ ì§„í–‰í•œ weight.grad.zero_()ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•´ì¤ë‹ˆë‹¤.
    # ìˆ˜í–‰ëŒ€ìƒì€ ì„ ì–¸í•  ë•Œ ë“±ë¡í•œ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.

    '''ì´ì „ ë°˜ë³µì—ì„œ ê³„ì‚°ëœ gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    PyTorchëŠ” gradientë¥¼ ëˆ„ì í•˜ë¯€ë¡œ ë§¤ ë°˜ë³µë§ˆë‹¤ ì´ˆê¸°í™” í•„ìš”'''
    optimizer.zero_grad()

    # ì˜ˆì¸¡ê°’ ë§Œë“¤ì–´ì£¼ê¸°: model(X)ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤
    # ì´ì— ëŒ€í•œ ìì„¸í•œ ì‘ë™ë°©ì‹ì€ í›„ì— ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
    '''ëª¨ë¸ì— ì…ë ¥ Xë¥¼ ë„£ì–´ ì˜ˆì¸¡ê°’ ìƒì„±
    ë‚´ë¶€ì ìœ¼ë¡œ y_pred = weight * X + bias ê³„ì‚°'''
    y_pred = model(X)

    # Loss ê³„ì‚°ë„ ë§ˆì°¬ê°€ì§€ë¡œ ìœ„ì—ì„œ ì„ ì–¸í•œ ì†ì‹¤í•¨ìˆ˜ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
    '''ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ MSE ì†ì‹¤ ê³„ì‚°'''
    loss = criterion(y_pred, y_true)

    # Lossë¥¼ ê³„ì‚°í•˜ê³  ë¯¸ë¶„ì„ ê³„ì‚°í•´ì¤ë‹ˆë‹¤.
    '''ì—­ì „íŒŒ(backpropagation) ìˆ˜í–‰
    ì†ì‹¤ì— ëŒ€í•œ ê° ë§¤ê°œë³€ìˆ˜ì˜ gradient ê³„ì‚°
    ê³„ì‚°ëœ gradientëŠ” ê° ë§¤ê°œë³€ìˆ˜ì˜ .grad ì†ì„±ì— ì €ì¥ë¨'''
    loss.backward()

    # ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•´ì¤ë‹ˆë‹¤.
    # ìœ„ì—ì„œ ì§„í–‰í•œ weight -= lr * weight.grad ë¶€ë¶„ì…ë‹ˆë‹¤.
    # ëª¨ë“  ë§¤ê°œë³€ìˆ˜ê°€ optimizer ì•ˆì— ë“±ë¡ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—
    # ë§¤ê°œë³€ìˆ˜ ë³„ë¡œ ì—…ë°ì´íŠ¸ ì½”ë“œë¥¼ ì§œì£¼ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
    '''gradientë¥¼ ì‚¬ìš©í•´ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸
    ë‚´ë¶€ì ìœ¼ë¡œ: parameter = parameter - lr * parameter.grad ìˆ˜í–‰'''
    optimizer.step()

    '''20 ì—í¬í¬ë§ˆë‹¤ í˜„ì¬ ì†ì‹¤ê°’ ì¶œë ¥
    .item(): Tensorì—ì„œ Python ìˆ«ìë¡œ ë³€í™˜'''
    if (epoch+1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

# í•™ìŠµ ê²°ê³¼ ì¶œë ¥
print("\ní•™ìŠµëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥:")
for name, param in model.named_parameters():
    print(f"{name}: {param.data}")


'''LossëŠ” ì—ëŸ¬ê°€ ì•„ë‹˜
ë…¸ì´ì¦ˆ ë•Œë¬¸ì— 0ì´ ë  ìˆ˜ ì—†ìŒ (ì •ìƒ)
ëª¨ë¸ì´ ì‹¤ì œ ê´€ê³„ì‹(y = 2x + 1)ì„ ì˜ í•™ìŠµí•¨
Lossê°€ ë” ì´ìƒ ê°ì†Œí•˜ì§€ ì•ŠëŠ” ê²ƒì€ ìˆ˜ë ´í–ˆë‹¤ëŠ” ì‹ í˜¸

ë§Œì•½ Lossë¥¼ ë” ë‚®ì¶”ê³  ì‹¶ë‹¤ë©´ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ë©´ ë¨:'''

"""
ê°€ì¥ ê¸°ë³¸ì ì¸ PyTorch í•™ìŠµì½”ë“œë¥¼ ì™„ì„±ì‹œì¼°ìŠµë‹ˆë‹¤ ğŸ‰. ê·¸ ì™¸ì— ëŒ€ë‹¤ìˆ˜ ì—°ì‚°ì€ `numpy`ì™€ ìœ ì‚¬í•œ ì ì´ ë§ê¸° ë•Œë¬¸ì— í¬ê²Œ ì–´ë ¤ìš´ ì ì´ ì—†ì„ê²ë‹ˆë‹¤. ì¶”ê°€ë¡œ ì•Œì•„ì•¼í•  ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤.

1. ê°€ì†í™” ê¸°ê¸° í™œìš©í•˜ê¸°: torch.Tensorë¥¼ CPUê°€ ì•„ë‹Œ GPU ì—°ì‚°ì„ ì‚¬ìš©í•˜ë ¤ë©´ `.to` ë©”ì†Œë“œë¥¼ í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ì…€ì—ì„œ ì˜ˆì œë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
2. `numpy.array` â†”ï¸ `torch.Tensor`: ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ì´ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒì€ ìì£¼ ë“±ì¥í•˜ê¸° ë•Œë¬¸ì— ì•Œì•„ë‘ì–´ì•¼í•©ë‹ˆë‹¤.
3. ë³µì¡í•œ ëª¨ë¸ êµ¬ì„±í•˜ê¸°: ìœ„ì—ì„œëŠ” `nn.Linear` í•˜ë‚˜ë¡œ ëª¨ë¸ì´ ì™„ì„±ë˜ì—ˆì§€ë§Œ, ì‹¤ì œë¡œ ìˆ˜ì—…ì‹œê°„ì— ë°°ìš´ Multi-Layer Perceptron ê°™ì€ ë¹„ì„ í˜• ëª¨ë¸ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì˜ ëª¨ë“ˆë¡œ ê²°í•©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤."""

# 1. ê°€ì†í™”ê¸°ê¸° í™œìš©í•˜ê¸°
cuda_on = torch.cuda.is_available()
print(f"CUDAê°€ ì‚¬ìš©ì´ ê°€ëŠ¥í•œê°€ìš”?: {cuda_on}")

cpu_tensor = torch.eye(4)
# ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë™ì‘í•œë‹¤ë©´ ëŸ°íƒ€ì„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.
gpu_tensor = cpu_tensor.to("cuda")

print(f"CPU Tensor: {cpu_tensor}")
print(f"GPU Tensor: {gpu_tensor}")

# 2. NumPy and Torch
import numpy as np

# numpy to torch
np_array = np.eye(4)
t_from_numpy = torch.from_numpy(np_array)
print(np_array, t_from_numpy)

# torch to numpy
# .numpyë¥¼ í˜¸ì¶œí•˜ë©´ ì‰½ê²Œ ë³€í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.
# í•˜ì§€ë§Œ í…ì„œê°€ CUDAì— ì˜¬ë¼ê°€ ìˆë‹¤ë©´ CPUë¡œ ë³€í™˜í•˜ê³  ì‘ë™ì‹œì¼œì¤˜ì•¼í•©ë‹ˆë‹¤.
t_tensor = torch.eye(4).to("cuda")
np_from_tensor = t_tensor.cpu().numpy()
print(t_tensor, np_from_tensor)

# 3. Multi-Layer Perceptron êµ¬ì„±í•˜ê¸°
# `nn.Linear`ë¡œë„ í•´ê²°ì´ ê°€ëŠ¥í•˜ë©´ ì¢‹ê² ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ì¸ê³µì‹ ê²½ë§ì€ ì´ëŸ¬í•œ ì„ í˜•ë ˆì´ì–´ì™€
# ëª‡ ê°€ì§€ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ êµ¬ì„±ë©ë‹ˆë‹¤.
# ë³µì¡í•œ êµ¬ì„±ë“¤ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í•©ì³ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤.

import torch
import torch.nn as nn


class SimpleMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        # ëª¨ë¸ì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì •ì˜í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

        # ğŸŒŸ PyTorch ëª¨ë¸ì„ êµ¬ì„±í•  ë•Œ í•­ìƒ ë„£ì–´ì¤˜ì•¼í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
        super().__init__()

        # ì—¬ê¸°ì— Layerë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # ì‹¤ì œ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•˜ì„ ë•Œ __init__ì—ì„œ ì •ì˜í•œ ë ˆì´ì–´ ìˆœì„œëŒ€ë¡œ ì…ë ¥ì„ ë³´ë‚´ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x


class SequentialMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()

        # ìœ„ì—ì„œ ì œì‘í•œ ì½”ë“œê°€ ê¸°ë³¸ì ì¸ í˜•íƒœì…ë‹ˆë‹¤.
        # í•˜ì§€ë§Œ ë§¤ë²ˆ forwardì—ì„œ ì •ì˜í•œ layerë“¤ì„ ì“°ë©´ì„œ ë„˜ê¸°ê¸°ëŠ” ê·€ì°®ë„¤ìš”.
        # nn.Sequentialì„ ì´ìš©í•˜ë©´ ê·€ì°®ìŒì„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        # ê·€ì°®ìŒ í•´ê²° â˜ï¸
        return self.layers(x)


# ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ëª¨ë¸ì„ ì„ ì–¸í•˜ë ¤ë©´ ìœ„ì—ì„œ ì •ì˜í•œ í´ë˜ìŠ¤ë¥¼ ê°ì²´ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼í•©ë‹ˆë‹¤.
model = SequentialMLP(input_dim=20, hidden_dim=10, output_dim=1)

# ë”ë¯¸ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì„œ ì˜ë„í•œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
# ìœ„ì—ì„œ ëª¨ë¸ì„ ì •ì˜í•  ë•Œ input_dim=20ìœ¼ë¡œ ë§Œë“¤ì—ˆê¸° ë•Œë¬¸ì— íŠ¹ì„±ì´ 20ê°œì¸ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
# ì˜ˆì‹œë¡œ ì „ì²´ ë°ì´í„°ì˜ ìˆ˜ëŠ” 40ê±´ì´ ìˆë‹¤ê³  ê°€ì •í•©ì‹œë‹¤.
# ê·¸ë ‡ë‹¤ë©´ XëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.
X = torch.ones(size=(40, 20))
y_pred = model(X)
y_pred.size()

"""ì§€ê¸ˆ ìœ„ì—ì„œ ë”ë¯¸ë°ì´í„°ë¥¼ ìƒì„±í•  ë•Œ í…ì„œì˜ ëª¨ì–‘ì´ (40, 20)ì´ì—ˆìŠµë‹ˆë‹¤. `nn.Linear`ì˜ ì…ë ¥ì€ í•­ìƒ (ë°ì´í„°ì˜ìˆ˜, ì…ë ¥ì°¨ì›ìˆ˜)ë¡œ ë°›ì•„ì¤˜ì•¼í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ PyTorchëŠ” ë°ì´í„° í•™ìŠµì„ ë°°ì¹˜ì²˜ë¦¬í•˜ëŠ”ë° íš¨ìœ¨ì ì¸ ë°©ë²•ì„ êµ¬í˜„í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤ :) ê·¸ë˜ì„œ ì¶œë ¥ê°’ ë˜í•œ (ë°ì´í„°ìˆ˜, 1)ì´ ë©ë‹ˆë‹¤. `SequentialMLP`ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ `nn.Linear(hidden_dim, 1)`ì´ê¸° ë–„ë¬¸ì— ì¶œë ¥ íŠ¹ì„±ìˆ˜ê°€ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

### ğŸ“ ì°¸ê³ : ê°ì²´ì— `()`ë¥¼ ê±¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

ìœ„ì—ì„œ ì¡°ê¸ˆ ë…íŠ¹í•œ ì—°ì‚°ì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ `model(X)`ì™€ `criterion(y_pred, y_true)`ì…ë‹ˆë‹¤. ë‹¨ìˆœ ê°ì²´ì¸ë° ì™œ í•¨ìˆ˜ì²˜ëŸ¼ ì‘ë™ì´ ê°€ëŠ¥í• ê¹Œìš”? ì •ë‹µì€ ì˜ˆì „ì— ì„¤ëª…í•œ magic methodì— ìˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ì— `__call__`ì´ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©´ ìƒì„±í•œ ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

ê·¸ë ‡ë‹¤ë©´ PyTorch ëª¨ë“ˆë„ `__call__`ë¥¼ ì»¤ìŠ¤í…€í•˜ë©´ ë ê¹Œìš”? ìœ„ì— ì½”ë“œë¥¼ ë³´ì…”ì„œ ì•„ì‹œê² ì§€ë§Œ ìš°ë¦¬ëŠ” `forward`ë¥¼ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ `model(X)`ë¥¼ ìˆ˜í–‰í•˜ë©´ `forward`ê°€ í˜¸ì¶œí•˜ëŠ”ë°, ì´ê²ƒì€ PyTorch êµ¬í˜„ì—ì„œ í•„ìš”í•œ ëª¨ë“  ë™ì‘ì´ `__call__`ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

`nn.Module`ì˜ `__call__` ë‚´ë¶€ëŠ” ëŒ€ëµ ë‹¤ìŒ ìˆœì„œë¡œ ë™ì‘í•©ë‹ˆë‹¤:
  1. ì…ë ¥ í…ì„œì— ëŒ€í•œ ì „ì²˜ë¦¬(ë“±ë¡ëœ pre-forward hook ì‹¤í–‰)
  2. ì‹¤ì œ forward ë©”ì„œë“œ í˜¸ì¶œ
  3. ì¶œë ¥ í…ì„œì— ëŒ€í•œ í›„ì²˜ë¦¬(ë“±ë¡ëœ post-forward hook ì‹¤í–‰)
  4. ìë™ë¯¸ë¶„ì„ ìœ„í•œ ê·¸ë˜í”„ ì—°ê²° ë° backward hook ë“±ë¡

ë”°ë¼ì„œ ì‚¬ìš©ìëŠ” ì˜¤ì§ `forward`ë§Œ êµ¬í˜„í•˜ë©´ ë˜ê³ , ëª¨ë¸ì„ í˜¸ì¶œí•  ë•Œë§ˆë‹¤ PyTorchê°€ ì•Œì•„ì„œ `__call__` â†” `forward` íë¦„ì„ ê´€ë¦¬í•´ ì¤ë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ, ì»¤ìŠ¤í…€ ë ˆì´ì–´ë¥¼ ë§Œë“¤ ë•ŒëŠ” `__call__`ì´ ì•„ë‹ˆë¼ `forward`ë§Œ ì •ì˜í•˜ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤.

### ğŸ§‘â€ğŸ’» **ì‹¤ìŠµ**: ìˆ«ì íŒë…ê¸° ë§Œë“¤ê¸°

ì´ë²ˆì—ëŠ” ìˆ«ì íŒë…ê¸°ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤. `sklearn.datasets.load_digits`ëŠ” ì‚¬ëŒ ì†ìœ¼ë¡œ ì“°ì—¬ì§„ 0-9ê¹Œì§€ì˜ í‘ë°±í•„ê¸° ì‚¬ì§„ì´ ë‹´ê²¨ì ¸ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ì´ì§€ë§Œ (8, 8) ì‚¬ì´ì¦ˆ ë°–ì— ë˜ì§€ ì•Šì•„ì„œ ì´ë¥¼ ì¼ë ¬ë¡œ í´ì„œ Multi-Layer Perceptronì— í†µê³¼ì‹œì¼œ ìˆ«ìë¥¼ ì˜ˆì¸¡í•´ë´…ì‹œë‹¤.

ì•„ë˜ ì½”ë“œëŠ” ë°ì´í„°ì— ëŒ€í•œ ìƒ˜í”Œì…ë‹ˆë‹¤.
"""

import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

digits = load_digits()
X = digits.data.astype(np.float32)  # (n_samples, 64)
y = digits.target.astype(np.int64)

# ë°ì´í„°ë¥¼ í•œ ë²ˆ ì‚´í´ë´…ì‹œë‹¤.
fig, ax = plt.subplots(figsize=(13, 5), nrows=2, ncols=5)
for i in range(10):
    ax[i // 5, i % 5].imshow(X[i].reshape(8, 8), cmap="gray")
    ax[i // 5, i % 5].set_title(f"Label: {y[i]}")
    ax[i // 5, i % 5].axis("off")
fig.tight_layout()

# ì•„ë˜ëŠ” ë‚œìˆ˜ë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
# ì¬í˜„ì„±ì„ ìœ„í•´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
import random
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

SEED = 42
set_seed(seed=42)

# TODO
# 1. ë°ì´í„°ë¥¼ ë¶„í• í•˜ê³  í‘œì¤€í™”í•´ì¤ë‹ˆë‹¤.
# ì´ë²ˆì—ëŠ” ê²€ì¦ë°ì´í„° (X_valid)ê³¼ í…ŒìŠ¤íŠ¸ë°ì´í„° (X_test)ê¹Œì§€ ë§Œë“¤ì–´ì¤ì‹œë‹¤.
# X_testëŠ” ì‹¤ì œë¡œ ì •ë‹µì´ ì—†ëŠ” ì²˜ìŒ ë³´ëŠ” ë°ì´í„°ë¡œ ìµœì¢… ì„±ëŠ¥ì„ í™•ì¸í•´ì•¼í•  ë°ì´í„°,
# X_valì€ ì¤‘ê°„ì¤‘ê°„ ëª¨ë¸ì˜ ê²€ì¦ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…ë‹ˆë‹¤.
# í•™ìŠµë°ì´í„°ëŠ” ì „ì²´ì˜ 80%, ë‚˜ë¨¸ì§€ ê²€ì¦ê³¼ í…ŒìŠ¤íŠ¸ë°ì´í„°ëŠ” ê°ê° 10%ì”© í• ë‹¹í•´ì£¼ì„¸ìš”.
# ë‚œìˆ˜ëŠ” ìœ„ì—ì„œ ì •ì˜ëœ `SEED`ë¥¼ í™œìš©í•´ì£¼ì„¸ìš”.
# stratify í•˜ëŠ”ê²ƒë„ ìŠì§€ ë§ˆì‹œêµ¬ìš”.
from sklearn.model_selection import train_test_split
# X_train, X_valid, X_test, y_train, y_valid, y_test
from sklearn.preprocessing import StandardScaler
# scaler = None
# X_train = None
# X_valid = None
# X_test  = None

# 1ë‹¨ê³„: ì „ì²´ ë°ì´í„°ë¥¼ train(80%) + temp(20%)ë¡œ ë¶„í• 
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.2,        # 20%ë¥¼ ì„ì‹œ ë°ì´í„°ë¡œ
    random_state=SEED,    # ì¬í˜„ì„±ì„ ìœ„í•œ ë‚œìˆ˜ ê³ ì •
    stratify=y            # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
)

# 2ë‹¨ê³„: temp ë°ì´í„°ë¥¼ valid(50%) + test(50%)ë¡œ ë¶„í• 
# tempì˜ 50%ëŠ” ì „ì²´ì˜ 10%
X_valid, X_test, y_valid, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5,        # tempì˜ 50% = ì „ì²´ì˜ 10%
    random_state=SEED,
    stratify=y_temp       # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
)

# 3ë‹¨ê³„: í‘œì¤€í™” (train ë°ì´í„°ë¡œ í•™ìŠµ)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # trainìœ¼ë¡œ í‰ê· /í‘œì¤€í¸ì°¨ í•™ìŠµ + ë³€í™˜
X_valid = scaler.transform(X_valid)       # í•™ìŠµëœ scalerë¡œ ë³€í™˜ë§Œ
X_test = scaler.transform(X_test)         # í•™ìŠµëœ scalerë¡œ ë³€í™˜ë§Œ

# TODO
# 2. PyTorch Datasetì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤
# batch ì²˜ë¦¬ë°©ì‹ì„ ê¸°ì–µí•˜ì‹œë‚˜ìš”?
# ë§¤ë²ˆ Xì—ì„œ ì¸ë±ì‹±ì„ í†µí•´ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë½‘ì•„ëƒˆìŠµë‹ˆë‹¤.
# ì—¬ê°„ ê·€ì°®ì€ê²Œ ì•„ë‹ˆì—ˆëŠ”ë°ìš”, ì´ë¥¼ ê°„í¸í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ì‹ì´ PyTorchì— ì¡´ì¬í•©ë‹ˆë‹¤.

# TODO:
# ëª¨ë“  ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”
X_train_t = torch.tensor(X_train, dtype=torch.float32)
y_train_t = torch.tensor(y_train, dtype=torch.long)      # ë¶„ë¥˜ ë¬¸ì œëŠ” long íƒ€ì…
X_valid_t = torch.tensor(X_valid, dtype=torch.float32)
y_valid_t = torch.tensor(y_valid, dtype=torch.long)
X_test_t  = torch.tensor(X_test, dtype=torch.float32)
y_test_t  = torch.tensor(y_test, dtype=torch.long)


# TODO:
# Pytorch TensorDatasetì— ë„£ì–´ì¤ì‹œë‹¤.
# https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset
from torch.utils.data import TensorDataset

train_ds = TensorDataset(X_train_t, y_train_t)
valid_ds = TensorDataset(X_valid_t, y_valid_t)
test_ds  = TensorDataset(X_test_t, y_test_t)



# TODO:
# ìœ„ì—ì„œ ë§Œë“  TensorDatasetì„ DataLoaderë¡œ ë³€í™˜ì‹œì¼œì¤ë‹ˆë‹¤.
# ë‘˜ì˜ ì°¨ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´
#   - Dataset: ë°ì´í„° í•˜ë‚˜ë¥¼ ì–´ë–»ê²Œ ì½ì–´ì˜¬ì§€ ì •ì˜
#   - DataLoader: ë°°ì¹˜ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œí• ì§€ ì •ì˜
# ë”°ë¼ì„œ DataLoaderëŠ” batch_sizeë¥¼ ì •ì˜í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.
# ì•„ë˜ ì„¤ì ›ì• ë‘” BATCH_SIZEë¥¼ í™œìš©í•´ì£¼ì„¸ìš”.
from torch.utils.data import DataLoader
BATCH_SIZE = 32

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)   # í•™ìŠµ ì‹œ ì„ê¸°
valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)  # ê²€ì¦ ì‹œ ì•ˆ ì„ìŒ
test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)   # í…ŒìŠ¤íŠ¸ ì‹œ ì•ˆ ì„ìŒ

# ê²°ê³¼ í™•ì¸
print(f"Train dataset: {len(train_ds)} samples")
print(f"Valid dataset: {len(valid_ds)} samples")
print(f"Test dataset:  {len(test_ds)} samples")
print(f"\nTrain batches: {len(train_loader)}")
print(f"Valid batches: {len(valid_loader)}")
print(f"Test batches:  {len(test_loader)}")

# ë°°ì¹˜ í•˜ë‚˜ í™•ì¸í•´ë³´ê¸°
for X_batch, y_batch in train_loader:
    print(f"\nBatch shape: X={X_batch.shape}, y={y_batch.shape}")
    break

# ìœ„ì—ì„œ ë§Œë“  Datasetê³¼ DataLoaderì˜ ì°¨ì´ë¥¼ ì‚´í´ë´…ì‹œë‹¤.
# Dataset: í•œ ê°œë¥¼ ì •ì˜í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— ë‹¨ì¼ ì¸ë±ì‹±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
# Return ê°’ì€ TensorDatasetì—ì„œ ë„£ì–´ì¤€ ë°ì´í„°ì˜ ìˆœì„œì…ë‹ˆë‹¤.
# ì¦‰ ë‹¤ìŒ ì½”ë“œëŠ” (X_train_t[0], y_train_t[0])ì„ ë¶ˆëŸ¬ì˜¨ ê²ƒì´ ë©ë‹ˆë‹¤.
train_ds[0]

# ë˜í•œ TensorDatasetì— ëª‡ ê°œì˜ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ”ì§€ë„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤
len(train_ds)

# # ë°˜ë©´ DataLoaderëŠ” ë‹¨ì¼ë°ì´í„° ì²˜ë¦¬ìš©ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì¸ë±ì‹±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
# # ë”°ë¼ì„œ ë‹¤ìŒ ì½”ë“œëŠ” ì—ëŸ¬ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
# train_loader[0]

# ê·¸ëŸ¬ë©´ ë‹¨ì¼ ë°°ì¹˜ê°€ ì˜ ë¶ˆëŸ¬ì¡ŒëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•ì—ëŠ” ë­ê°€ ìˆì„ê¹Œìš”?
# íŒíŠ¸ëŠ” ìœ„ì—ì„œ ë‚œ ì—ëŸ¬ë©”ì‹œì§€ì— ìˆìŠµë‹ˆë‹¤. DataLoaderë¥¼ Iterable ê°ì²´ë¡œ ë³€í™˜ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤.
next(iter(train_loader))

# ì‹¤ì œë¡œ í•™ìŠµì—ì„œ í™œìš©í•  ë•ŒëŠ” for loopì— ë„£ì–´ì„œ êº¼ë‚´ì¤ë‹ˆë‹¤.
for idx, batch in enumerate(train_loader):
    print(idx, batch)
    break

# DataLoaderë„ __len__ í•¨ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.
# ì „ì²´ ë°ì´í„°ìˆ˜ê°€ ì•„ë‹Œ, batchì˜ ê°œìˆ˜ë¼ëŠ” ì  ê¸°ì–µí•˜ì„¸ìš”!
len(train_loader)

"""ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒê¹Œì§€ ì˜ ì„±ê³µí–ˆë„¤ìš”. ì´ì œëŠ” í•™ìŠµ ë¶€ë¶„ìœ¼ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤. ëª¨ë¸ì„ ì •ì˜í•´ì£¼ê³  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë¶€ë¶„ê¹Œì§€ ì •ë¦¬í•´ì£¼ë©´ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤ :) ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ í•˜ë‚˜ì”© ë‹¬ì„±í•´ì£¼ì„¸ìš”.

1. ëª¨ë¸ ì •ì˜: ê¸°ë³¸ì ì¸ MLP ëª¨ë¸ì„ ì§€ì‹œì‚¬í•­ì— ë”°ë¼ êµ¬í˜„í•´ì£¼ì„¸ìš”.
2. ë°°ì¹˜ì²˜ë¦¬ ì •ì˜: í•˜ë‚˜ì˜ DataLoaderì™€ ëª¨ë¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
3. 1, 2ë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµíŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

# TODO
# 3. Multi-Layer Perceptron êµ¬í˜„í•˜ê¸°
# ê°€ì¥ ê¸°ë³¸ì ì¸ MultiLayer Perceptronì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.
# ê³„ì¸µì€ ì´ 3ê°œì…ë‹ˆë‹¤.
# ì…ë ¥í‘œí˜„ -> ì€ë‹‰í‘œí˜„1 -> ì€ë‹‰í‘œí˜„2 -> ì¶œë ¥í‘œí˜„
# ì…ë ¥ê°’ë“¤ì€ `nn.Linear`ë¥¼ í†µê³¼í•œ ë’¤ ReLUë¥¼ ê±°ì¹˜ê³  Dropoutì„ ê±°ì¹˜ë„ë¡ ì„¤ê³„í•´ì£¼ì„¸ìš”.
# ì œê°€ ì •ì˜í•´ì¤€ `__init__` ì•ˆì˜ ì¸ìì— ë§ì¶°ì„œ ì‘ì—…í•´ì£¼ì„¸ìš” :)

class MLP(nn.Module):
    def __init__(self,
                 input_dim: int,
                 num_classes: int,
                 hidden_dims=(128, 64),
                 dropout=0.2):
        super().__init__()
        h1, h2 = hidden_dims

        # TODO
        self.net = nn.Sequential(
            # ì…ë ¥ -> ì€ë‹‰í‘œí˜„1
            nn.Linear(input_dim, h1),    # 64 -> 128
            nn.ReLU(),
            nn.Dropout(dropout),

            # ì€ë‹‰í‘œí˜„1 -> ì€ë‹‰í‘œí˜„2
            nn.Linear(h1, h2),           # 128 -> 64
            nn.ReLU(),
            nn.Dropout(dropout),

            # ì€ë‹‰í‘œí˜„2 -> ì¶œë ¥
            nn.Linear(h2, num_classes)   # 64 -> 10
        )

    def forward(self, x):
        # TODO
        return self.net(x)

# TODO
# 4. DataLoader, Model, Optimizerê°€ ì£¼ì–´ì¡Œì„ ë•Œ
# í•™ìŠµë°ì´í„°ì— ëŒ€í•´ modelì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
import torch.nn.functional as F # <-- Added this line

def train_one_epoch(model, loader, optimizer, device):
    # ëª¨ë¸ì´ í•™ìŠµëª¨ë“œë¡œ ë“¤ì–´ê°€ê²Œ ì„¤ì •í•´ì•¼í•©ë‹ˆë‹¤.
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for xb, yb in loader:
        # ë°ì´í„°ë¥¼ ê°€ì†í™”ê¸°ê¸°ë¡œ ë³´ë‚´ì£¼ì„¸ìš”.
        xb, yb = xb.to(device), yb.to(device)

        # ìœ„ì—ì„œ ë°°ìš´ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë°©ì‹ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.
        # 1. ëª¨ë¸ì˜ ì¶œë ¥ ë§Œë“¤ì–´ì£¼ê¸°
        # 2. ë¯¸ë¶„ê°’ ì§€ì›Œì£¼ê¸° (zero_grad)
        # 3. Loss ê³„ì‚°í•´ì£¼ê¸°: lossëŠ” cross_entropyë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”
        #   - https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html
        # 4. 3ì—ì„œ ê³„ì‚°í•œ Lossë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        # 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”

        # 1. ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()

        # 2. ëª¨ë¸ì˜ ì¶œë ¥ ë§Œë“¤ê¸°
        logits = model(xb)

        # 3. Loss ê³„ì‚° (Cross Entropy)
        loss = F.cross_entropy(logits, yb)

        # 4. ì—­ì „íŒŒ ìˆ˜í–‰
        loss.backward()

        # 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()


        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ëë‚¬ìœ¼ë©´ ëª‡ ê°€ì§€ë¥¼ ê¸°ë¡í•´ì¤ë‹ˆë‹¤.
        # 1. batchë‹¹ ë¡œìŠ¤ ê³„ì‚° (running_lossì— ì—…ë°ì´íŠ¸)
        # 2. ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ê³„ì‚° (`preds`)
        # 3. batch ë‚´ ì •í™•ë„ ê³„ì‚° (batch ë‚´ì—ì„œ ì •ë‹µê°œìˆ˜ë¥¼ correctì— ì¶”ê°€)

        # 1. batchë‹¹ ë¡œìŠ¤ ëˆ„ì 
        running_loss += loss.item() * xb.size(0)  # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ëˆ„ì 

        # 2. ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)  # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í´ë˜ìŠ¤

        # 3. ì •í™•ë„ ê³„ì‚°
        correct += (preds == yb).sum().item()
        total += xb.size(0)


    # ìœ„ì—ì„œ ì˜ ê³„ì‚°ëœ ê°’ë“¤ì„ ì •ë¦¬í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    avg_loss = running_loss / total
    acc = correct / total
    return avg_loss, acc

# TODO
# 5. ì´ë²ˆì—ëŠ” ëª¨ë¸ê³¼ ê²€ì¦/í…ŒìŠ¤íŠ¸ë°ì´í„°ê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì™”ì„ ë•Œ ì •í™•ë„ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
def evaluate(model, loader, device):
    # ëª¨ë¸ì´ ì¶”ë¡ í•  ë•ŒëŠ” ì¶”ë¡ ëª¨ë“œë¡œ ì„¤ì •í•´ì£¼ì–´ì•¼í•©ë‹ˆë‹¤.
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    # ë¯¸ë¶„ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì§€ ì•Šë„ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì…”ì•¼í•©ë‹ˆë‹¤.
    with torch.no_grad():
        for xb, yb in loader:
            # ë°ì´í„°ë¡œë”ì—ì„œ ë‚˜ì˜¨ ë°°ì¹˜ë¥¼ `device`ë¡œ ë³´ë‚´ì£¼ì„¸ìš”.
            xb, yb = xb.to(device), yb.to(device)

            # ìœ„ì—ì„œ í•œ ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ
            # ëª¨ë¸ì˜ ì¶œë ¥ê°’ì„ ë§Œë“¤ì–´ `logits`ì— ë‹´ê³ 
            # í˜„ì¬ ì¶œë ¥ê°’ `logits`ì™€ `yb` ì‚¬ì´ì˜ Cross Entropyë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.
            logits = model(xb)
            loss = F.cross_entropy(logits, yb)


            # ì´ë²ˆì—ë„ ë‹¤ìŒì„ ê¸°ë¡í•´ì¤ë‹ˆë‹¤.
            # 1. batchë‹¹ ë¡œìŠ¤ ê³„ì‚° (running_lossì— update)
            # 2. ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ê³„ì‚° (`preds`)
            # 3. batch ë‚´ ì •í™•ë„ ê³„ì‚° (batch ë‚´ì—ì„œ ì •ë‹µê°œìˆ˜ë¥¼ correctì— ì¶”ê°€)
            # 4. í›„ì— ì •í™•í•œ ë¶„ë¥˜ì„±ëŠ¥ì„ ë³´ê¸° ìœ„í•´ prediction ê°’ë“¤ë„ ì „ë¶€ ê¸°ë¡í•´ì¤ë‹ˆë‹¤.
            #  - all_preds, all_targetsì— appendí•´ì¤ë‹ˆë‹¤.

            # 1. batchë‹¹ ë¡œìŠ¤ ëˆ„ì 
            running_loss += loss.item() * xb.size(0)

            # 2. ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ê³„ì‚°
            preds = torch.argmax(logits, dim=1)

            # 3. ì •í™•ë„ ê³„ì‚°
            correct += (preds == yb).sum().item()
            total += xb.size(0)

            # 4. ì˜ˆì¸¡ê°’ ë° ì‹¤ì œê°’ ê¸°ë¡
            all_preds.append(preds.cpu())
            all_targets.append(yb.cpu())


    # ìœ„ì—ì„œ ì˜ ê³„ì‚°ëœ ê°’ë“¤ì„ ì •ë¦¬í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    avg_loss = running_loss / total
    acc = correct / total
    preds_cat = torch.cat(all_preds).numpy()
    targets_cat = torch.cat(all_targets).numpy()
    return avg_loss, acc, preds_cat, targets_cat

"""### ğŸ“ ì°¸ê³ : `model.train()` vs `model.eval()`

PyTorchì—ì„œ model.train()ê³¼ model.eval()ì€ ëª¨ë¸ì˜ â€œí•™ìŠµ/í‰ê°€ ëª¨ë“œâ€ë¥¼ ì „í™˜í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
ì´ ë©”ì„œë“œëŠ” íŒŒë¼ë¯¸í„° ê°’ì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë ˆì´ì–´ ë™ì‘ ë°©ì‹ì„ ë°”ê¾¸ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì£¼ë¡œ ì˜í–¥ì„ ë°›ëŠ” ë ˆì´ì–´
- **Dropout**: í•™ìŠµ ì¤‘ì—ë§Œ ì¼ë¶€ ë‰´ëŸ°ì„ í™•ë¥ ì ìœ¼ë¡œ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ì¶”ë¡ ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **BatchNorm**: í•™ìŠµ ì¤‘ì—ëŠ” í˜„ì¬ ë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. ì¶”ë¡ ëª¨ë“œì—ì„œëŠ” í•™ìŠµëœ running mean/varianceë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.


ì •í™•í•œ ëª¨ë¸ì˜ ì¶”ë¡ ê²°ê³¼ë¥¼ ë³´ë ¤ë©´ ì¶”ë¡  ì‹œì‘ ì „ì— `model.eval()`ì„ í˜¸ì¶œí•´ì•¼í•©ë‹ˆë‹¤.
"""

# ëª¨ë“  ê²ƒì„ í•˜ë‚˜ë¡œ!
# ì—¬ëŸ¬ë¶„ì´ ì½”ë“œë¥¼ ì˜ ì§°ë‹¤ë©´ ì•„ë˜ ì½”ë“œê°€ ì˜ ì‘ë™í• ê²ë‹ˆë‹¤!
# ì—ëŸ¬ê°€ ëœ¬ë‹¤ë©´ ì—ëŸ¬ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì„œ ì˜ êµ¬í˜„ë˜ë„ë¡ ë§ì¶°ì£¼ì„¸ìš”.
import os
import math
import time

from sklearn.metrics import classification_report, confusion_matrix

device = torch.device("cuda")
model = MLP(
    input_dim=64,
    num_classes=10,
    hidden_dims=(128, 64),
    dropout=0.2
).to(device)

# ì´ë²ˆì—ëŠ” ì§€ë‚œ ê³¼ì œ2ì—ì„œ ë°°ì› ë˜ Adamì„ ì‚¬ìš©í•´ë´…ì‹œë‹¤.
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

best_val_loss = math.inf
best_val_acc = -1.0
epochs_no_improve = 0

checkpoint_path = "model.ckpt"
earlystop_patience = 5

train_losses, train_accs = [], []
valid_losses, valid_accs = [], []
for epoch in range(1, 201):
    t0 = time.time()
    # ì—¬ëŸ¬ë¶„ì´ ì •ì˜í•œ `train_one_epoch`ì™€ `evaluate`ì…ë‹ˆë‹¤.
    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    val_loss, val_acc, _, _ = evaluate(model, valid_loader, device)
    valid_losses.append(val_loss)
    valid_accs.append(val_acc)

    dt = time.time() - t0
    print(
        f"Epoch {epoch:03d} | "
        f"train loss {train_loss:.4f} acc {train_acc*100:5.2f}% | "
        f"val loss {val_loss:.4f} acc {val_acc*100:5.2f}% | "
        f"{dt:.1f}s"
    )

    # ğŸŒŸ ê°œì„  ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    improved = (val_loss < best_val_loss) or (val_acc > best_val_acc)
    if improved:
        best_val_loss = min(best_val_loss, val_loss)
        best_val_acc = max(best_val_acc, val_acc)
        torch.save({
            'model_state_dict': model.state_dict(),
            'input_dim': 64,
            'num_classes': 10,
        }, checkpoint_path)
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    # ì¡°ê¸° ì¢…ë£Œ
    if epochs_no_improve >= earlystop_patience:
        print(f"Early stopping at epoch {epoch} (no improve {earlystop_patience})")
        break

# ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ í›„ í…ŒìŠ¤íŠ¸ í‰ê°€
if os.path.exists(checkpoint_path):
    ckpt = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(ckpt['model_state_dict'])
    print(f"Loaded best checkpoint: val_best_acc={best_val_acc*100:.2f}% val_best_loss={best_val_loss:.4f}")

test_loss, test_acc, y_pred, y_true = evaluate(model, test_loader, device)
print("\n==== Test Result ====")
print(f"Test loss: {test_loss:.4f}")
print(f"Test acc : {test_acc*100:.2f}%")

# ë¶„ë¥˜ ë¦¬í¬íŠ¸ / í˜¼ë™í–‰ë ¬
print("\nClassification report:")
print(classification_report(y_true, y_pred, digits=4))

print("Confusion matrix:")
print(confusion_matrix(y_true, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme()

fig, ax1 = plt.subplots(figsize=(8, 5))

# ì²« ë²ˆì§¸ yì¶•: Loss
color1 = "tab:red"
ax1.set_xlabel("Epoch")
ax1.set_ylabel("Loss")
sns.lineplot(data=train_losses, label="Train BCE", ax=ax1, color="red")
sns.lineplot(data=valid_losses, label="Valid BCE", ax=ax1, color="orange")
ax1.tick_params(axis='y')

# ë‘ ë²ˆì§¸ yì¶•: Accuracy
ax2 = ax1.twinx()
color2 = "tab:blue"
ax2.set_ylabel("Accuracy")
sns.lineplot(data=train_accs, label="Train Accuracy", ax=ax2, color="blue")
sns.lineplot(data=valid_accs, label="Validation Accuracy", ax=ax2, color="skyblue")
ax2.tick_params(axis='y')

# ë²”ë¡€ í†µí•©
lines_1, labels_1 = ax1.get_legend_handles_labels()
lines_2, labels_2 = ax2.get_legend_handles_labels()
ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc="center right")
if ax2.legend_:
    ax2.legend_.remove()

plt.title("Training Progress", size="large")
plt.tight_layout()
plt.show()

"""### ğŸ“ ì°¸ê³ : í•™ìŠµì´ ê¸¸ì–´ì§€ë©´...?

í˜„ì¬ ì œì‘í•œ ì½”ë“œëŠ” lossë¥¼ ì¤‘ê°„ì¤‘ê°„ ê³„ì‚°í•˜ê¸´ í•˜ì§€ë§Œ, ì „ì²´ í•™ìŠµê³¼ì •ì—ì„œ ì‚°ì¶œëœ lossë¥¼ ë³´ê´€í•˜ê³  ìˆëŠ” `train_losses` ì™€ ê°™ì€ ë³€ìˆ˜ë“¤ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì‹œê°í™”ë¥¼ ì¤‘ê°„ì¤‘ê°„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì½”ë“œê°€ ìˆê¸°ëŠ” í•˜ì§€ë§Œ ë§ì´ ë²ˆê±°ë¡­ìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ `wandb`ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ëŠ” ì„œë²„ì— ì¤‘ê°„ì¤‘ê°„ ê¸°ë¡ì„ ë³´ë‚´ í”Œë¡¯ì„ ê·¸ë ¤ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. í•œ ë²ˆ ì‚´í´ë³´ì‹œê³  ë„ì…í•´ë³´ì„¸ìš” :)
- [Weight and Biases Colaboratory Tutorial](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb?_gl=1*1cyn3uu*_ga*MTQ5NjIyMTUxNy4xNzQ4Mzk2MDgw*_ga_JH1SJHJQXJ*czE3NTUyNjYzNzYkbzEwMiRnMSR0MTc1NTI2NjM5OCRqNjAkbDAkaDA.*_ga_GMYDGNGKDT*czE3NTUyNjYzODUkbzUkZzAkdDE3NTUyNjYzOTgkajYwJGwwJGgw)

### ğŸ“ ì°¸ê³ : `sklearn.classification`ì— ìˆëŠ” ëª¨ë“ˆë¡œë„ ê³„ì‚°ì´ ë ê¹Œìš”?

ìš°ë¦¬ê°€ ì§€ê¸ˆ íŒŒë¦¬ ì¡ëŠ”ë° ë„ë¼ë¥¼ ë“  ê²©ì´ ì•„ë‹Œê°€ ìƒê°í•´ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. 8*8 ì‚¬ì´ì¦ˆì˜ ì´ë¯¸ì§€ ì •ë„ë©´ ì‚¬ì‹¤ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œë„ ì–´ëŠ ì •ë„ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œ ë²ˆ ì‹œë„í•´ë³´ì„¸ìš” :)

# ë§ˆì¹˜ë©°
ìµœì¢… ê²€ì¦ë°ì´í„° ê²°ê³¼ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„° ê²°ê³¼ê°€ í¬ê²Œ ì°¨ì´ë‚˜ì§€ ì•Šê³  ì„±ëŠ¥ì´ ì¢‹ìœ¼ë©´ í•™ìŠµì´ ì˜ ë§ˆë¬´ë¦¬ëœ ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‘˜ ì‚¬ì´ì˜ ê°­ì´ ë„ˆë¬´ í¬ê±°ë‚˜ í•™ìŠµ ì¤‘ê°„ì— train_lossëŠ” ì¤„ê³  ìˆëŠ”ë° valid_lossëŠ” ë‹¤ì‹œ ì¦ê°€í•˜ë©´ ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤. ì£¼ì˜í•´ì„œ ì˜ í•™ìŠµì‹œì¼œì£¼ì„¸ìš” :)
"""