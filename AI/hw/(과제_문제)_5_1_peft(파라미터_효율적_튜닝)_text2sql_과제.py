# -*- coding: utf-8 -*-
"""(ê³¼ì œ_ë¬¸ì œ)_5-1_PEFT(íŒŒë¼ë¯¸í„°_íš¨ìœ¨ì _íŠœë‹)_Text2SQL_ê³¼ì œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VB4YWRUv8W00rnJ0HTi4Pig13dz0sOIG

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**

1. ì‹¤ìŠµëª… : Text-to-SQL taskì— ëŒ€í•´ì„œ LoRA í•™ìŠµí•˜ê¸°
2. í•µì‹¬ ì£¼ì œ:
    1. ko_text2sql ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° ë° EDA
    2. base ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
    3. Text-to-SQL task ë°ì´í„° ì „ì²˜ë¦¬
    4. Text-to-SQL task ëª¨ë¸ LoRA fine-tuning
3. í•™ìŠµ ëª©í‘œ :
    1. ko_text2sql ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  EDAë¥¼ í•  ìˆ˜ ìˆë‹¤.
    2. Text-to-SQL task í•™ìŠµì„ ìœ„í•œ base ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
    3. ko_text2sql ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤.
    4. base ëª¨ë¸ì— LoRA fine-tuningì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.
4. í•™ìŠµ ê°œë…: í‚¤ì›Œë“œëª… : ì£¼ìš” í•™ìŠµ í‚¤ì›Œë“œì— ëŒ€í•œ ì„¤ëª…ì„ 1ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. í‚¤ì›Œë“œëŠ” 3ê°œ ì´ìƒ ì‘ì„± ë¶€íƒë“œë¦½ë‹ˆë‹¤.
    1. EDA
    2. Text-to-SQL
    3. LoRA fine-tuning
5. í•™ìŠµ ë°©í–¥ :
  - Text-to-SQLì´ë¼ëŠ” íŠ¹ì • taskì— ëŒ€í•´ì„œ íŠ¹í™”ì‹œí‚¤ëŠ” LoRA fine-tuningì„ ì§„í–‰í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì•„ì‰¬ìš¸ ê²½ìš° í•™ìŠµ ë°ì´í„°ì™€ ì ì€ GPU ë©”ëª¨ë¦¬ë¡œë„ ì„±ëŠ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì²´ë“í•©ë‹ˆë‹¤.
  - ì‹¤ìŠµ ì½”ë“œëŠ” ì¡°êµê°€ ì§ì ‘ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
  - í•´ë‹¹ ì‹¤ìŠµì€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ê²½ìš° ë¬´ì—‡ì´ í•„ìš”í•˜ê³  ì–´ë–»ê²Œ í•˜ë©´ í•™ìŠµì„ íš¨ìœ¨ì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ”ì§€ ê³ ë¯¼í•´ë´…ë‹ˆë‹¤.

6. ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´
  - ë°ì´í„°ì…‹ ëª… : [shangrilar/ko_text2sql](https://huggingface.co/datasets/shangrilar/ko_text2sql)
  - ë°ì´í„°ì…‹ ê°œìš” : table ì •ë³´ì™€ ì§ˆë¬¸ ê·¸ë¦¬ê³  SQLë¬¸ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ë°ì´í„°ì…‹
  - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ : cc-by-4.0

### **Prerequisites**
```
numpy==2.0.2
pandas==2.2.2
transformers==4.56.0
torch==2.8.0+cu126
accelerate==1.10.1
datasets==4.0.0
peft==0.17.1
trl==0.22.2
```
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy==2.0.2 pandas==2.2.2 peft==0.17.1 torch==2.8.0+cu126 --index-url https://download.pytorch.org/whl
# %pip install transformers==4.56.0 accelerate==1.10.1 datasets==4.0.0
# %pip install trl==0.22.2

import torch
import torch.nn as nn
import numpy as np
import random

# ì‹œë“œ ì„¤ì •
random.seed(1234)
np.random.seed(1234)
torch.manual_seed(1234)
torch.cuda.manual_seed(1234)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° ë° EDA

- í•™ìŠµ ëª©í‘œ : ko_text2sql ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ EDAë¥¼ í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë… : EDA
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
    - ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    - ë°ì´í„°ì…‹ EDA

ì²¨ë¶€í•œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
"""

from datasets import load_dataset

text_to_sql = load_dataset("shangrilar/ko_text2sql", data_dir="origin")
train_df = text_to_sql["train"].to_pandas()
train_df.head()

"""columnê³¼ ê° ë°ì´í„°ì˜ íƒ€ì…ì„ í™•ì¸í•©ë‹ˆë‹¤.

### ko_text2sql column ì„¤ëª…
db_id, context, question, answer ì»¬ëŸ¼ ì„¤ëª…

- db_id : DBì˜ id ì •ë³´
- context : DDLì˜ í…Œì´ë¸” ì •ë³´
- question : ìœ ì €ì˜ ì§ˆë¬¸
- answer : ìœ ì €ì˜ ì§ˆë¬¸ì— ë”°ë¥¸ SQLë¬¸
"""

train_df.info()

# í•œ ì¤„ë§Œ ê°€ì ¸ì˜¤ê¸°
for i, row in train_df.iterrows():
    for k, v in row.items():
        print(k, ":", v)
    break

"""`db_id` columnì€ êµ³ì´ í•„ìš”í•œ ì»¬ëŸ¼ì€ ì•„ë‹ˆë¯€ë¡œ í•´ë‹¹ columnì„ ì œê±°í•˜ê³  columnì„ ì¬ì •ì˜í•©ë‹ˆë‹¤."""

columns = ["context", "question", "answer"]

train_data = train_df[columns]
train_data

"""ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.

NLPì—ì„œ ì¤‘ìš”í•˜ê²Œ ì²´í¬í•´ì•¼ í•  EDA í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
1. sequence length
2. language
3. input data domain

ìœ„ì˜ ì‚¬í•­ë“¤ì„ ì²´í¬í•´ë´…ì‹œë‹¤.

2ë²ˆê³¼ 3ë²ˆì€ ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì´ë¯¸ ì²´í¬ê°€ ë©ë‹ˆë‹¤.
- language : í•œêµ­ì–´ ë° SQL ì½”ë“œ
- input data domain : ë‹¤ì–‘í•œ ì¼ë°˜ í‘œ(table)ì—ì„œ ë½‘ì€ ë°ì´í„°(Open domain)

ê·¸ëŸ¬ë©´ 1ë²ˆì„ ì§ì ‘ ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ ì²´í¬í•´ë´…ë‹ˆë‹¤.
"""

# 1. sequence length
# TODO: max_sequence lengthë¥¼ í™•ì¸í•  ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
max_sequence = [0, 0, 0]
for i, row in train_data.iterrows():
    for i, (k, v) in enumerate(row.items()):
        # FIXME
        max_sequence[i] = max(max_sequence[i], len(str(v))) # Calculate max length of string representation

max_sequence

"""`max_length`ê°€ ëª¨ë¸ì˜ `max_sequence_length`ë¥¼ ë„˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  ëª¨ë¸ ì…ë ¥ì€ tokenì´ë‹ˆê¹Œ token ê¸¸ì´ë¥¼ ì¸¡ì •í•´ì•¼ í•˜ëŠ”ê±° ì•„ë‹Œê°€ìš”?</b><br>
ë§ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì¬ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í• ì§€ ì •í•´ì§€ì§€ ì•Šì€ ìƒí™©ì—ì„œëŠ” token ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ëŒ€ëµì ì¸ ê¸¸ì´ë¥¼ ë³´ê³  ëª¨ë¸ì´ ê²°ì •ë˜ë©´ token ê¸¸ì´ë¥¼ ë‹¤ì‹œ ì¸¡ì •í•´ì•¼ í•©ë‹ˆë‹¤.
</blockquote>

ì´ì œë¶€í„° ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µì‹œë‹¤.

# 2. base ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
- í•™ìŠµ ëª©í‘œ : base ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë… : base ëª¨ë¸
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
    - base ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    - í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°

access ê¶Œí•œì„ ë°›ì•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ huggingface tokenì´ í•„ìš”í•©ë‹ˆë‹¤. ì•„ë˜ huggingface tokenì„ ì…ë ¥í•´ì£¼ì„¸ìš”.
"""

from huggingface_hub import login
from google.colab import userdata
hf_token = userdata.get('HF_TOKEN')

login(token=hf_token)

"""ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
if tokenizer.pad_token is None: # pad_token ì„¤ì •ì´ ë˜ì–´ìˆì§€ ì•ŠëŠ” ê²½ìš°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto"
)

"""ëª¨ë¸ì˜ max_sequence_lengthë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

max_position_embeddingsì˜ ê¸¸ì´ëŠ” position embeddingsì˜ ê¸¸ì´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

position embeddingsëŠ” ì…ë ¥ í† í°ì— ìˆœì„œ ì •ë³´ë¥¼ ë„£ëŠ” ì„ë² ë”©ì´ë¯€ë¡œ í•´ë‹¹ ì°¨ì›ì´ ì‹¤ì œ ì…ë ¥ í† í°ì˜ ê¸¸ì´ì™€ ë™ì¼í•©ë‹ˆë‹¤.
"""

max_sequence_length = model.config.max_position_embeddings
max_sequence = [0, 0, 0]
for i, row in train_data.iterrows():
    for i, (k, v) in enumerate(row.items()):
        max_sequence[i] = max(max_sequence[i], len(tokenizer(str(v))["input_ids"]))

print("Max sequence length of model:", max_sequence_length)
print("Max sequence length:", max_sequence)

"""131,072ì˜ ê¸¸ì´ë¼ë©´ í† í¬ë‚˜ì´ì§•ì„ í•œ ì´í›„ì˜ `max_length`ì¸ 319 + 109 ë³´ë‹¤ í›¨ì”¬ í¬ê¸° ë•Œë¬¸ì— ì¶©ë¶„íˆ ì—¬ìœ ë¡­ìŠµë‹ˆë‹¤.

# 3. Text-to-SQL task ë°ì´í„° ì „ì²˜ë¦¬
- í•™ìŠµ ëª©í‘œ : chat í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë… : ë°ì´í„° ì „ì²˜ë¦¬
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
    - apply_chat_templateìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°
    - datasetsë¥¼ ì´ìš©í•˜ì—¬ Datasets í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•˜ê¸°

í—ˆê¹…í˜ì´ìŠ¤ tokenizerì—ì„œëŠ” jinja2 í¬ë§·ì˜ `chat_template`ì„ ì œê³µí•©ë‹ˆë‹¤.

ë³´í†µ LLMì„ ì œê³µí•˜ëŠ” íšŒì‚¬, ëª¨ë¸ë§ˆë‹¤ `chat_template`ì´ ë‹¤ë¦…ë‹ˆë‹¤. ë˜í•œ, ìµœê·¼ì—ëŠ” `chat_template`ì„ ëŒ€ë¶€ë¶„ ì§€ì›í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ, `chat_template`ì„ í™•ì¸í•˜ê³  `apply_chat_template` ë§¤ì„œë“œë¥¼ ì´ìš©í•´ì„œ ì–´ë–»ê²Œ ì ìš©ì´ ë˜ëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.
"""

if tokenizer.chat_template:
    print("=== chat template ì‚¬ìš© ê°€ëŠ¥ ===\n")
    print(tokenizer.chat_template)
else:
    print("=== chat template ì‚¬ìš© ë¶ˆê°€ëŠ¥ ===\n")

"""Jinja2 í¬ë§·ì´ ì´ìƒí•˜ê¸´ í•˜ì§€ë§Œ ì½”ë“œë¥¼ ì–´ëŠì •ë„ ì´í•´í•˜ì‹œëŠ” ë¶„ë“¤ì´ë¼ë©´ ì´ê²Œ ì–´ë–¤ ì˜ë¯¸ì¸ì§€ë¥¼ ëŒ€ì¶© ìœ ì¶”í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```jinja
{% for message in messages %}
```
ì–´ë– í•œ `iterable` ê°ì²´ë¥¼ `for loop`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë´ì„œëŠ” `list` ë“±ì˜ ê°ì²´ë¡œ ë„£ì–´ì¤˜ì•¼ í• ê±° ê°™ìŠµë‹ˆë‹¤.


```jinja
{{'<|im_start|>' + message['role'] + '' + message['content'] + '<|im_end|>' + ''}}
```

`message` ë‚´ì— ë³´ë©´ `role`ê³¼ `content`ë¼ëŠ” í‚¤ê°’ì´ í•„ìš”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ `dictionary` ë“±ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•˜ë©´ ë ê±° ê°™ìŠµë‹ˆë‹¤.

ë˜í•œ, ì•ë’¤ì— `<|im_start|>`, `<|im_end|>`ì™€ ê°™ì€ ìŠ¤í˜ì…œ í† í°ì´ ë¶™ëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ ì €í¬ê°€ ê±´ë“¤ í•„ìš” ì—†ì´ ì•Œì•„ì„œ ìƒì„±í•´ì¤ë‹ˆë‹¤.

`apply_chat_template`ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ê¸° ìœ„í•´ì„œëŠ” chat(conversations) í˜•ì‹ì„ ì§€ì¼œì„œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤.

ê¸°ì¡´ì— ë¶ˆëŸ¬ì˜¨ ë°ì´í„°í”„ë ˆì„ì„ chat í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ê³  `apply_chat_template`ì„ ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.
"""

# í•™ìŠµìš© ë°ì´í„° ì „ì²˜ë¦¬

example = train_df[columns].iloc[0]
messages = [
    {
        "role": "system",
        "content": example["context"] # ì—¬ê¸°ì„œëŠ” ë¹ ë¥¸ í™•ì¸ì„ ìœ„í•´ input ì—´ì„ ë„£ìŠµë‹ˆë‹¤. ì¶”í›„ì— ì¢€ë” system promptë¥¼ ê³ ë„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    },
    {
        "role": "user",
        "content": example["question"]
    },
    {
        "role": "assistant",
        "content": example["answer"]
    }
]

print(messages)
print()
chat = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = False)
print(chat)

"""ë³´ì‹œëŠ” ë°”ì™€ ê°™ì´ ëª¨ë¸ì˜ ì…ë ¥ì— ë§ê²Œ í˜•ì‹ì„ ë§ì¶°ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì£¼ëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ì˜í•  ì ì´ í¬ê²Œ 2ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.
1. `assistant`ëŠ” ì •ë‹µì´ê¸° ë•Œë¬¸ì— í•™ìŠµì—ì„œë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê³  ì¶”ë¡ ì—ì„œëŠ” ë„£ì–´ì¤˜ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
2. `add_generation_prompt=False`ëŠ” `assistant`ê°€ ë“¤ì–´ê°”ê¸° ë•Œë¬¸ì— `False`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì¶”ë¡ ìš©ì´ë¼ë©´ `True`ë¡œ ì„¤ì •í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.
"""

## ì¶”ë¡ ìš© ë°ì´í„° ì „ì²˜ë¦¬
chat_inference = tokenizer.apply_chat_template(messages[:-1], tokenize = False, add_generation_prompt = True)
print(chat_inference)

"""ì¶”ë¡ ìš© ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ì‹œë©´

`SELECT * FROM players;<|im_end|>`

ì´ ì‚¬ë¼ì§„ ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìœ ì˜í•  ì ì€
1. ëª¨ë¸ì´ ì •ë‹µìœ¼ë¡œ ìƒì„±í•  í…ìŠ¤íŠ¸ : `SELECT * FROM players;`
2. EOS ìŠ¤í˜ì…œ í† í° : `<|im_end|>`

ë‘ê°€ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” `assistant` ë¶€ë¶„ì„ í•™ìŠµì— ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.

ì´ì œ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ë„£ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ë°ì´í„°ë¥¼ êº¼ë‚´ì„œ `Dataset` ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import datasets

system_prompt = """You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA."""
user_prompt = """Given the <USER_QUERY> and the <SCHEMA>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.

<SCHEMA>
{context}
</SCHEMA>

<USER_QUERY>
{question}
</USER_QUERY>"""

# TODO: ì§ì ‘ ì „ì²˜ë¦¬ ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”.
def convert_to_conversation(examples, tokenizer):
    train_data = []
    for i in range(len(examples["context"])):  # ì»¬ëŸ¼ëª…ìœ¼ë¡œ ê¸¸ì´ í™•ì¸
        messages = [
            {"role": "system", "content": examples["context"][i]},
            {"role": "user", "content": examples["question"][i]},
            {"role": "assistant", "content": examples["answer"][i]}
        ]
        train_data.append({
            "messages": messages
        })
    return train_data

train_data_list = convert_to_conversation(train_data, tokenizer)
train_dataset = datasets.Dataset.from_list(train_data_list)
print(train_dataset["messages"][0])

"""ë³´í†µ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” labelsëŠ” inputsì™€ ë™ì¼í•©ë‹ˆë‹¤.
(ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ labelsë¥¼ í•œ ì¹¸ ì˜¤ë¥¸ìª½ìœ¼ë¡œ shiftí•œ í›„ Cross-Entropy lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
ì´ëŠ” ë‹¤ìŒ í† í° ì˜ˆì¸¡(next token prediction)ê³¼ ê°™ì€ auto-regressive ë¶„ë¥˜ ì‘ì—…ì—ì„œ í‘œì¤€ ë°©ì‹ì…ë‹ˆë‹¤.)

ë”°ë¼ì„œ ì „ì²˜ë¦¬ëœ ìƒ˜í”Œì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ë©ë‹ˆë‹¤:
```python
{"input_ids": instruction + model response, "labels": instruction + model response}  # HF ëª¨ë¸ì´ shift +1 ì²˜ë¦¬ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰
```

í•˜ì§€ë§Œ ìš°ë¦¬ê°€ í•˜ë ¤ëŠ” ì‘ì—…ì€ instruction ë¶€ë¶„ì„ -100ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
{"input_ids": instruction + model response, "labels": [-100]*len(instruction) + model response}
```

ì´ë ‡ê²Œ í•˜ë©´ Cross-Entropy í•¨ìˆ˜ì— instruction í† í°ì€ ë¬´ì‹œ(ignore) í•˜ë¼ê³  ì•Œë ¤ì£¼ëŠ” ì…ˆì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´,

```
<|im_start|>system
You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.<|im_end|>
<|im_start|>user
Given the <USER_QUERY> and the <SCHEMA>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.

<SCHEMA>
CREATE TABLE players (
  player_id INT PRIMARY KEY AUTO_INCREMENT,
  username VARCHAR(255) UNIQUE NOT NULL,
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  date_joined DATETIME NOT NULL,
  last_login DATETIME
);
</SCHEMA>

<USER_QUERY>
ëª¨ë“  í”Œë ˆì´ì–´ ì •ë³´ë¥¼ ì¡°íšŒí•´ ì¤˜
</USER_QUERY><|im_end|>
<|im_start|>assistant
SELECT * FROM players;<|im_end|>
```

ìœ„ì—ì„œ í•™ìŠµí•´ì•¼ í•  ë¶€ë¶„(ì •ë‹µ ë¼ë²¨)ì€
```
SELECT * FROM players;<|im_end|>
```
ì´ ë¶€ë¶„ì´ë¯€ë¡œ, ë‚˜ë¨¸ì§€ ë¶€ë¶„ë“¤ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.
"""

def convert_train_data(examples, tokenizer):
    messages = examples["messages"]
    label_message = messages[-1]["content"]
    label_input_ids = tokenizer.encode(
        label_message, add_special_tokens=False, return_tensors="pt"
    ).squeeze(0)

    prompt_input_ids = tokenizer.apply_chat_template(
        messages[:2], add_generation_prompt=False, tokenize=True, return_tensors="pt"
    ).squeeze(0)

    response_start_template_ids = tokenizer.encode("<|im_start|>assistant", return_tensors="pt")[0]

    input_ids = torch.cat(
        [
            prompt_input_ids,
            response_start_template_ids,
            label_input_ids,
            torch.tensor([tokenizer.eos_token_id]),
        ],
        dim=0,
    )
    attention_mask = torch.ones(len(input_ids), dtype=torch.int64)
    labels = torch.cat(
        [
            torch.tensor(
                [-100] * (len(input_ids) - len(label_input_ids) - 1)
            ),  # prompt + label_start_template
            label_input_ids,  # label
            torch.tensor([tokenizer.eos_token_id]),  # [EOS]
        ],
        dim=0,
    )

    return (
        {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    )

train_dataset = train_dataset.map(
    lambda x: convert_train_data(x, tokenizer),
    batched=False,
    remove_columns=train_dataset.column_names  # ì›ë³¸ ì»¬ëŸ¼ ì œê±°í•˜ê³  ìƒˆ ì»¬ëŸ¼ë§Œ ë‚¨ê¹€
)

train_dataset

"""1. input_ids â€“ í† í¬ë‚˜ì´ì¦ˆëœ ì‹œí€€ìŠ¤ ë°ì´í„°
- í† í¬ë‚˜ì´ì €(tokenizer)ê°€ í…ìŠ¤íŠ¸ë¥¼ **ì„œë¸Œì›Œë“œ ë‹¨ìœ„(subword units)**ë¡œ ë¶„í• í•˜ê³ , ì´ë¥¼ **ì–´íœ˜ ì‚¬ì „(vocabulary)**ì— ì •ì˜ëœ **í† í° ì¸ë±ìŠ¤(token index)**ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- ì´ ì‹œí€€ìŠ¤ê°€ Transformer ëª¨ë¸ì˜ **ì„ë² ë”© ë ˆì´ì–´(embedding layer)**ì— ì…ë ¥ë˜ì–´, ê° ì •ìˆ˜ê°€ ê³ ì°¨ì› ë²¡í„°ë¡œ ì„ë² ë”©ë©ë‹ˆë‹¤.

2. attention_mask â€“ ì‹œí€€ìŠ¤ ë§ˆìŠ¤í‚¹ í…ì„œ
- TransformerëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê³ ì •ëœ ìµœëŒ€ ê¸¸ì´(max sequence length) ì…ë ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì‹¤ì œ ì…ë ¥ ê¸¸ì´ê°€ ì§§ìœ¼ë©´ íŒ¨ë”©(padding)ì„ ì¶”ê°€í•´ ë§ì¶”ëŠ”ë°, ì´ë•Œ íŒ¨ë”© í† í°ì€ self-attention ì—°ì‚°ì—ì„œ ë¬´ì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- attention_maskëŠ” ê°™ì€ ê¸¸ì´ì˜ ë°”ì´ë„ˆë¦¬ ë²¡í„°ë¡œ,
  - ì‹¤ì œ í† í° ìœ„ì¹˜ â†’ 1
  - íŒ¨ë”© í† í° ìœ„ì¹˜ â†’ 0
- ì´ ë§ˆìŠ¤í¬ëŠ” ì–´í…ì…˜ ìŠ¤ì½”ì–´(attention score) ê³„ì‚° ì‹œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì´ì „ì— ë§¤ìš° ì‘ì€ ìŒìˆ˜(âˆ’âˆ)ë¥¼ ë”í•´, íŒ¨ë”© ìœ„ì¹˜ì˜ ê¸°ì—¬ë„ë¥¼ ì™„ì „íˆ ì œê±°í•©ë‹ˆë‹¤.

3. labels â€“ í•™ìŠµ ëŒ€ìƒ(target) ì‹œí€€ìŠ¤
- labelsëŠ” **ëª¨ë¸ì˜ ì¶œë ¥ ë¡œì§“(logits)**ê³¼ ë¹„êµí•  íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤.
- ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ì‹œ ì˜¤í† ë ˆê·¸ë ˆì‹œë¸Œ(next-token prediction) ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ë¯€ë¡œ, labelsëŠ” ë³´í†µ input_idsì™€ ë™ì¼í•˜ì§€ë§Œ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸í•  í† í° ìœ„ì¹˜ë¥¼ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
- ì´ë ‡ê²Œ í•˜ë©´ Cross-Entropy Loss ê³„ì‚° ì‹œ ë¬´ì‹œëœ ìœ„ì¹˜ëŠ” loss=0ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
"""

print("input_ids : ", train_dataset["input_ids"][0])
print("attention_mask : ", train_dataset["attention_mask"][0])
print("labels : ", train_dataset["labels"][0])

"""# 4. Text-to-SQL task ëª¨ë¸ LoRA fine-tuning

LoRA ì„¤ì •ê³¼ Trainer ì„¤ì •ì„ í•˜ê³  ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

LoRA config ì„¤ì •ì„ í•©ë‹ˆë‹¤. ìì„¸í•œ ì„¤ì •ì€ [Huggingface LoRA Config](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.
"""

from peft import LoraConfig, get_peft_model
from transformers import TrainingArguments, Trainer

# LoRA Config ì„¤ì •
lora_config = LoraConfig(
    r=8,                          # LoRA rank (ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)
    lora_alpha=16,                # LoRA scaling factor (ë³´í†µ r*2)
    target_modules=[              # LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_dropout=0.05,            # Dropout ë¹„ìœ¨
    bias="none",                  # Bias í•™ìŠµ ì—¬ë¶€
    task_type="CAUSAL_LM"         # íƒœìŠ¤í¬ íƒ€ì…
)

"""### SFTConfig ì£¼ìš” ì„¤ì • ì„¤ëª…
ê° ì„¤ì •ë“¤ì€ ê¸°ë³¸ì ì¸ AIì— ëŒ€í•œ ì§€ì‹ì´ í•„ìš”í•œ ë¶€ë¶„ë“¤ì´ ë§ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ê° ì„¤ì •ë“¤ì€ í•˜ë‚˜í•˜ë‚˜ ë¸”ë¡œê·¸ ê¸€ì„ ì°¸ê³ í•´ê°€ë©° ê¹Šì´ ì´í•´í•´ì£¼ì„¸ìš”.

1. output_dir
- ì„¤ëª…: í•™ìŠµì´ ëë‚œ ëª¨ë¸ ê°€ì¤‘ì¹˜, ì²´í¬í¬ì¸íŠ¸, ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ (ë˜ëŠ” Hugging Face Hub ì €ì¥ì†Œ ì´ë¦„)
- ì¤‘ìš”ì„±: ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¬í˜„í•˜ê±°ë‚˜ ì´ì–´ì„œ í•™ìŠµí•˜ë ¤ë©´ í•„ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•¨

2. max_seq_length
- ì„¤ëª…: í•œ í•™ìŠµ ìƒ˜í”Œì˜ ìµœëŒ€ í† í° ê¸¸ì´
- ì˜í–¥:
	1. ê¸¸ì´ë¥¼ í¬ê²Œ í•˜ë©´ ë” ê¸´ ë¬¸ì¥ì„ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë‚˜ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
	2. ë„ˆë¬´ ì‘ìœ¼ë©´ ê¸´ ë¬¸ì¥ì´ ì˜ë ¤ì„œ í•™ìŠµ í’ˆì§ˆ ì €í•˜

3. packing
	- ì„¤ëª…: ì—¬ëŸ¬ ì§§ì€ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ì´ì–´ ë¶™ì—¬(max_seq_lengthê¹Œì§€) í•™ìŠµ
	- ì¥ì : GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ â†‘, í•™ìŠµ ì†ë„ â†‘ (íŒ¨ë”© ë‚­ë¹„ ê°ì†Œ)
	- ì£¼ì˜ì : ì‹œí€€ìŠ¤ ì‚¬ì´ë¥¼ êµ¬ë¶„í•˜ëŠ” special token(EOS ë“±)ì„ ë„£ì–´ì•¼ ë°ì´í„° ì„ì„ ë°©ì§€
  - PACKING ì˜ˆì‹œ

    ë°ì´í„°ì…‹ ìƒ˜í”Œì´ ê°ê° ë…ë¦½ëœ ì‹œí€€ìŠ¤ë¡œ í•™ìŠµë˜ê³  í•™ìŠµí•  í† í°ë“¤ì´ ì§§ìœ¼ë©´ íŒ¨ë”©ì´ ë§ì•„ì§‘ë‹ˆë‹¤.

    ```
    ìƒ˜í”Œ 1: [USER] ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ? [EOS] [PAD] [PAD] [PAD] ...
    ìƒ˜í”Œ 2: [USER] ë‚´ì¼ ë¹„ ì™€? [EOS] [PAD] [PAD] [PAD] ...
    ìƒ˜í”Œ 3: [USER] ì£¼ë§ ì¼ì • ë³´ì—¬ì¤˜ [EOS] [PAD] [PAD] ...
    ```
    GPUëŠ” [PAD]ë„ ì—°ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë‚­ë¹„ & ì†ë„ ì €í•˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.

    ì§§ì€ ìƒ˜í”Œë“¤ì„ ì´ì–´ë¶™ì´ê³ , ìƒ˜í”Œ ì‚¬ì´ì— EOS ê°™ì€ êµ¬ë¶„ í† í°ì„ ì¶”ê°€í•˜ì—¬ ìµœëŒ€ ê¸¸ì´(max_seq_length)ì— ë§ê²Œ í•˜ë‚˜ì˜ ê¸´ ì‹œí€€ìŠ¤ë¡œ ë¬¶ìŠµë‹ˆë‹¤.

    Packed ì‹œí€€ìŠ¤:
    ```
    [USER] ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ? [EOS]
    [USER] ë‚´ì¼ ë¹„ ì™€? [EOS]
    [USER] ì£¼ë§ ì¼ì • ë³´ì—¬ì¤˜ [EOS]
    ```
    - íŒ¨ë”©ì´ ê±°ì˜ ì—†ê³  GPU ì—°ì‚° íš¨ìœ¨ â†‘
    - í•œ ì‹œí€€ìŠ¤ ì•ˆì—ì„œ ì—¬ëŸ¬ ì˜ˆì‹œë¥¼ í•œ ë²ˆì— í•™ìŠµ â†’ í•™ìŠµ ì†ë„ â†‘
    - `Packing=False` : `[ìƒ˜í”Œ1][PAD PAD PAD] [ìƒ˜í”Œ2][PAD PAD PAD]`
    - `Packing=True` : `[ìƒ˜í”Œ1][EOS][ìƒ˜í”Œ2][EOS][ìƒ˜í”Œ3][EOS]	`

4. num_train_epochs
- ì„¤ëª…: ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ ê²°ì •
- íŒ: LoRA/QLoRAëŠ” ì ì€ Epoch(2~3)ìœ¼ë¡œë„ ì˜ ìˆ˜ë ´í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ

5. per_device_train_batch_size
- ì„¤ëª…: GPU í•œ ì¥(ë˜ëŠ” ì¥ì¹˜ í•˜ë‚˜)ì—ì„œ í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ê°œìˆ˜
- ì¡°ì ˆ í¬ì¸íŠ¸: GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì ˆ. ì‘ê²Œ í•˜ë©´ gradient_accumulation_stepsë¡œ ë³´ì™„ ê°€ëŠ¥

6. gradient_accumulation_steps
- ì„¤ëª…: ì—¬ëŸ¬ ìŠ¤í…ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•´ í•œ ë²ˆë§Œ ì—­ì „íŒŒ/ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
- ì¥ì : ì‘ì€ ë°°ì¹˜ë¡œë„ í° effective batch size êµ¬í˜„ ê°€ëŠ¥
- ì˜ˆì‹œ: batch_size=1, grad_accum=4 â†’ ì‹¤ì œ batch size = 4ì™€ ë¹„ìŠ·í•œ íš¨ê³¼
- `gradient_accumulation_steps`ì€ [ë§í¬]()ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.

7. gradient_checkpointing
- ì„¤ëª…: ìˆœì „íŒŒ(forward) ì¤‘ ì¼ë¶€ ì¤‘ê°„ ê°’ì„ ì €ì¥í•˜ì§€ ì•Šê³  í•„ìš”í•  ë•Œ ë‹¤ì‹œ ê³„ì‚°
- ì¥ì : GPU ë©”ëª¨ë¦¬ ì ˆì•½ (íŠ¹íˆ ëŒ€í˜• ëª¨ë¸ í•™ìŠµ ì‹œ í•„ìˆ˜)
- ë‹¨ì : ì—°ì‚°ëŸ‰ ì¦ê°€ â†’ í•™ìŠµ ì†ë„ ì†Œí­ ëŠë ¤ì§

8. optim
- ì„¤ëª…: ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì € ì„ íƒ (adamw_torch_fusedëŠ” PyTorchì˜ fused AdamW)
- ì¥ì : ì¼ë°˜ AdamWë³´ë‹¤ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ ì¢‹ìŒ (ì§€ì› GPU í•„ìš”)

9. logging_steps
- ì„¤ëª…: ëª‡ stepë§ˆë‹¤ í•™ìŠµ ë¡œê·¸ë¥¼ ê¸°ë¡í• ì§€
- íŒ: ë„ˆë¬´ ì‘ìœ¼ë©´ ë¡œê·¸ ê³¼ë‹¤, ë„ˆë¬´ í¬ë©´ í•™ìŠµ ì¶”ì  í˜ë“¦ â†’ 10~50 step ì •ë„ ê¶Œì¥

10. save_strategy
- ì„¤ëª…: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œì  (epoch, steps, no)
- ì˜ˆì‹œ: "epoch" â†’ ê° epochê°€ ëë‚  ë•Œë§ˆë‹¤ ì €ì¥

11. learning_rate
- ì„¤ëª…: ëª¨ë¸ í•™ìŠµë¥ 
- íŒ: QLoRA ë…¼ë¬¸ì—ì„œ 2e-4ê°€ ì•ˆì •ì ì´ë¼ê³  ê¶Œì¥
- ì£¼ì˜: ì§€ë‚˜ì¹˜ê²Œ ë†’ìœ¼ë©´ loss í­ì£¼, ë‚®ìœ¼ë©´ í•™ìŠµ ëŠë¦¼

12. fp16 / bf16
- ì„¤ëª…: í•™ìŠµ ì‹œ ì—°ì‚° ì •ë°€ë„ ì„ íƒ
- fp16: ëŒ€ë¶€ë¶„ GPU ì§€ì›, ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ ë¹ ë¦„
- bf16: ìµœì‹  GPU(Ampere ì´ìƒ)ì—ì„œ ê¶Œì¥, ì•ˆì •ì„± ë” ì¢‹ìŒ
- íŒ: Colab A100/T4 â†’ fp16 / A100(Ampere) ì´ìƒ â†’ bf16 ì¶”ì²œ

13. max_grad_norm
- ì„¤ëª…: Gradient clipping ê°’ (ë„ˆë¬´ í° gradientë¥¼ ì˜ë¼ í•™ìŠµ ì•ˆì •í™”)
- QLoRA ê¶Œì¥ê°’: 0.3

14. warmup_ratio
- ì„¤ëª…: í•™ìŠµ ì´ˆê¸°ì— learning rateë¥¼ ì²œì²œíˆ ì˜¬ë¦¬ëŠ” ë¹„ìœ¨
- ì¥ì : í•™ìŠµ ì•ˆì •í™”, ì´ˆê¸° ì†ì‹¤ í­ì£¼ ë°©ì§€
- QLoRA ë…¼ë¬¸ ê°’: 0.03 (~3% ë‹¨ê³„ëŠ” warmup)

15. lr_scheduler_type
- ì„¤ëª…: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ë°©ì‹
- constant: í•™ìŠµ ë‚´ë‚´ ì¼ì •í•œ í•™ìŠµë¥ 
- (ë‹¤ë¥¸ ì˜µì…˜: linear, cosine ë“±)

16. push_to_hub
- ì„¤ëª…: í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ Hugging Face Hubì— ìë™ ì—…ë¡œë“œí• ì§€ ì—¬ë¶€
- í˜‘ì—…/ê³µìœ : íŒ€ì›ì´ë‚˜ ê³µê°œ í”„ë¡œì íŠ¸ì— ìœ ìš©

17. report_to
- ì„¤ëª…: í•™ìŠµ ë©”íŠ¸ë¦­ì„ ì–´ë””ë¡œ ë³´ë‚¼ì§€ (ì˜ˆ: "tensorboard", "wandb")
- ì¥ì : í•™ìŠµ ê³¼ì • ì‹œê°í™” ê°€ëŠ¥

í•™ìŠµ ì„¤ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤.
"""

from trl import SFTConfig, SFTTrainer

output_dir = "outputs"
num_train_epochs=1
per_device_train_batch_size = 1 # GPUê°€ ë¶€ì¡±í•˜ë‹¤ë©´ í•´ë‹¹ ê°’ì„ ì¤„ì—¬ì£¼ì„¸ìš”.
gradient_accumulation_steps = 4
warmup_ratio = 0.03
max_steps = 100 # í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ë©´ í•´ë‹¹ ê°’ì„ ì¤„ì—¬ì£¼ì„¸ìš”.
learning_rate = 5e-5
logging_steps = 1
weight_decay = 0.01
max_grad_norm=1.0
lr_scheduler_type = "linear"
report_to = "none" # Use this for WandB etc
bf16=False
gradient_checkpointing=False
optim="adamw_torch"

train_cfg = SFTConfig(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    learning_rate=learning_rate,
    lr_scheduler_type=lr_scheduler_type,
    max_grad_norm=max_grad_norm,
    warmup_ratio=warmup_ratio,
    weight_decay=weight_decay,
    logging_steps=logging_steps,
    save_steps=max_steps,
    max_steps=max_steps,
    fp16=False if bf16 else True,
    bf16=bf16,
    report_to=report_to,
    gradient_checkpointing=gradient_checkpointing,
    optim=optim,
)

"""í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."""

trainer = SFTTrainer(
    model = model,
    processing_class = tokenizer,
    train_dataset = train_dataset.select(range(max_steps * per_device_train_batch_size)), # max_steps * batch_size ë§Œí¼ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.
    peft_config=peft_config,
    args = train_cfg,
)

trainer_stats = trainer.train()
print(trainer_stats)

# ============================================
# Save LoRA adapter (small)
# ============================================
merge_and_save = False
if merge_and_save:
    trainer.model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"LoRA adapter saved to: {output_dir}")

# ============================================
# (Optional) Merge LoRA into base weights and save a full model
# WARNING: creates a large model; only do this if you need standalone weights
# ============================================
if merge_and_save:
    from peft import AutoPeftModelForCausalLM
    merged_model = AutoPeftModelForCausalLM.from_pretrained(
        output_dir,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,
        device_map="auto",
    )
    merged_model = merged_model.merge_and_unload()
    merged_dir = output_dir + "-merged"
    merged_model.save_pretrained(merged_dir, safe_serialization=True)
    tokenizer.save_pretrained(merged_dir)
    print(f"Merged full model saved to: {merged_dir}")

"""í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."""

text_idx = len(train_df) - 1

test_input = [
    {'role': 'system', 'content': system_prompt},
    {'role': 'user', 'content': user_prompt.format(question=train_df["question"][text_idx], context=train_df["context"][text_idx])},
]
print("ì •ë‹µ :", train_df["answer"][text_idx])

inputs = tokenizer.apply_chat_template(test_input, add_generation_prompt=True, return_dict=True, return_tensors="pt")
input_prompt = tokenizer.apply_chat_template(test_input, add_generation_prompt=True, tokenize=False)
with torch.no_grad():
    output_ids = model.generate(
        **inputs.to("cuda"),
        max_new_tokens=128,
        stop_strings=["<|endofturn|>", "<|stop|>"],
        tokenizer=tokenizer
    )

print(tokenizer.batch_decode(output_ids)[0][len(input_prompt) - 1:])