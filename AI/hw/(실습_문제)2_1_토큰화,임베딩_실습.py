# -*- coding: utf-8 -*-
"""(ì‹¤ìŠµ-ë¬¸ì œ)2-1_í† í°í™”,ì„ë² ë”©_ì‹¤ìŠµ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1njMwUe98hbw3K_WuFN7pioU_AkASAHcz

### **Content License Agreement**

<font color='red'><b>**WARNING**</b></font> : ë³¸ ìë£ŒëŠ” ì‚¼ì„±ì²­ë…„SWÂ·AIì•„ì¹´ë°ë¯¸ì˜ ì»¨í…ì¸  ìì‚°ìœ¼ë¡œ, ë³´ì•ˆì„œì•½ì„œì— ì˜ê±°í•˜ì—¬ ì–´ë– í•œ ì‚¬ìœ ë¡œë„ ì„ì˜ë¡œ ë³µì‚¬, ì´¬ì˜, ë…¹ìŒ, ë³µì œ, ë³´ê´€, ì „ì†¡í•˜ê±°ë‚˜ í—ˆê°€ ë°›ì§€ ì•Šì€ ì €ì¥ë§¤ì²´ë¥¼ ì´ìš©í•œ ë³´ê´€, ì œ3ìì—ê²Œ ëˆ„ì„¤, ê³µê°œ ë˜ëŠ” ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë¬´ë‹¨ ì‚¬ìš© ë° ë¶ˆë²• ë°°í¬ ì‹œ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **Objectives**

1. ì‹¤ìŠµëª…: í† í°í™”/ì„ë² ë”© ì‹¤ìŠµ
2. í•µì‹¬ ì£¼ì œ
    1) tokenizerë¥¼ ì´ìš©í•˜ì—¬ ë‹¨ì–´ë“¤ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´
    2) í† í°í™”ëœ í† í°ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´
    3) RNNë¶€í„° íŠ¸ëœìŠ¤í¬ë¨¸ê¹Œì§€ ëª¨ë¸ì˜ ë°œì „ì‚¬ë¥¼ ì§ì ‘ ì²´í—˜í•˜ê³  ê° ìš”ì†Œ ê¸°ìˆ ì˜ ì—­í• ì„ ì´í•´
3. í•™ìŠµ ëª©í‘œ
    1) í† í¬ë‚˜ì´ì €ê°€ ë¬´ì—‡ì´ê³  í† í°í™”ê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
    2) í† í°í™”ë¥¼ ì™œ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
    3) í† í°í™”ëœ í† í°ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.
    4) ì„ë² ë”© ë²¡í„°ë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–¤ ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
    5) ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ë°œì „ì‚¬ì— ëŒ€í•´ ì§ì ‘ ì²´í—˜í•˜ê³  ê° ì•„í‚¤í…ì³ê°€ ê°€ì§€ëŠ” íŠ¹ì§•ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.

4. í•™ìŠµ ê°œë…
    1) í† í°í™”:
    2) ì„ë² ë”© ë²¡í„°:
    3) ì¸ì½”ë”/ë””ì½”ë”:
  
5. í•™ìŠµ ë°©í–¥
    - ì‹¤ìŠµì€ ì•„ë˜ ë‚´ìš©ë“¤ì„ ì§ì ‘ ì²´í—˜í•˜ê³  ê° ì•„í‚¤í…ì³ê°€ ê°€ì§€ëŠ” íŠ¹ì§•ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
      - í† í°í™”
      - ì„ë² ë”©
      - RNN
      - LSTM
      - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
      - ì¸ì½”ë”
      - ë””ì½”ë”
    - ì‹¤ìŠµ ì½”ë“œëŠ” ì¡°êµê°€ ì§ì ‘ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ë©° í•™ìŠµí•©ë‹ˆë‹¤.
    - ìì—°ìŠ¤ëŸ½ê²Œ ì½”ë“œë¥¼ êµ¬í˜„í•˜ë©´ì„œ ì•„í‚¤í…ì³ì˜ ë°œì „ì‚¬ë¥¼ ì²´í—˜í•©ë‹ˆë‹¤.

6. ë°ì´í„°ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´
    - ë°ì´í„°ì…‹ ëª… : NSMC(Naver Sentiment Movie Corpus)
    - ë°ì´í„°ì…‹ ê°œìš” : ë„¤ì´ë²„ ì˜í™” ê°ì •ë¶„ì„ ë°ì´í„°ì…‹
    - ë°ì´í„°ì…‹ ì €ì‘ê¶Œ : CC0 1.0

### **Prerequisites**
```
numpy==2.0.2
pandas==2.2.2
tokenizers==0.21.4
transformers==4.55.2
torch==2.8.0+cu126
```

- ë§Œì•½, ê¸°ë³¸ ì½”ë©ê³¼ ë²„ì „ì´ ë‹¤ë¥´ë‹¤ë©´ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ë³µì‚¬í•´ì„œ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.
```
%pip install numpy==2.0.2 pandas==2.2.2 tokenizers==0.21.4 transformers==4.55.2 torch==2.8.0+cu126 --index-url https://download.pytorch.org/whl
```
"""

import torch
import torch.nn as nn
import numpy as np
from typing import (
    Generic,
    Tuple,
    TypeVar,
    List,
    Union,
    get_args
)
# ì‹œë“œ ì„¤ì •
np.random.seed(1234)
torch.manual_seed(1234)
torch.cuda.manual_seed(1234)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

Batch = TypeVar("Batch", bound=int)
Token = TypeVar("Token", bound=int)
Sequence = TypeVar("Sequence", bound=int)
Layers = TypeVar("Layers", bound=int)
HiddenStates = TypeVar("HiddenStates", bound=int)
VocabSize = TypeVar("VocabSize", bound=int)
EmbeddingSize = TypeVar("EmbeddingSize", bound=int)
MaxLength = TypeVar("MaxLength", bound=int)

_1D = TypeVar("_1D")
_2D = TypeVar("_2D")
_3D = TypeVar("_3D")

def _label_str(self) -> str:
    """ì¸ìŠ¤í„´ìŠ¤ì˜ ì œë„¤ë¦­ ë¼ë²¨ ì´ë¦„ì„ ì˜ˆì˜ê²Œ í‘œì‹œ (e.g., [Sequence])"""
    oc = getattr(self, "__orig_class__", None)
    if oc is None:
        return "[]"
    args = get_args(oc)
    names = [getattr(a, "__name__", str(a)) for a in args]
    return "[" + ", ".join(names) + "]"


class Tensor1D(Generic[_1D]):
    def __init__(self, tensor: torch.Tensor):
        assert tensor.dim() == 1, ValueError("Tensor must be 1-dimensional")
        self.tensor = tensor
        self.s: _1D = tensor.size(0)  # sequence length

    def size(self) -> Tuple[int, int]:
        return self.tensor.size()

    def __repr__(self) -> str:
        return f"Tensor(shape=({self.s}))"

class Tensor2D(Generic[_1D, _2D]):
    def __init__(self, tensor: torch.Tensor):
        assert tensor.dim() == 2, ValueError("Tensor must be 2-dimensional")
        self.tensor = tensor
        self.b: _1D = tensor.size(0)  # batch size
        self.s: _2D = tensor.size(1)  # sequence length
        assert self.b == tensor.size(0), ValueError(
            f"Expected batch {self.b}, but got {tensor.size(0)}"
        )
        assert self.s == tensor.size(1), ValueError(
            f"Expected Sequence {self.s}, but got {tensor.size(1)}"
        )

    def size(self) -> Tuple[int, int]:
        return self.tensor.size()

    def __repr__(self) -> str:
        return f"Tensor(shape=({self.b}, {self.s}))"


class Tensor3D(Generic[_1D, _2D, _3D]):
    def __init__(self, tensor: torch.Tensor):
        assert tensor.dim() == 3, ValueError("Tensor must be 3-dimensional")
        self.tensor = tensor
        self.b: _1D = tensor.size(0)  # batch size
        self.s: _2D = tensor.size(1)  # sequence length
        self.h: _3D = tensor.size(2)  # hidden state size
        assert self.b == tensor.size(0), ValueError(
            f"Expected batch {self.b}, but got {tensor.size(0)}"
        )
        assert self.s == tensor.size(1), ValueError(
            f"Expected Sequence {self.s}, but got {tensor.size(1)}"
        )
        assert self.h == tensor.size(2), ValueError(
            f"Expected Hidden State {self.h}, but got {tensor.size(2)}"
        )

    def size(self) -> Tuple[int, int]:
        return self.tensor.size()

    def __repr__(self) -> str:
        return f"Tensor(shape=({self.b}, {self.s}, {self.h}))"

"""# 1. í† í¬ë‚˜ì´ì € / ì›Œë“œ ì„ë² ë”©

- í•™ìŠµ ëª©í‘œ
  1. í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  2. í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í° ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆã….
- í•™ìŠµ ê°œë…
  1. í† í¬ë‚˜ì´ì €
  2. í† í°í™”
  3. ì„ë² ë”©
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. ì œê³µëœ ë§ë­‰ì¹˜ë¡œ WordPiece í† í¬ë‚˜ì´ì €ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ì½”ë“œ í•œ ì¤„ì„ ì™„ì„±
  2. í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ íŠ¹ì • ë¬¸ì¥ì„ í† í° ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ
  3. nn.Embedding ë ˆì´ì–´(í˜¹ì€ ê°„ë‹¨í•œ dict lookup)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í† í° IDì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì¡°íšŒí•˜ëŠ” ì½”ë“œ

### 1.1. Tokenizer í•™ìŠµ

<blockquote>
<b>ğŸ§  í† í¬ë‚˜ì´ì € í•™ìŠµ</b><br>
ì–¸ì–´ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.
</blockquote>

í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ ë‘ê°€ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.
1. í† í¬ë‚˜ì´ì € ê°ì²´(í´ë˜ìŠ¤)
2. í•™ìŠµ ë°ì´í„°

ê·¸ëŸ¬ë©´ ìš°ì„  í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ë³´ê² ìŠµë‹ˆë‹¤.

í•™ìŠµí•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” NSMC(Naver Sentiment Movie Corpus) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.
"""

!wget https://github.com/e9t/nsmc/raw/master/ratings.txt

"""ë°ì´í„°ì…‹ì„ í™•ì¸í•´ë´…ë‹ˆë‹¤."""

import pandas as pd
import os

file_list = os.listdir()
for file in file_list:
    if "ratings.txt" == file:
        print('í•™ìŠµì— í•„ìš”í•œ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤!', file)
        df = pd.read_table( (os.getcwd() + '/' + file), encoding='utf-8') # ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³´ê¸° í¸í•˜ê²Œ ë°”ê¿”ì¤ì‹œë‹¤!
        df = df.dropna(how = 'any') # ë„ê°’ì„ ì—†ì• ì¤ë‹ˆë‹¤!
        print('ë¦¬ë·° ê°¯ìˆ˜ :', len(df))
df.head()

"""í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆëŠ” 'document'ì—´ë§Œì„ ê°€ì ¸ì˜¤ê³ 

í•´ë‹¹ ë°ì´í„°ë¥¼ txt íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

with open((os.getcwd() + '/' + 'naver_review.txt'), 'w', encoding='utf8') as f:
    # TODO: document ì—´ë§Œ ê°€ì ¸ì™€ì„œ ì €ì¥í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    f.write('\n'.join(df['document'].tolist()))

"""í•™ìŠµì´ ë˜ì–´ ìˆì§€ ì•Šì€ ë¹ˆ tokenizerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” BertWordPieceTokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

##### íŒŒë¼ë¯¸í„°:
- `strip_accents` : ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì•…ì„¼íŠ¸(ì•¡ì„¼íŠ¸)ë¥¼ ì œê±°í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ì˜µì…˜ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¥¼ í•™ìŠµí• ë•Œì—ëŠ” `False`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
- `lowercase` : ì˜ì–´ë¥¼ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¿‰ë‹ˆë‹¤. `False`ë¡œ ì„¤ì •í•˜ë©´ ì˜ì–´ë¥¼ ëŒ€ë¬¸ìë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
"""

from tokenizers import BertWordPieceTokenizer

# ë¹ˆ tokenizer ìƒì„± : vocabulary_size = 0 ì¸ ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
tokenizer = BertWordPieceTokenizer(
    lowercase=False,
    strip_accents=False,
)
tokenizer

"""ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
#### íŒŒë¼ë¯¸í„° ì„¤ëª…:
- `data_file` : ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•´ì¤ë‹ˆë‹¤. list í˜•íƒœë¡œ ì—¬ëŸ¬ê°œì˜ íŒŒì¼ì„ ì§€ì •í•´ì¤„ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
- `vocab_size (default: 30000)` : ë‹¨ì–´ì‚¬ì „ í¬ê¸°ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë– í•œ ê°’ì´ ê°€ì¥ ì¢‹ë‹¤ëŠ” ê²ƒì€ ì—†ì§€ë§Œ, ê°’ì´ í´ìˆ˜ë¡ ë§ì€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‹´ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `initial_alphabet` : ê¼­ í¬í•¨ëìœ¼ë©´ í•˜ëŠ” initial alphabetì„ í•™ìŠµ ì „ì— ì¶”ê°€í•´ì¤ë‹ˆë‹¤.
    - initialì€ í•™ìŠµí•˜ê¸° ì´ì „ì— ë¯¸ë¦¬ ë‹¨ì–´ë¥¼ vocabì— ë„£ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    - special tokenë“¤ë„ initialì— vocabì— ì¶”ê°€ë©ë‹ˆë‹¤.
- `limit_alphabet (default: 1000)` : initial tokensì˜ ê°¯ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
- `min_frequency (default: 2)` : ìµœì†Œ ë¹ˆë„ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ ì–´ë–¤ ë‹¨ì–´ê°€ 1ë²ˆ ë‚˜ì˜¤ë©´ vocabì— ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- `special_tokens` : íŠ¹ìˆ˜ í† í°ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.. BERTì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í† í°ì´ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.
    - `[PAD]` : íŒ¨ë”©ì„ ìœ„í•œ í† í°
    - `[UNK]` : OOV ë‹¨ì–´ë¥¼ ìœ„í•œ í† í°
    - `[CLS]` : ë¬¸ì¥ì˜ ì‹œì‘ì„ ì•Œë¦¬ê³  ë¶„ë¥˜ ë¬¸ì œì— ì‚¬ìš©ë˜ëŠ” í† í°
    - `[SEP]` : ë¬¸ì¥ ì‚¬ì´ì‚¬ì´ë¥¼ êµ¬ë³„í•´ì£¼ëŠ” í† í°
    - `[MASK]` : MLM íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ë§ˆìŠ¤í¬ í† í°
- `wordpiece_prefix(default: '##')` : sub-wordë¼ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ëŠ” í‘œì‹œì…ë‹ˆë‹¤.
    - BERTëŠ” ê¸°ë³¸ì ìœ¼ë¡œ '##'ì„ ì”ë‹ˆë‹¤.
    - ì˜ˆë¥¼ ë“¤ì–´, `SS, ##AF, ##Y` ì²˜ëŸ¼ sub-wordë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ '##'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `show_progress` : í•™ìŠµ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

data_file = 'naver_review.txt'
vocab_size = 30000
min_frequency = 2
initial_alphabet = []
limit_alphabet = 6000
special_tokens = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
wordpieces_prefix = "##"
show_progress=True

tokenizer.train(
    files = data_file,
    vocab_size = vocab_size,
    min_frequency = min_frequency,
    initial_alphabet = initial_alphabet,
    limit_alphabet = limit_alphabet,
    special_tokens = special_tokens,
    wordpieces_prefix = wordpieces_prefix,
    show_progress = True,
)

vocab = tokenizer.get_vocab()
print("vocab size : ", len(vocab))
print(sorted(vocab, key=lambda x: vocab[x])[:20])

"""### 1.2. í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•œ í† í° ID ì‹œí€€ìŠ¤ ë°˜í™˜

<blockquote>
<b>ğŸ§  í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•œ í† í° ID ì‹œí€€ìŠ¤ ë°˜í™˜</b><br>
ëª¨ë¸ì´ í† í°ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì •ìˆ˜ê°’ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í† í°ì„ ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
</blockquote>

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í† í°ì„ ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

text = "I'm a student of SSAFY!"

encoded = tokenizer.encode(text)
print('ğŸŒ±í† í°í™” ê²°ê³¼ :',encoded.tokens)
print('ğŸŒ±ì •ìˆ˜ ì¸ì½”ë”© :',encoded.ids)
print('ğŸŒˆë””ì½”ë”© :',tokenizer.decode(encoded.ids))

"""<blockquote>
<b>ğŸ§  í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•œ ëª¨ë¸ ì…ë ¥ ë§Œë“¤ê¸°</b><br>
ê·¸ë ‡ë‹¤ë©´ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í• ê¹Œìš”?
</blockquote>

ìœ„ì— ëŒ€í•œ ë‹µë³€ì€ ì•ìœ¼ë¡œ ì‹¤ìŠµ ì½”ë“œë¥¼ ì§„í–‰í•˜ë©´ì„œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì´ ì ì„ ìŠì§€ ë§ê³  ê³„ì† ë”°ë¼ê°€ì‹œê¸° ë°”ëë‹ˆë‹¤.

### 1.3. ì„ë² ë”© ë²¡í„°

<blockquote>
<b>ğŸ§  í† í° IDì— ë”°ë¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë²¡í„°í™”ê°€ ë ê¹Œìš”?</b><br>
í† í° IDì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
</blockquote>

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ íŠ¹ì • í† í° IDì— ë”°ë¥¸ ì„ë² ë”© ë²¡í„°ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

ì„ë² ë”© ë²¡í„°ëŠ” torchì˜ nn.Embedding ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë©ë‹ˆë‹¤. í•´ë‹¹ ì„ë² ë”© ë²¡í„°ëŠ” ëª¨ë‘ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
"""

# embedding_vector = nn.Embedding()

"""ì„ë² ë”© ë²¡í„°ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ê³  í•˜ë‹ˆ ë‹¤ìŒ ë‘ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜ë“œì‹œ ë„£ìœ¼ë¼ê³  í•©ë‹ˆë‹¤.

1. `num_embeddings`: ì„ë² ë”© ì‚¬ì „ì˜ í¬ê¸° (size of the dictionary of embeddings)
2. `embedding_dim`: ê° ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› (the size of each embedding vector)

<blockquote>
<b>ğŸ§  num_embeddings </b><br>
ì„ë² ë”© ì‚¬ì „ì˜ í¬ê¸°ëŠ” ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œìš”?
</blockquote>

ì—¬ê¸°ì„œ `num_embeddings`ëŠ” ê³ ìœ í•œ í† í°(ë‹¨ì–´, ë¬¸ì ë“±)ì˜ ì´ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ì–´ë–¤ `ì¸ë±ìŠ¤ â†’ ë²¡í„°` ë§¤í•‘ í…Œì´ë¸”ì„ ë§Œë“¤ ê±´ë°, ê·¸ í…Œì´ë¸”ì— ëª‡ ê°œì˜ í•­ëª©ì´ ë“¤ì–´ê°€ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. tokenizerë¥¼ ë§Œë“¤ë•Œ `vocab_size`ì™€ ë™ì¼í•œ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  embedding_dim </b><br>
ê° ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì€ ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œìš”?
</blockquote>

`embedding_dim`ì€ ê° ë‹¨ì–´(ë˜ëŠ” í† í°)ê°€ í‘œí˜„ë˜ëŠ” ë²¡í„°ì˜ ê¸¸ì´ì…ë‹ˆë‹¤. ì¦‰, í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì–´ë–¤ ìˆ«ì ë²¡í„°ë¡œ ë‚˜íƒ€ë‚¼ ë•Œ ê·¸ ë²¡í„°ê°€ ëª‡ ì°¨ì›ì¸ì§€ ì •í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. ë³´í†µì˜ embeddingì€ `768`, `1024` ë“± 2ì˜ ì œê³±ìˆ˜ ì°¨ì›ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ("ì–´ë–¤ ê°’ì´ ì •ë‹µì´ë‹¤" í•˜ëŠ” ê°’ì´ ìˆëŠ” ê±´ ì•„ë‹™ë‹ˆë‹¤.)

ì—¬ê¸°ì„œëŠ” vocab_sizeì™€ embedding_dimì„ 768ë¡œ ì •ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤.
"""

embedding_vector: Tensor2D[VocabSize, EmbeddingSize] = nn.Embedding(vocab_size, 768)
embedding_vector.weight.shape

"""ê·¸ëŸ¬ë©´ íŠ¹ì • í† í°ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."""

token_id = tokenizer.token_to_id("I")
print("token_id:", token_id)
input_id = torch.tensor([token_id], dtype=torch.long)
print("input_id ì°¨ì›:", input_id.shape)

vector = embedding_vector(input_id)
print("vector ì°¨ì›:", vector.shape)
print("vector:", vector)

"""# 2. RNN/LSTM

- í•™ìŠµ ëª©í‘œ
  1. RNN/LSTMì„ ì´ìš©í•˜ì—¬ ë¬¸ì¥ ì „ì²´ì˜ ì •ë³´ë¥¼ ì••ì¶•í•œ ë¬¸ë§¥ ë²¡í„°ì— ëŒ€í•œ ì´í•´ë¥¼ í•  ìˆ˜ ìˆë‹¤.
  2. Encoder Decoder êµ¬ì¡°ë¥¼ í†µí•´ ë¬¸ë§¥ ë²¡í„°ë¥¼ ì´ìš©í•˜ì—¬ íŠ¹ì • taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë…
  1. RNN/LSTM
  2. Encoder/Decoder
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. ê°„ë‹¨í•œ RNN/LSTMì„ êµ¬í˜„í•œë‹¤.
  2. ë²ˆì—­ taskì™€ ê´€ë ¨ëœ encoder decoder êµ¬ì¡°ë¥¼ êµ¬í˜„í•œë‹¤.

<blockquote>
<b>ğŸ§  Recurrent Neural Network(RNN)ì´ë€? </b><br>
ìˆœì°¨ì (Sequential) ì´ì „ì˜ ì •ë³´ë¥¼ ê¸°ì–µí•˜ì—¬ í˜„ì¬ì˜ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
</blockquote>

RNNì´ ê°–ëŠ” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- ì…ë ¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- RNNì€ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì¬ê·€ì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

ê·¸ëŸ¬ë©´ ì´ì œë¶€í„° ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ RNNì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì„œ ì¶œë ¥ì¸µì˜ ê²°ê³¼ê°’ì„ ë°›ì•„ë´…ì‹œë‹¤!

í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê¸° ìœ„í•´ì„œëŠ” ìœ„ì—ì„œ ë³´ì•˜ë“¯, ì›Œë“œ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
ì›Œë“œ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.
"""

word_embeddings: Tensor2D[VocabSize, EmbeddingSize] = nn.Embedding(vocab_size, 768)
print("ì›Œë“œ ì„ë² ë”© ì°¨ì› :", word_embeddings.weight.shape)

"""ì›Œë“œ ì„ë² ë”© ì°¨ì›ì— ë§ê²Œ RNNì„ êµ¬í˜„í•©ë‹ˆë‹¤."""

input_size: int = word_embeddings.weight.size()[1] # RNNì˜ input sizeëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
hidden_size: int = 1024  # RNNì˜ hidden size
num_layers: int = 1  # ìŒ“ì„ RNN layerì˜ ê°œìˆ˜
bidirectional: bool = False  # ë‹¨ë°©í–¥ RNN

rnn = nn.RNN(
    input_size=input_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
    bidirectional=bidirectional
)

# ì´ˆê¸° hidden state ì´ˆê¸°í™”

hidden_state_shape: int = (num_layers * (2 if bidirectional else 1), hidden_size)

h_0: Tensor2D[Sequence, HiddenStates] = torch.zeros(hidden_state_shape)  # (num_layers * num_dirs, hidden_size)
print("h_0ì˜ ì°¨ì› :",h_0.shape)

"""ì…ë ¥ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”í•œ í›„, idsë§Œ êº¼ëƒ…ë‹ˆë‹¤."""

text: str = "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤."

# í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
encoded = tokenizer.encode(text)
# í† í°ì˜ idsë§Œ êº¼ëƒ…ë‹ˆë‹¤.
input_ids: List[int] = encoded.ids

# í…ì„œí™”ë¥¼ í•©ë‹ˆë‹¤.
input_ids: Tensor1D[Sequence] = torch.tensor(input_ids, dtype=torch.long)
input_ids

"""ë³€í™˜ëœ input_idsë¥¼ ì›Œë“œ ì„ë² ë”©ìœ¼ë¡œ ë„£ê³ 
ì›Œë“œ ì„ë² ë”©ì„ RNNì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ë‘ outputì„ ì–»ìŠµë‹ˆë‹¤.

1. `hidden_states`: ê° time stepì— í•´ë‹¹í•˜ëŠ” hidden stateë“¤ì˜ ë¬¶ìŒ.
2. `h_n`: ëª¨ë“  sequenceë¥¼ ê±°ì¹˜ê³  ë‚˜ì˜¨ ë§ˆì§€ë§‰ hidden state(`last hidden state`). hidden_statesì˜ ë§ˆì§€ë§‰ê³¼ ë™ì¼.
"""

input_embeds: Tensor2D[Sequence, EmbeddingSize] = word_embeddings(input_ids)
print("ì›Œë“œ ì„ë² ë”© ì°¨ì› : ", input_embeds.shape)  # (vocab_size, embedding_dim)
outputs = rnn(input_embeds, h_0)
hidden_states: Tensor2D[Sequence, HiddenStates] = outputs[0]
h_n: Tensor2D[Layers, HiddenStates] = outputs[1]

# sequence_length: input_tokenì˜ ê¸¸ì´(length), hidden size: hidden state ì°¨ì› ìˆ˜, num_layers: layer ê°œìˆ˜, num_dirs: ë°©í–¥ì˜ ê°œìˆ˜
print("hidden_states ì°¨ì› : ", hidden_states.shape)  # (sequence_length, d_h)
print("h_n ì°¨ì› : ", h_n.shape)  # (num_layers * num_dirs, d_h) = (1, d_h)

if torch.equal(hidden_states[-1].unsqueeze(0), h_n):
    print("hidden_statesì˜ ë§ˆì§€ë§‰ê³¼ h_nì´ ê°™ìŠµë‹ˆë‹¤.")

"""ê·¸ëŸ¬ë©´ ì´ëŸ¬í•œ ì€ë‹‰ ìƒíƒœ(hidden state)ë¥¼ ì–»ì–´ì„œ ì–´ë– í•œ ì‘ì—…ì„ í•  ìˆ˜ ìˆì„ê¹Œìš”?

<blockquote>
<b>ğŸ§  ì€ë‹‰ ìƒíƒœ(hidden state)ëŠ” ë¬¸ì¥ì˜ ì •ë³´ë“¤ì„ ì••ì¶•ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.</b><br>
RNN layerë¥¼ í†µê³¼í•˜ë©´ì„œ ë¬¸ì¥ ì „ì²´ì˜ ì •ë³´ë¥¼ ì••ì¶•í•˜ê²Œ ë˜ê³  ì´ëŸ¬í•œ ì •ë³´ë“¤ì€ hidden stateì— ë‹´ê¸°ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ hidden stateëŠ” ë¬¸ë§¥ ë²¡í„°(context vector)ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
</blockquote>

ë¬¸ë§¥ ë²¡í„°(context vector)ëŠ” ì…ë ¥ ë¬¸ì¥ì˜ ì •ë³´ë“¤ì„ ë²¡í„°ìƒì— ì••ì¶•í•˜ì—¬ ì €ì¥í•œ ê²ƒìœ¼ë¡œ, ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ë²ˆì—­(translation) taskë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ hidden stateë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

ë²ˆì—­ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” last hidden stateë¥¼ ë‹¤ì‹œ ì €í¬ì˜ ì…ë ¥ ë°ì´í„°ì™€ ìœ ì‚¬í•œ í˜•íƒœì¸ í…ìŠ¤íŠ¸(í† í°) idë¡œ ë³€í™˜í•˜ëŠ” layerê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ì €í¬ëŠ” Decoderë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

![image](https://raw.githubusercontent.com/Ssunbell/TIL/refs/heads/master/assets/Seq2SeqRNN.png)

ê·¸ëŸ¬ë©´ ì•„ë˜ì—ì„œ Encoderì™€ Decoderë¥¼ ì—°ê²°í•˜ì—¬ ë²ˆì—­ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € ì¸ì½”ë”ë¥¼ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ êµ¬í˜„í•œ rnnì„ ê·¸ëŒ€ë¡œ ì´ìš©í•˜ì—¬ í´ë˜ìŠ¤í™”ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤.
"""

from abc import ABC, abstractmethod

# ì¸ì½”ë” ëª¨ë¸ì€ RNNì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì•„ë˜ëŠ” ì¶”ìƒí™” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
class Encoder(nn.Module, ABC):
    def __init__(self: "Encoder") -> None:
        super().__init__()
        pass

    @abstractmethod
    def forward(self: "Encoder", input_ids: torch.Tensor) -> torch.Tensor:
        # forwardì—ì„œ ì‹¤ì œë¡œ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë ˆì´ì–´ë¥¼ ìŒ“ìŠµë‹ˆë‹¤.
        pass

class RNNEncoder(Encoder):
    def __init__(
        self: "RNNEncoder",
        vocab_size: int,
        embedding_dim: int,
        hidden_size: int,
        num_layers: int,
        bidirectional: bool,
    ) -> None:
        super().__init__()
        # word embedding layer
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # rnn layer
        self.rnn = nn.RNN(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
        )

    def forward(
        self: "RNNEncoder",
        input_ids: Tensor1D[Sequence]
    ) -> Tuple[Tensor2D[Sequence, HiddenStates], Tensor2D[Layers, HiddenStates]]:
        """ì…ë ¥ í† í°ì„ ì›Œë“œ ì„ë² ë”©ì„ í†µí•´ ì„ë² ë”© ë³€í™˜ì„ í•©ë‹ˆë‹¤."""
        input_embeds = self.word_embeddings(input_ids)

        """RNNì„ í†µí•´ ì…ë ¥ ì„ë² ë”©ì„ ë¬¸ë§¥ ë²¡í„°(context vector)í™” í•©ë‹ˆë‹¤."""
        outputs = self.rnn(input_embeds)
        # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
        # hidden_states: Tensor2D[Sequence, HiddenStates] = FIXME
        # h_n: Tensor2D[Layers, HiddenStates] = FIXME

        return hidden_states, h_n

vocab_size = 30000
embedding_dim = 768
hidden_size = 1024  # RNNì˜ hidden size
num_layers = 1  # ìŒ“ì„ RNN layerì˜ ê°œìˆ˜
bidirectional = False  # ë‹¨ë°©í–¥ RNN

rnn_encoder = RNNEncoder(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    hidden_size=hidden_size,
    num_layers=num_layers,
    bidirectional=bidirectional
)

outputs = rnn_encoder(input_ids)
hidden_states: Tensor2D[Sequence, HiddenStates] = outputs[0]
h_n: Tensor2D[Layers, HiddenStates] = outputs[1]
print("hidden_states ì°¨ì› : ", hidden_states.shape)  # (L, B, d_h)
print("h_n ì°¨ì› : ", h_n.shape)  # (num_layers*num_dirs, B, d_h) = (1, B, d_h)

"""ë‹¤ìŒ ë””ì½”ë” ë¶€ë¶„ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤."""

# ë””ì½”ë” ëª¨ë¸ ë˜í•œ RNNì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
class Decoder(nn.Module, ABC):
    def __init__(self: "Decoder") -> None:
        super().__init__()

    @abstractmethod
    def forward(self, input_ids: torch.Tensor, init_hidden_state: torch.Tensor) -> torch.Tensor:
        pass

class RNNDecoder(Decoder):
    def __init__(
        self: "RNNDecoder",
        vocab_size: int,
        embedding_dim: int,
        hidden_size: int,
        num_layers: int,
        bidirectional: bool,
        start_token_id: int,
        end_token_id: int,
    ) -> None:
        super().__init__()
        self.start_token_id = start_token_id
        self.end_token_id = end_token_id
        # word embedding layer
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # rnn layer
        self.rnn = nn.RNN(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
        )
        # fully connected layer
        self.fully_connected_layer = nn.Linear(hidden_size, vocab_size)

    def forward(
        self: "RNNDecoder",
        init_hidden_state: Tensor2D[Layers, HiddenStates],
        max_len: int = 10
    ) -> Tuple[Tensor2D[MaxLength, VocabSize], List[int]]:
        logits: List[Tensor1D[VocabSize]] = []
        input_token: Tensor1D[Token] = torch.tensor([self.start_token_id], dtype=torch.long)
        output_token_ids: List[int] = [input_token.item()] # tensorì—ì„œ item()ì„ ì‚¬ìš©í•˜ì—¬ intë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        h_n = init_hidden_state # h_nì€ encoderì˜ h_0ì™€ ë™ì¼í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.

        for _ in range(max_len):
            if input_token == self.end_token_id:
                # ë¬¸ì¥ì˜ ì¢…ë£Œë¥¼ ì˜ë¯¸í•˜ëŠ” special token([SEP])ì´ ë‚˜ì™”ë‹¤ë©´ ì¶”ë¡ (ìƒì„±)ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
                break

            """ì§ì „ í† í°ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ê³  ìƒì„±í•œ context vectorëŠ” logitsì— ì €ì¥í•©ë‹ˆë‹¤."""
            # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
            # embedded: Tensor2D[Token, EmbeddingSize] = FIXME
            # outputs = FIXME
            # h_n: Tensor2D[Layers, HiddenStates] = FIXME
            concat_h_n: Tensor1D[HiddenStates] = h_n.squeeze(0) # ì—¬ê¸°ì„œëŠ” layer ê°¯ìˆ˜ê°€ 1ì´ê³ , bidirectionalì´ Falseì´ë¯€ë¡œ squeezeë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. (ì›ë˜ëŠ” torch.catìœ¼ë¡œ h_nì„ í•©ì¹˜ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.)

            """fully connected layerë¥¼ í†µí•´ [VocabSize]ì˜ logitì„ ìƒì„±í•©ë‹ˆë‹¤."""
            logit: Tensor1D[VocabSize] = self.fully_connected_layer(concat_h_n)
            logits.append(logit)

            """logit ë‚´ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ê°’ì„ ê°€ì§„ í† í°ì„ ì„ íƒí•©ë‹ˆë‹¤."""
            input_token: Tensor1D[Token] = torch.argmax(logit, dim=-1).unsqueeze(0)
            output_token_ids.append(input_token.item())

        """ë¦¬ìŠ¤íŠ¸ì˜ logitsë¥¼ torchì˜ Tensorë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
        logits = torch.stack(logits, dim=0)  # [max_len, vocab_size]

        return logits, output_token_ids

start_token_id: int = tokenizer.encode("[CLS]").ids[0]
end_token_id: int = tokenizer.encode("[SEP]").ids[0]

vocab_size: int = 30000
embedding_dim: int = 768
hidden_size: int = 1024  # RNNì˜ hidden size
num_layers: int = 1  # ìŒ“ì„ RNN layerì˜ ê°œìˆ˜
bidirectional: bool = False  # ë‹¨ë°©í–¥ RNN

rnn_decoder = RNNDecoder(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    hidden_size=hidden_size,
    num_layers=num_layers,
    bidirectional=bidirectional,
    start_token_id=start_token_id,
    end_token_id=end_token_id,
)
logits, output_token_ids = rnn_decoder(h_n)
output_texts = tokenizer.decode(output_token_ids)
print(output_texts)

"""ì´ì œ êµ¬í˜„í•œ encoderì™€ decoderë¥¼ ì—°ê²°í•˜ì—¬ seq2seq ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."""

class RNNSeq2Seq(nn.Module):
    def __init__(self: "RNNSeq2Seq", encoder: nn.Module, decoder: nn.Module) -> None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self: "RNNSeq2Seq", input_ids: Tensor1D[Sequence]):
        hidden_states, context_vector = self.encoder(input_ids) # encoderì—ì„œ ìƒì„±í•œ context_vector(h_n)ì„ decoder layerë¡œ ì „ë‹¬
        logits, output_tokens = self.decoder(context_vector)

        return logits, output_tokens

seq2seq = RNNSeq2Seq(rnn_encoder, rnn_decoder)
logits, output_tokens = seq2seq(input_ids)
output_token_ids = logits.argmax(dim=-1)
output_texts = tokenizer.decode(output_token_ids.tolist())
print(output_texts)

"""<blockquote>
<b>ğŸ¤” ê²°ê³¼ê°’ì´ ì´ìƒí•´ìš”</b><br>
ë°ì´í„°ë¡œ ì¶©ë¶„íˆ í•™ìŠµì„ í•˜ì§€ ì•Šì•„ì„œ ê·¸ë ‡ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ì— ëŒ€í•´ì„œ ì§‘ì¤‘í•˜ê³  ì¶”í›„ì— ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ê²½í—˜í•´ë³´ê² ìŠµë‹ˆë‹¤.
</blockquote>

ì €í¬ëŠ” Sequence to Sequence(Encoder - Decoder) êµ¬ì¡°ë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

Seq2Seq êµ¬ì¡° ë‚´ì—ì„œ ì‹¤ì œ ì›Œë“œ ì„ë² ë”©ì„ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ê·¸ ë³€í™˜ëœ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ í…ìŠ¤íŠ¸(í† í°)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì“°ì¸ ëª¨ë¸ì€ RNNì´ì˜€ìŠµë‹ˆë‹¤.

RNNë¿ë§Œ ì•„ë‹ˆë¼ LSTM, ì–´í…ì…˜ ë“±ì„ ì‚¬ìš©í•˜ì—¬ Seq2Seq êµ¬ì¡°ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì „ì²´ì ì¸ í° í‹€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•œ ì±„, RNN ëª¨ë“ˆë§Œ ë°”ê¿”ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

ê·¸ëŸ¬ë©´ ì´ì œë¶€í„° LSTMìœ¼ë¡œ ë‹¤ì‹œ í•œë²ˆ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

RNNê³¼ LSTMì˜ ê°€ì¥ í° ì°¨ì´ì ì€ LSTMì—ëŠ” cell stateê°€ ì¶”ê°€ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

ì¥ê¸° ê¸°ì–µì„ ë‹´ë‹¹í•˜ëŠ” cell stateë¥¼ í†µí•´ ì¢€ë” ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  Key point!</b><br>
ëª¨ë¸ì˜ ì•„í‚¤í…ì³ë§ˆë‹¤ ëª¨ë¸ì˜ ì…ì¶œë ¥ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ëª¨ë¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì´ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ì§€ì— ëŒ€í•´ì„œ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
</blockquote>

ê·¸ëŸ¬ë©´ Encoderì—ì„œ LSTMì„ ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.
"""

class LSTMEncoder(Encoder):
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int,
        hidden_size: int,
        num_layers: int,
        bidirectional: bool,
    ) -> None:
        super().__init__()
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
        )

    def forward(
        self: "LSTMEncoder",
        input_ids: Tensor1D[Sequence]
    )-> Tuple[
        Tensor2D[Sequence, HiddenStates], # hidden states
        Tuple[
            Tensor2D[Layers, HiddenStates], # h_n
            Tensor2D[Layers, HiddenStates] # c_n
        ]
    ]:
        # Embed -> same leading dims + embedding_dim
        input_embeds = self.word_embeddings(input_ids)  # [S,B,E] or [B,S,E]
        outputs = self.lstm(input_embeds)   # outputs: [S,B,D*H] or [B,S,D*H]
        # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
        hidden_states: Tensor2D[Sequence, HiddenStates] = outputs[0]
        h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]
        c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]

        return hidden_states, (h_n, c_n)

vocab_size = 30000
embedding_dim = 768
hidden_size = 1024  # RNNì˜ hidden size
num_layers = 1  # ìŒ“ì„ RNN layerì˜ ê°œìˆ˜
bidirectional = False  # ë‹¨ë°©í–¥ RNN

lstm_encoder = LSTMEncoder(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    hidden_size=hidden_size,
    num_layers=num_layers,
    bidirectional=bidirectional
)

outputs = lstm_encoder(input_ids)
hidden_states: Tensor2D[Sequence, HiddenStates] = outputs[0]
h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]
c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]
print("hidden_states ì°¨ì› : ", hidden_states.shape)  # (L, B, d_h)
print("h_n ì°¨ì› : ", h_n.shape)  # (num_layers*num_dirs, B, d_h) = (1, d_h)
print("c_n ì°¨ì› : ", c_n.shape)  # (num_layers*num_dirs, B, d_h) = (1, d_h)

"""ì´ë²ˆì—ëŠ” LSTMì„ ì‚¬ìš©í•˜ì—¬ Decoder Layerë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."""

class LSTMDecoder(Decoder):
    def __init__(
        self: "LSTMDecoder",
        vocab_size: int,
        embedding_dim: int,
        hidden_size: int,
        num_layers: int,
        bidirectional: bool,
        start_token_id: int,
        end_token_id: int,
    ) -> None:
        super().__init__()
        self.start_token_id = start_token_id
        self.end_token_id = end_token_id
        # word embedding layer
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # rnn layer
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
        )
        # fully connected layer
        self.fully_connected_layer = nn.Linear(hidden_size, vocab_size)

    def forward(
        self: "LSTMDecoder",
        init_hidden_state: Tensor2D[Layers, HiddenStates],
        init_cell_state: Tensor2D[Layers, HiddenStates],
        max_len: int = 10
    ) -> Tuple[Tensor2D[MaxLength, VocabSize], List[int]]:
        logits: List[Tensor1D[VocabSize]] = []
        input_token: Tensor1D[Token] = torch.tensor([self.start_token_id], dtype=torch.long)
        output_token_ids: List[int] = [input_token.item()] # tensorì—ì„œ item()ì„ ì‚¬ìš©í•˜ì—¬ intë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        h_n = init_hidden_state # h_nì€ encoderì˜ h_0ì™€ ë™ì¼í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
        c_n = init_cell_state

        for _ in range(max_len):
            if input_token == self.end_token_id:
                # ë¬¸ì¥ì˜ ì¢…ë£Œë¥¼ ì˜ë¯¸í•˜ëŠ” special token([SEP])ì´ ë‚˜ì™”ë‹¤ë©´ ì¶”ë¡ (ìƒì„±)ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
                break

            """ì§ì „ í† í°ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ê³  ìƒì„±í•œ context vectorëŠ” logitsì— ì €ì¥í•©ë‹ˆë‹¤."""
            embedded: Tensor2D[Token, EmbeddingSize] = self.word_embeddings(input_token)  # ì§ì „ ì…ë ¥ í† í°ë§Œ ì‚¬ìš© [1, embedding_dim]
            outputs = self.lstm(embedded, (h_n, c_n))   # outputs: [S,B,D*H] or [B,S,D*H]
            h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]
            c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]

            concat_h_n: Tensor1D[HiddenStates] = h_n.squeeze(0) # ì—¬ê¸°ì„œëŠ” layer ê°¯ìˆ˜ê°€ 1ì´ê³ , bidirectionalì´ Falseì´ë¯€ë¡œ squeezeë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. (ì›ë˜ëŠ” torch.catìœ¼ë¡œ h_nì„ í•©ì¹˜ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.)

            """fully connected layerë¥¼ í†µí•´ [VocabSize]ì˜ logitì„ ìƒì„±í•©ë‹ˆë‹¤."""
            logit: Tensor1D[VocabSize] = self.fully_connected_layer(concat_h_n)
            logits.append(logit)

            """ê°€ì¥ ë†’ì€ ì ìˆ˜ê°’ì„ ê°€ì§„ í† í°ì„ ì„ íƒí•©ë‹ˆë‹¤."""
            input_token: Tensor1D[Token] = torch.argmax(logit, dim=-1).unsqueeze(0)
            output_token_ids.append(input_token.item())

        """ë¦¬ìŠ¤íŠ¸ì˜ logitsë¥¼ torchì˜ Tensorë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
        logits = torch.stack(logits, dim=0)  # [max_len, vocab_size]

        return logits, output_token_ids


start_token_id: int = tokenizer.encode("[CLS]").ids[0]
end_token_id: int = tokenizer.encode("[SEP]").ids[0]

vocab_size: int = 30000
embedding_dim: int = 768
hidden_size: int = 1024 # RNNì˜ hidden size
num_layers: int = 1 # ìŒ“ì„ RNN layerì˜ ê°œìˆ˜
bidirectional: bool = False # ë‹¨ë°©í–¥ RNN

lstm_decoder = LSTMDecoder(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    hidden_size=hidden_size,
    num_layers=num_layers,
    bidirectional=bidirectional,
    start_token_id=start_token_id,
    end_token_id=end_token_id,
)

logits, output_tokens = lstm_decoder(h_n, c_n)
output_token_ids = logits.argmax(dim=-1)
output_texts = tokenizer.decode(output_token_ids.tolist())
print(output_texts)

"""Encoderì™€ Decoderë¥¼ ì‚¬ìš©í•˜ì—¬ Seq2Seq ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤."""

class LSTMSeq2Seq(nn.Module):
    def __init__(self: "LSTMSeq2Seq", encoder: nn.Module, decoder: nn.Module) -> None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self: "LSTMSeq2Seq", input_ids: Tensor1D[Sequence]):
        hidden_states, (context_vector, cell_states) = self.encoder(input_ids) # encoderì—ì„œ ìƒì„±í•œ context_vector(h_n)ì„ decoder layerë¡œ ì „ë‹¬
        logits, output_tokens = self.decoder(context_vector, cell_states)

        return logits, output_tokens

seq2seq = LSTMSeq2Seq(lstm_encoder, lstm_decoder)
logits, output_tokens = seq2seq(input_ids)
output_token_ids = logits.argmax(dim=-1)
output_texts = tokenizer.decode(output_token_ids.tolist())
print(output_texts)

"""# 3. Attention Mechanism

- í•™ìŠµ ëª©í‘œ
  1. Luong Attention(Dot Attention)ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
  2. Attentionì„ ì´ìš©í•˜ì—¬ Decoderë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë…
  1. Luong Attention
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. Luong Attentionì„ êµ¬í˜„í•œë‹¤.
  2. Seq2Seq êµ¬ì¡°ì— ë“¤ì–´ê°ˆ Decoderë¥¼ êµ¬í˜„í•œë‹¤.


ì´ë²ˆì—ëŠ” Attentionì„ ì‚¬ìš©í•œ seq2seq ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

<blockquote>
<b>ğŸ§  Attention Mechanism</b><br>
í˜„ì¬ êµ¬í˜„í•  seq2seq ëª¨ë¸ì—ì„œì˜ Attentionì€ ìµœê·¼ ì‚¬ìš©í•˜ëŠ” attentionì€ ì•„ë‹™ë‹ˆë‹¤. ìµœê·¼ì˜ Transformers ëª¨ë¸ë“¤ì€ Multi-Head Scaled Dot-Product Attentionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ê³¼ì œì—ì„œ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.
</blockquote>

1. ì „ì²´ì ì¸ Seq2Seq ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ë™ì¼í•©ë‹ˆë‹¤.
2. Encoderì—ì„œ context vectorë¥¼ ì–»ì„ ë•Œ, LSTMì„ ì‚¬ìš©í•˜ëŠ” Encoder ëª¨ë“ˆì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. Decoderì—ì„œ output tokenì„ ìƒì„±í•  ë•Œ, attention mechanismì„ ì¶”ê°€í•©ë‹ˆë‹¤.

ê·¸ëŸ¬ë©´ ìš°ì„  Dot Attention(Luong attention)ì„ ë¨¼ì € êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class LuongAttention(nn.Module):
    def __init__(self: "LuongAttention", hidden_size: int):
        super().__init__()
        self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)

    @torch.no_grad()  # í•™ìŠµ ì‹œ ì œê±°í•˜ì„¸ìš”
    def forward(
        self:"LuongAttention",
        h_t: Tensor1D[HiddenStates],
        encoder_outputs: Tensor2D[Sequence, HiddenStates],
    ) -> Tuple[Tensor1D[HiddenStates], Tensor1D[Sequence]]:
        """hidden stateë¥¼ W_aì— projectioní•˜ì—¬ Wa_htë¥¼ êµ¬í•©ë‹ˆë‹¤."""
        Wa_ht: Tensor1D[HiddenStates] = self.W_a(h_t)

        """encoder_outputsì™€ Wa_htë¥¼ ë‚´ì í•˜ì—¬ attention scoreë¥¼ êµ¬í•©ë‹ˆë‹¤."""
        # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
        attention_score: Tensor1D[Sequence] = torch.matmul(encoder_outputs, Wa_ht)

        """attention scoreë¥¼ softmax layerì— í†µê³¼ì‹œì¼œ attention weights(attention distribution)ì„ êµ¬í•©ë‹ˆë‹¤."""
        # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
        attention_weights: Tensor1D[Sequence] = F.softmax(attention_score, dim=0)

        """ê° encoderì˜ attention weightsì™€ encoderì˜ hidden stateë¥¼ ë‚´ì í•˜ì—¬ context vector(attention value)ë¥¼ êµ¬í•©ë‹ˆë‹¤."""
        # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
        context_vector: Tensor1D[HiddenStates] = torch.sum(attention_weights.unsqueeze(-1) * encoder_outputs, dim=0)

        return context_vector, attention_weights

"""<blockquote>
<b>ğŸ¤” ì—‡ ì—¬ê¸°ì„œë„ context vectorê°€ ë‚˜ì˜¤ë„¤ìš”?</b><br>
ë„¤ ê·¸ë ‡ìŠµë‹ˆë‹¤. ê³¼ê±°ì—ëŠ” encoderì˜ ë§ˆì§€ë§‰ hidden state(h_n)ì„ context vectorë¼ê³  ë¶ˆë €ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, attentionì´ ë‚˜ì˜¤ë©´ì„œ context vectorëŠ” ê° ë””ì½”ë”© ì‹œì ë§ˆë‹¤ ì¸ì½”ë”ì˜ ëª¨ë“  hidden statesì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘í•©ì´ë¼ê³  ìƒê°í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.
</blockquote>

êµ¬í˜„í•œ attention mechanismì„ ì´ìš©í•˜ì—¬ Decoder layerì— ì ìš©í•©ë‹ˆë‹¤.
"""

class AttentionDecoder(nn.Module):
    def __init__(
        self: "AttentionDecoder",
        vocab_size: int,
        embedding_dim: int,
        hidden_size: int,
        num_layers: int,
        bidirectional: bool,
        start_token_id: int,
        end_token_id: int,
    ):
        super().__init__()
        self.start_token_id = start_token_id
        self.end_token_id = end_token_id
        # word embedding layer
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # rnn layer
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
        )

        """attentionì„ ì¶”ê°€í•©ë‹ˆë‹¤."""
        self.attn = LuongAttention(hidden_size)
        """context vectorì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” trainable weights"""
        self.W_c = nn.Linear(hidden_size * 2, hidden_size)
        # fully connected layer
        self.fully_connected_layer = nn.Linear(hidden_size, vocab_size)

    @torch.no_grad()  # í•™ìŠµ ì‹œ ì œê±°
    def forward(
        self:"AttentionDecoder",
        init_hidden_state: Tensor1D[HiddenStates],
        init_cell_state: Tensor1D[HiddenStates],
        encoder_outputs: Tensor2D[Sequence, HiddenStates],
        max_len: int = 10,
    ):
        logits: List[Tensor1D[VocabSize]] = []
        input_token: Tensor1D[Token] = torch.tensor([self.start_token_id], dtype=torch.long)
        output_token_ids: List[int] = [input_token.item()] # tensorì—ì„œ item()ì„ ì‚¬ìš©í•˜ì—¬ intë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        h_n = init_hidden_state # h_nì€ encoderì˜ h_0ì™€ ë™ì¼í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
        c_n = init_cell_state

        for _ in range(max_len):
            if input_token == self.end_token_id:
                # ë¬¸ì¥ì˜ ì¢…ë£Œë¥¼ ì˜ë¯¸í•˜ëŠ” special token([SEP])ì´ ë‚˜ì™”ë‹¤ë©´ ì¶”ë¡ (ìƒì„±)ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
                break

            """ì§ì „ í† í°ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ê³  ìƒì„±í•œ context vectorëŠ” logitsì— ì €ì¥í•©ë‹ˆë‹¤."""
            embedded: Tensor2D[Token, EmbeddingSize] = self.word_embeddings(input_token)  # ì§ì „ ì…ë ¥ í† í°ë§Œ ì‚¬ìš© [1, embedding_dim]
            outputs = self.lstm(embedded, (h_n, c_n))   # outputs: [S,B,D*H] or [B,S,D*H]
            h_n: Tensor2D[Layers, HiddenStates] = outputs[1][0]
            c_n: Tensor2D[Layers, HiddenStates] = outputs[1][1]

            concat_h_n: Tensor1D[HiddenStates] = h_n.squeeze(0) # ì—¬ê¸°ì„œëŠ” layer ê°¯ìˆ˜ê°€ 1ì´ê³ , bidirectionalì´ Falseì´ë¯€ë¡œ squeezeë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. (ì›ë˜ëŠ” torch.catìœ¼ë¡œ h_nì„ í•©ì¹˜ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.)

            # ì–´í…ì…˜
            context_vector, attention_weights = self.attn(concat_h_n, encoder_outputs)

            """h_n(ì€ë‹‰ ìƒíƒœ)ì™€ context_vectorë¥¼ ì—°ê²°í•©ë‹ˆë‹¤. (Concatenate)"""
            v_t: Tensor1D[HiddenStates * 2] = torch.cat([concat_h_n, context_vector], dim=-1)

            """v_të¥¼ trainable weightsë¥¼ í†µê³¼ì‹œí‚¤ê³  tanhë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
            # TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
            attentional_hidden_state: Tensor1D[HiddenStates] = torch.tanh(self.W_c(v_t))

            """fully connected layerë¥¼ í†µí•´ [VocabSize]ì˜ logitì„ ìƒì„±í•©ë‹ˆë‹¤."""
            logit: Tensor1D[VocabSize] = self.fully_connected_layer(attentional_hidden_state)
            logits.append(logit)

            """ê°€ì¥ ë†’ì€ ì ìˆ˜ê°’ì„ ê°€ì§„ í† í°ì„ ì„ íƒí•©ë‹ˆë‹¤."""
            input_token: Tensor1D[Token] = torch.argmax(logit, dim=-1).unsqueeze(0)
            output_token_ids.append(input_token.item())

        logits = torch.stack(logits, dim=0) if logits else torch.empty(0, self.fully_connected_layer.out_features)

        return logits, output_token_ids

start_token_id: int = tokenizer.encode("[CLS]").ids[0]
end_token_id: int = tokenizer.encode("[SEP]").ids[0]

vocab_size: int = 30000
embedding_dim: int = 768
hidden_size: int = 1024 # RNNì˜ hidden size
num_layers: int = 1 # ìŒ“ì„ RNN layerì˜ ê°œìˆ˜
bidirectional: bool = False # ë‹¨ë°©í–¥ RNN

attention_decoder = AttentionDecoder(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    hidden_size=hidden_size,
    num_layers=num_layers,
    bidirectional=bidirectional,
    start_token_id=start_token_id,
    end_token_id=end_token_id,
)

logits, output_tokens = attention_decoder(h_n, c_n, hidden_states)
output_token_ids = logits.argmax(dim=-1)
output_texts = tokenizer.decode(output_token_ids.tolist())
print(output_texts)

"""Decoder layerë¥¼ êµ¬í˜„í–ˆìœ¼ë‹ˆ ì´ì œ Seq2Seq ëª¨ë¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤."""

class AttentionSeq2Seq(nn.Module):
    def __init__(self: "AttentionSeq2Seq", encoder: nn.Module, decoder: nn.Module) -> None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self: "AttentionSeq2Seq", input_ids: Tensor1D[Sequence]):
        hidden_states, (last_hidden_state, cell_states) = self.encoder(input_ids) # encoderì—ì„œ ìƒì„±í•œ h_nì„ decoder layerë¡œ ì „ë‹¬
        logits, output_tokens = self.decoder(last_hidden_state, cell_states, hidden_states)

        return logits, output_tokens

seq2seq = AttentionSeq2Seq(lstm_encoder, attention_decoder)
logits, output_tokens = seq2seq(input_ids)
output_token_ids = logits.argmax(dim=-1)
output_texts = tokenizer.decode(output_token_ids.tolist())
print(output_texts)

"""# 4. Huggingface ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©

- í•™ìŠµ ëª©í‘œ
  1. huggingface ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ê¸°í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
  2. ê¸°í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë…
  1. huggingface
- ì§„í–‰í•˜ëŠ” ì‹¤ìŠµ ìš”ì•½
  1. HuggingFace Hubì—ì„œ í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ì„ ìœ„í•´ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ(from_pretrained)ë¥¼ ì™„ì„±
  2. ë¶ˆëŸ¬ì˜¨ í† í¬ë‚˜ì´ì €ë¡œ ì…ë ¥ ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ê³ , model.generate() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë²ˆì—­ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ì™„ì„±
  3. ê³¼ì œ 2ì—ì„œ ì‚¬ìš©í•œ ë²ˆì—­ ëª¨ë¸ì´ ì‹¤ì œë¡œ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ëª¨ë‘ ê°€ì§€ê³  ìˆëŠ”ì§€ ì½”ë“œë¡œ í™•ì¸

huggingfaceëŠ” ê¸€ë¡œë²Œ ìµœëŒ€ AI ëª¨ë¸ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì…ë‹ˆë‹¤. ê³¼ê±°ì—ëŠ” ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ë§Œ ìˆì—ˆì§€ë§Œ, ìµœê·¼ì—ëŠ” ë¹„ì „, ë¡œë´‡ ë“± ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œ Seq2Seq ì•„í‚¤í…ì³ êµ¬ì¡°ì—ì„œ ë¯¸ë¦¬ í•™ìŠµí•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ë¡ ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "Helsinki-NLP/opus-mt-ko-en"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì´ Encoderì™€ Decoder ëª¨ë“ˆì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” 2ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

1. `print(model)`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì‹œê°ì ìœ¼ë¡œ ì˜ ì •ëˆëœ ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. `model.named_parameters()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í´ë˜ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

print(model)

for name, param in model.named_parameters():
    print(name)

"""ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ í†µí•´ ì¶”ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤.
ìœ„ì˜ ì‹¤ìŠµì—ì„œ ì¶”ë¡ í–ˆë˜ ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ í•™ìŠµëœ ëª¨ë¸ì´ë¯€ë¡œ ì„±ëŠ¥ì´ ë” ë†’ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
"""

text = "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤."
"""ì—¬ê¸°ì„œëŠ” batchë¡œ ì…ë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ì°¨ì›ì´ [seq_len]ì´ ì•„ë‹Œ [batch_size, seq_len]ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì…ë ¥ì´ í•œê°œì´ë¯€ë¡œ [1, seq_len]ì…ë‹ˆë‹¤."""
encoded = tokenizer(text, return_tensors="pt")

generated_ids = model.generate(
    **encoded,
    max_new_tokens=64,
)

translation = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)
print("SRC:", text)
print("MT :", translation)

"""# 5. ì•„í‚¤í…ì²˜ë³„ ëª¨ë¸ ë‹¤ë¤„ë³´ê¸°(Encoder model, Decoder model)

- í•™ìŠµ ëª©í‘œ
  1. huggingface ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë¸ êµ¬ì¡°ì˜ ëª¨ë¸ì„ ë‹¤ë£° ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ê°œë…
  1. huggingface
- í•™ìŠµ ë‚´ìš©
  1. ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•˜ëŠ” ë° ê°•ì ì´ ìˆëŠ” BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì˜ ë¹ˆì¹¸([MASK])ì— ê°€ì¥ ì ì ˆí•œ ë‹¨ì–´ë¥¼ ì¶”ë¡ 
  2. ì´ì „ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° íŠ¹í™”ëœ GPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ì•¼ê¸°ì˜ ë’·ë¶€ë¶„ì„ ì°½ì‘

ì§€ê¸ˆê¹Œì§€ëŠ” Seq2Seq(Encoder - Decoder) ëª¨ë¸ êµ¬ì¡°ë¥¼ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì€ Only Decoder ëª¨ë¸ì…ë‹ˆë‹¤.

1. Only Encoder ëª¨ë¸ : BERT ê°™ì€ ëª¨ë¸. RAGë“± ë¬¸ì„œ ê²€ìƒ‰ì— ì£¼ë¡œ ì‚¬ìš©
2. Only Decoder ëª¨ë¸ : Chat-GPT ê°™ì€ ëª¨ë¸. ëŒ€í™”, ë²ˆì—­, ì±—ë´‡ ë“± í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©
3. Encoder - Decoder ëª¨ë¸ : ìµœê·¼ì—ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

ê·¸ëŸ¬ë©´ Only Encoder ëª¨ë¸ê³¼ Only Decoder ëª¨ë¸ì„ ì´ìš©í•´ ëª¨ë¸ ì¶”ë¡ ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

Encoderì˜ ëŒ€í‘œ ëª¨ë¸ì¸ BERT ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
"""

from transformers import AutoTokenizer, AutoModelForMaskedLM

model_name = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)

"""BERT ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë¹ˆì¹¸ ë§ì¶”ê¸°(Masked Language Modeling)ë¥¼ ì¶”ë¡ í•´ë´…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, I [MASK] to school. ì´ë¼ëŠ” ë¬¸ì¥ì—ì„œ [MASK]ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë¥¼ ë§ì¶˜ë‹¤ê³  í•˜ë©´ I go to school. ì´ ë¬¸ì¥ì´ ì •ë‹µì´ ë©ë‹ˆë‹¤.

í•˜ì§€ë§Œ, I went to schoolë„ ì •ë‹µì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì²˜ëŸ¼ [MASK]ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ë  ìˆ˜ ìˆê³ , ëª¨ë¸ì˜ í•™ìŠµì— ë”°ë¼ ì–´ë–¤ ë‹¨ì–´ê°€ [MASK]ì— ë“¤ì–´ê°ˆì§€ ê²°ì •ë©ë‹ˆë‹¤.

ì´ëŸ¬í•œ íŠ¹ì„±ì„ ì´ìš©í•˜ì—¬ BERT ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë¹ˆì¹¸ ë§ì¶”ê¸°(`[MASK]`)ë¥¼ ì¶”ë¡ í•´ë´…ë‹ˆë‹¤.
"""

# 4. ìš°ë¦¬ê°€ ë§ì¶œ ë¬¸ì¥ ë§Œë“¤ê¸°. tokenizer.mask_token = "[MASK]" ì´ ë¶€ë¶„ì´ ë¹ˆì¹¸ì´ ë¨
sentence = f"I {tokenizer.mask_token} to school."

top_k = 5  # ìƒìœ„ 5ê°œ í›„ë³´ ë‹¨ì–´ë¥¼ ë³´ê³  ì‹¶ë‹¤

# 5. ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ì„œ BERTê°€ ì½ì„ ìˆ˜ ìˆê²Œ ì¤€ë¹„
encoded = tokenizer(sentence, return_tensors="pt", return_attention_mask=True)

# 6. ìˆ«ìë¡œ ëœ ë¬¸ì¥ ì •ë³´ì—ì„œ 'ì…ë ¥ í† í° ID' êº¼ë‚´ê¸°
input_ids = encoded.input_ids

# 7. [MASK]ì˜ ìˆ«ì ì•„ì´ë”” ê°€ì ¸ì˜¤ê¸°
mask_token_id = tokenizer.mask_token_id

# 8. ë¬¸ì¥ì—ì„œ [MASK]ê°€ ìˆëŠ” ìœ„ì¹˜(ì¸ë±ìŠ¤) ì°¾ê¸° mask_positionsëŠ” (ë°°ì¹˜ ë²ˆí˜¸, ë¬¸ì¥ ì† ìœ„ì¹˜) í˜•íƒœë¡œ ì €ì¥ë¨
# TODO: ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
mask_positions = (input_ids == mask_token_id).nonzero()

# 9. BERT ëª¨ë¸ì— ë¬¸ì¥(ìˆ«ìí˜•íƒœ)ì„ ë„£ì–´ì„œ ì˜ˆì¸¡ ê²°ê³¼(logits) ì–»ê¸°
outputs = model(**encoded)

# 10. logits: ê° ë‹¨ì–´ ìœ„ì¹˜ë§ˆë‹¤ 'ë‹¤ìŒ ë‹¨ì–´ì¼ ê°€ëŠ¥ì„±'ì„ ëª¨ë“  ë‹¨ì–´ ì‚¬ì „ í¬ê¸°ë§Œí¼ ê¸°ë¡í•œ ê°’
logits = outputs.logits.squeeze(0)  # (seq_len, vocab_size)

# 11. ëª¨ë“  [MASK] ìœ„ì¹˜ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê¸°
all_token_candidates: List[List[Tuple[str, float]]] = []
for _, pos in mask_positions:
    pos = pos.item()  # ìœ„ì¹˜ ìˆ«ì êº¼ë‚´ê¸°
    logits_at_pos = logits[pos]  # í•´ë‹¹ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ ì ìˆ˜
    probs = torch.softmax(logits_at_pos, dim=-1)  # ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜
    topk = torch.topk(probs, k=top_k)  # í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 5ê°œ ì„ íƒ

    ids = topk.indices.tolist()   # ë‹¨ì–´ ID
    scores = topk.values.tolist() # í™•ë¥  ê°’

    # ë‹¨ì–´ IDë¥¼ ì‹¤ì œ ë‹¨ì–´(í† í°)ë¡œ ë³€í™˜
    tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in ids]

    # (ë‹¨ì–´, í™•ë¥ ) í˜•íƒœë¡œ ë¬¶ì–´ì„œ ì €ì¥
    candidates = list(zip(tokens, scores))
    all_token_candidates.append(candidates)

# 12. [MASK]ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë¡œ ì™„ì„±ëœ ë¬¸ì¥ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
restored_sentences: List[str] = []

# 13. ì²« ë²ˆì§¸ [MASK] ìœ„ì¹˜ì˜ í›„ë³´ ë‹¨ì–´ë“¤
token_candidates: List[Tuple[str, float]] = all_token_candidates[0]

# 14. í›„ë³´ ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì”© ë„£ì–´ì„œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë³´ê¸°
for tok, _ in token_candidates:
    new_ids = input_ids.clone()  # ì›ë˜ ë¬¸ì¥ì˜ ìˆ«ì ë³µì‚¬
    tok_id = tokenizer.convert_tokens_to_ids(tok)  # í›„ë³´ ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë³€í™˜
    new_ids[0, mask_positions[0, 1]] = tok_id      # [MASK] ìœ„ì¹˜ì— í›„ë³´ ë‹¨ì–´ ID ë„£ê¸°
    text = tokenizer.decode(new_ids[0], skip_special_tokens=True)  # ë‹¤ì‹œ ê¸€ìë¡œ ë³€í™˜
    restored_sentences.append(text.strip())  # ì•ë’¤ ê³µë°± ì œê±° í›„ ì €ì¥

# 15. ê²°ê³¼ ì¶œë ¥
print("ì›ë³¸ ë¬¸ì¥:", sentence)
print("BERTê°€ ì˜ˆì¸¡í•œ ë¬¸ì¥ë“¤:")
for idx, sent in enumerate(restored_sentences, start=1):
    print("{}ìˆœìœ„: {}".format(idx, sent))

"""Only Decoder ëª¨ë¸ì˜ ëŒ€í‘œì¸ GPT ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì¶”ë¡ ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

GPT-2 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

"""GPT-2 ëª¨ë¸ì€ ì…ë ¥ìœ¼ë¡œ í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ë°›ê³ , ê·¸ ë’¤ì— ì˜¬ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡(Next token Prediction)í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ ìŠ¤í† ë¦¬(ì…ë ¥ í…ìŠ¤íŠ¸)ì˜ ë’· ë‚´ìš©ì„ ìƒì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.
"""

prompt = "Once upon a time in a small village, a curious child found a mysterious key."
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=64,
    )

output_tokens = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)
print(output_tokens)