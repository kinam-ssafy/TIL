# 가상 메모리 & 페이지 교체

## 들어가기 전에 알아야 할 배경 지식

### 물리 메모리의 한계
- **제한된 크기**: 8GB, 16GB 등 물리적 제약 존재
- **동시 실행 프로세스**: 여러 프로그램이 메모리를 나눠 사용
- **메모리 부족 현상**: 실행하려는 프로그램들의 총 크기 > 물리 메모리

### 프로그램 실행의 특성
- **지역성 원리**: 프로그램은 전체 코드를 동시에 사용하지 않음
- **순차적 실행**: 대부분의 시간은 일부 코드만 실행
- **조건부 실행**: 특정 조건에서만 실행되는 코드 존재

---

## 가상 메모리 (Virtual Memory)

### 사전적 의미
가상 메모리(Virtual Memory)는 "실제로는 존재하지 않지만 있는 것처럼 보이는 메모리"를 의미하며, 물리 메모리의 크기 제한을 극복하기 위한 메모리 관리 기법을 의미함

### 가상 메모리의 핵심 아이디어

#### 기본 개념
```
현실적 관찰:
- 프로그램 전체가 항상 메모리에 있을 필요 없음
- 현재 실행 중인 부분만 메모리에 있으면 됨
- 사용하지 않는 부분은 디스크에 저장해도 됨

가상 메모리 해결책:
- 필요한 부분만 메모리에 적재
- 나머지는 보조기억장치(디스크)에 저장
- 사용자에게는 큰 메모리가 있는 것처럼 보임
```


### 가상 메모리 구현 방식

#### 요구 페이징 (Demand Paging)
페이지 단위로 필요할 때만 메모리에 적재하는 방식

```
Pure Demand Paging 방식:
1. 프로세스 시작: 아무 페이지도 메모리에 없음
2. 페이지 접근 시도: 페이지 폴트 발생  
3. 운영체제: 해당 페이지를 디스크에서 메모리로 가져옴
4. 실행 계속
```

#### 요구 세그멘테이션 (Demand Segmentation)
세그먼트 단위로 관리하는 방식 (덜 일반적)


---

## 요구 페이징 (Demand Paging)

### 핵심 구성 요소

#### Valid/Invalid 비트
페이지 테이블의 각 엔트리에 있는 상태 비트

```
페이지 테이블 구조:
┌─────────┬─────────┬─────────┐
│페이지 번호│프레임 번호│ v/i 비트│
├─────────┼─────────┼─────────┤
│    0    │    3    │    v    │ ← 메모리에 있음
│    1    │    -    │    i    │ ← 디스크에 있음  
│    2    │    7    │    v    │ ← 메모리에 있음
│    3    │    -    │    i    │ ← 디스크에 있음
└─────────┴─────────┴─────────┘

v (valid): 해당 페이지가 메모리에 적재됨
i (invalid): 해당 페이지가 메모리에 없음 (디스크에 있음)
```

### 페이지 폴트 (Page Fault)

#### 발생 상황
CPU가 invalid로 표시된 페이지에 접근하려 할 때 발생하는 인터럽트

#### 페이지 폴트 처리 과정
```
1단계: 하드웨어 동작
├─ CPU가 페이지 테이블 확인
├─ Invalid 비트 발견
└─ 페이지 폴트 트랩 발생

2단계: 운영체제 개입
├─ 인터럽트 서비스 루틴 실행
├─ 잘못된 접근인지 확인 (보호 위반, 잘못된 주소 등)
└─ 정상적인 페이지 폴트라면 처리 계속

3단계: 빈 프레임 찾기
├─ Free Frame List에서 빈 프레임 찾기
├─ 빈 프레임이 없으면 페이지 교체 알고리즘 실행
└─ 희생 페이지 선택 및 디스크에 저장 (필요시)

4단계: 페이지 로딩
├─ 디스크에서 요청된 페이지 읽기
├─ 빈 프레임에 페이지 적재
└─ 페이지 테이블 업데이트 (i → v)

5단계: 실행 재개
├─ 인터럽트에서 복귀
└─ 페이지 폴트를 발생시킨 명령어 다시 실행
```
--------------------
### 페이지 폴트 트랩(Page Fault Trap)  
- CPU가 메모리에 없는 페이지에 접근하려 할 때 발생시키는 인터럽트 신호를 의미

### 디스크 읽기가 먼저인 이유

디스크 읽기는 매우 느린 작업:
- 디스크 회전 대기
- 헤드 이동 시간
- 실제 데이터 읽기

→ 읽는 동안 다른 작업 가능  
→ CPU가 다른 프로세스 실행 가능

-------------------------

<br>
<br>
<br>


## 페이지 폴트 처리 시간

### 시간 구성 요소
```
총 페이지 폴트 처리 시간:
= 서비스 오버헤드 
+ 페이지 읽기 시간 
+ 프로세스 재시작 시간

구체적 시간 (일반적):
서비스 오버헤드: 1-100μs
페이지 읽기 시간: 1-10ms (디스크 접근)
재시작 시간: 1-100μs

→ 디스크 접근이 전체 시간의 99% 차지
```

서비스 오버헤드 : 페이지 폴트를 처리하기 위한 운영체제의 관리 작업 시간.  
실제 페이지를 가져오는 것 외에 부가적으로 들어가는 시간  


### 페이지 교체의 필요성

#### 상황 발생
```
메모리 가득 참 + 새로운 페이지 필요:
- 빈 프레임 없음
- 새로운 페이지를 적재할 공간 없음

해결 방법:
- 기존 페이지 중 하나를 내보내기 (Swap Out)
- 새로운 페이지 가져오기 (Swap In)

문제: 어떤 페이지를 내보낼 것인가?
```

---

## 페이지 교체 알고리즘

### 평가 기준
- **페이지 폴트율**: 낮을수록 좋음
- **구현 복잡도**: 단순할수록 좋음  
- **오버헤드**: 적을수록 좋음

### 1. FIFO (First-In-First-Out)

#### 기본 개념
**가장 먼저 메모리에 들어온 페이지를 가장 먼저 교체**

#### 동작 방식
```
큐(Queue) 자료구조 사용:
┌─────┬─────┬─────┐
│ A   │ B   │ C   │ ← 메모리 (3개 프레임)
└─────┴─────┴─────┘
  ↑             ↑
 가장 오래됨    가장 최근

새 페이지 D 요청:
1. A를 제거 (가장 오래됨)
2. D를 끝에 추가
3. 결과: B → C → D
```

#### 구현 예시
```
참조 문자열: 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2
메모리 프레임: 3개

시간순 진행:
7: [7, -, -] (페이지 폴트)
0: [7, 0, -] (페이지 폴트)  
1: [7, 0, 1] (페이지 폴트)
2: [2, 0, 1] (페이지 폴트, 7 교체)
0: [2, 0, 1] (히트)
3: [2, 3, 1] (페이지 폴트, 0 교체)
0: [2, 3, 0] (페이지 폴트, 1 교체)
...

총 페이지 폴트: 많이 발생
```

#### 장단점
```
장점:
- 구현이 매우 단순함
- 오버헤드 최소 (비교할 것 없음. O(1) 시간 복잡도)
- 공정함 (들어온 순서대로 처리)

단점:
- Belady의 이상 현상 발생 가능
- 자주 사용하는 페이지도 교체될 수 있음
- 프로그램의 지역성을 고려하지 않음 (프로그램이 특정 시간에 특정 부분만 집중적으로 사용하는 경향)
```

#### Belady의 이상 현상
```
일반적 예상: 프레임 수 증가 → 페이지 폴트 감소
FIFO 현실: 프레임 수 증가 → 페이지 폴트 증가 (이상!)

예시:
참조 문자열: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5

3개 프레임:

1:  [1, -, -]     폴트
2:  [1, 2, -]     폴트
3:  [1, 2, 3]     폴트
4:  [4, 2, 3]     폴트 (1 교체)
1:  [4, 1, 3]     폴트 (2 교체)
2:  [4, 1, 2]     폴트 (3 교체)
5:  [5, 1, 2]     폴트 (4 교체)
1:  [5, 1, 2]     히트
2:  [5, 1, 2]     히트
3:  [5, 3, 2]     폴트 (1 교체)
4:  [5, 3, 4]     폴트 (2 교체)
5:  [5, 3, 4]     히트

총 9회 페이지 폴트


4개 프레임:

1:  [1, -, -, -]  폴트
2:  [1, 2, -, -]  폴트
3:  [1, 2, 3, -]  폴트
4:  [1, 2, 3, 4]  폴트
1:  [1, 2, 3, 4]  히트
2:  [1, 2, 3, 4]  히트
5:  [5, 2, 3, 4]  폴트 (1 교체)
1:  [5, 1, 3, 4]  폴트 (2 교체)
2:  [5, 1, 2, 4]  폴트 (3 교체)
3:  [5, 1, 2, 3]  폴트 (4 교체)
4:  [4, 1, 2, 3]  폴트 (5 교체)
5:  [4, 5, 2, 3]  폴트 (1 교체)

총 10회 페이지 폴트 ← 더 많음

```



### 2. LRU (Least Recently Used)

#### 기본 개념
**가장 오래 전에 사용된 페이지를 교체**

#### 지역성 원리 기반
```
시간적 지역성 (Temporal Locality):
- 최근에 참조된 페이지는 가까운 미래에 다시 참조될 가능성 높음
- 오래 전에 참조된 페이지는 다시 참조될 가능성 낮음

LRU 아이디어:
→ 가장 오래 전에 사용된 페이지 = 앞으로도 사용 안 될 가능성 높음
```

#### 구현 방법

##### Counter 방식
```
각 페이지마다 카운터 유지:
┌─────┬─────────┬─────────────┐
│페이지│프레임 번호│ 마지막 참조시간│
├─────┼─────────┼─────────────┤
│ A   │    1    │    100      │
│ B   │    2    │    150      │ ← 가장 최근 사용
│ C   │    3    │     80      │ ← 가장 오래 전 사용
└─────┴─────────┴─────────────┘

교체 시: 카운터 값이 가장 작은 페이지 선택
```

##### Stack 방식
```
참조될 때마다 스택 맨 위로 이동:

참조 순서: A → B → C → A
┌─────┐
│  A  │ ← 가장 최근 (Stack Top)
├─────┤
│  C  │
├─────┤  
│  B  │ ← 가장 오래됨 (교체 대상)
└─────┘
```

#### 동작 예시
```
참조 문자열: 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2
메모리 프레임: 3개

시간순 진행:
7: [7] (페이지 폴트)
0: [0, 7] (페이지 폴트)
1: [1, 0, 7] (페이지 폴트)  
2: [2, 1, 0] (페이지 폴트, 7 교체 - 가장 오래됨)
0: [0, 2, 1] (히트, 0을 맨 앞으로)
3: [3, 0, 2] (페이지 폴트, 1 교체)
0: [0, 3, 2] (히트, 0을 맨 앞으로)
...

FIFO보다 페이지 폴트 적게 발생
```

#### 장단점
```
장점:
- 지역성 원리 활용으로 좋은 성능
- Belady의 이상 현상 없음
- 이론적으로 최적에 가까운 성능

단점:
- 구현이 복잡함
- 하드웨어 지원 필요 (시간 정보 관리)
- 상당한 오버헤드
```

### 3. LFU (Least Frequently Used)

#### 기본 개념
**참조 횟수가 가장 적은 페이지를 교체**

#### 동작 방식
```
각 페이지의 참조 횟수 카운트:
┌─────┬─────────┬─────────┐
│페이지│프레임 번호│ 참조 횟수│
├─────┼─────────┼─────────┤
│ A   │    1    │    5    │
│ B   │    2    │    8    │
│ C   │    3    │    2    │ ← 교체 대상 (최소 참조)
└─────┴─────────┴─────────┘
```

#### 장단점
```
장점:
- 장기적 참조 패턴 반영
- 자주 사용하는 페이지 보호

단점:
- 최근성을 반영하지 못함
- 초기 참조가 많은 페이지가 계속 메모리에 남음
- 구현 복잡
```

### 4. Clock 알고리즘 (Second Chance)

#### 기본 개념
**LRU의 근사 알고리즘으로, 참조 비트를 이용해 간단히 구현**

#### 하드웨어 지원
```
참조 비트 (Reference Bit):
- 페이지 참조 시 하드웨어가 자동으로 1로 설정
- 운영체제가 주기적으로 0으로 초기화
- 최근에 참조되었는지 간단히 판단 가능
```

#### 동작 방식
```
원형 큐 구조로 페이지들을 배치:

     [A, ref=1]
         ↑
 [D, ref=0] → [B, ref=0]  
         ↓
     [C, ref=1]

포인터가 시계 방향으로 이동하면서:
1. 참조 비트 = 1이면 → 0으로 바꾸고 다음으로
2. 참조 비트 = 0이면 → 해당 페이지 교체
```

#### 구체적 동작 예시
```
교체할 페이지를 찾는 과정:

1단계: A 확인 (ref=1) → 0으로 바꾸고 다음
2단계: B 확인 (ref=0) → 교체 대상 발견!

결과: B 페이지를 새 페이지로 교체
```

#### 개선된 Clock 알고리즘
```
참조 비트 + 수정 비트 사용:
(ref, mod) 조합으로 우선순위 결정

우선순위 (낮을수록 교체 우선):
1. (0, 0): 최근 참조X, 수정X → 최우선 교체
2. (0, 1): 최근 참조X, 수정O → 디스크 쓰기 필요  
3. (1, 0): 최근 참조O, 수정X → 참조 비트 클리어 후 재고려
4. (1, 1): 최근 참조O, 수정O → 최후 순위
```

#### 장단점
```
장점:
- LRU의 성능과 FIFO의 단순성 결합
- 하드웨어 지원 최소 (참조 비트만)
- 실제 시스템에서 널리 사용

단점:
- LRU보다는 성능이 떨어질 수 있음
> LRU는 정확한 마지막 사용 시간 기록. Clock은 참조 비트만 사용하므로 누가 더 오래 됐는지 모름
- 참조 비트 관리 필요
```

### 5. Optimal (OPT) 알고리즘

#### 기본 개념
**미래에 가장 오래 사용되지 않을 페이지를 교체** (이론적 최적해)

#### 동작 방식
```
미래 참조 문자열을 미리 알고 있다고 가정:
참조 문자열: 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2
현재 위치: ↑

메모리 상태: [7, 0, 1]
다음 참조: 2

각 페이지의 다음 참조 시점:
- 7: 미래에 참조 안 됨 → ∞
- 0: 위치 4에서 참조 → 4
- 1: 미래에 참조 안 됨 → ∞

결정: 7 또는 1 교체 (둘 다 미래에 사용 안 됨)
```

#### 의의
```
실용성:
- 실제 구현 불가능 (미래를 알 수 없음)
- 다른 알고리즘의 성능 평가 기준으로 사용
- 이론적 하한선 제공
```

---

## 성능 최적화 기법

### 1. TLB (Translation Lookaside Buffer)

#### TLB의 필요성
```
페이징 시스템의 문제점:
1. 논리 주소 → 페이지 테이블 접근 (메모리 접근 1번)
2. 물리 주소 → 실제 데이터 접근 (메모리 접근 2번)
총 2번의 메모리 접근 → 성능 저하

TLB 해결책:
- 최근 사용한 페이지 테이블 엔트리를 고속 캐시에 저장
- 캐시 히트 시 1번의 메모리 접근으로 단축
```

논리 주소 (Logical Address):
- 프로그램이 사용하는 가상의 주소
- 프로세스마다 독립적

물리 주소 (Physical Address):
- 실제 메모리(RAM)에서의 위치




#### TLB 동작
```
주소 변환 과정:
1. TLB에서 페이지 번호 검색
2. TLB 히트 → 바로 물리 주소 획득
3. TLB 미스 → 페이지 테이블 접근 → TLB 업데이트

성능 효과:
TLB 히트율 90%, TLB 접근 1사이클, 메모리 접근 100사이클
평균 시간 = 0.9 × 1 + 0.1 × 100 = 10.9사이클
TLB 없이: 100사이클 → 약 9배 빨라짐
```




### 2. 프리페칭 (Prefetching)

#### 개념
**페이지 폴트가 발생하기 전에 미리 페이지를 메모리에 적재**

#### 종류
```
공간적 지역성 기반:
- 요청된 페이지 주변 페이지들도 함께 적재
- 순차적 접근 패턴에서 효과적

시간적 지역성 기반:  
- 과거 참조 패턴 분석해서 예측
- 반복적 접근 패턴에서 효과적
```

### 3. 페이지 크기 최적화

#### 크기에 따른 트레이드오프
```
큰 페이지 (예: 4MB):
✅ 페이지 테이블 크기 감소
✅ TLB 미스 감소
❌ 내부 파편화 증가
❌ 불필요한 데이터까지 적재

작은 페이지 (예: 4KB):
✅ 내부 파편화 감소
✅ 필요한 데이터만 적재
❌ 페이지 테이블 크기 증가
❌ TLB 미스 증가
```

---

## 스래싱 (Thrashing)

### 개념
**페이지 폴트가 너무 자주 발생하여 시스템 성능이 급격히 저하되는 현상**

### 발생 원인
```
악순환 구조:
1. 프로세스가 필요한 페이지들을 메모리에 유지하지 못함
2. 페이지 폴트 빈번 발생
3. 디스크 I/O 시간이 대부분 차지
4. CPU 이용률 급감
5. 운영체제가 더 많은 프로세스를 메모리에 올림 (멀티프로그래밍 증가)
6. 1단계로 돌아가서 더욱 악화
```

### Working Set 모델

#### 개념
**프로세스가 정상적으로 실행하기 위해 메모리에 있어야 하는 페이지들의 집합**

#### Working Set 크기 결정
```
Working Set Window (Δ):
- 최근 Δ번의 페이지 참조에서 참조된 서로 다른 페이지들
- 예: Δ = 10이고 최근 10번 참조가 {1,2,3,2,1,3,4,2,3,4}
- Working Set = {1, 2, 3, 4} (4개 페이지)

Working Set이 메모리에 있어야 스래싱 방지
```

### 스래싱 방지 방법
```
1. Working Set 모니터링:
   - 각 프로세스의 Working Set 크기 추적
   - 전체 Working Set > 물리 메모리 시 일부 프로세스 swap out

2. 페이지 폴트 빈도 제어:
   - 페이지 폴트율이 임계값 초과 시 프로세스 중단
   - 시스템 부하 조절

3. 지역성 개선:
   - 프로그램 구조 최적화
   - 데이터 구조 재배치
```

---

## 실제 시스템에서의 구현

### Linux 가상 메모리

#### 4-Level 페이징
```
가상 주소 구조 (x86-64):
┌────┬────┬────┬────┬─────────┐
│PGD │PUD │PMD │PTE │ Offset  │
└────┴────┴────┴────┴─────────┘
 9bit 9bit 9bit 9bit   12bit

4단계 페이지 테이블:
PGD → PUD → PMD → PTE → Physical Page
```

#### LRU 근사 알고리즘
```
Linux의 2-list 전략:
- Active List: 최근 참조된 페이지들
- Inactive List: 오래된 페이지들

페이지 이동:
참조 → Active List 이동
시간 경과 → Inactive List 이동  
교체 → Inactive List에서 선택
```

### Windows 가상 메모리

#### 페이지 교체 정책
```
Modified Page Writer:
- 수정된 페이지들을 백그라운드에서 디스크에 기록
- 페이지 교체 시 디스크 쓰기 시간 단축

Cluster-based 교체:
- 여러 페이지를 한 번에 교체
- 디스크 I/O 효율성 향상
```

---

## 성능 측정 및 튜닝

### 주요 성능 지표
```
페이지 폴트율:
Page Fault Rate = 페이지 폴트 수 / 전체 메모리 참조 수

유효 메모리 접근 시간:
EAT = (1 - p) × 메모리 접근 시간 + p × 페이지 폴트 처리 시간
여기서 p = 페이지 폴트율

예시: p = 0.01, 메모리 접근 = 100ns, 페이지 폴트 = 10ms
EAT = 0.99 × 100ns + 0.01 × 10ms = 99ns + 100,000ns = 100,099ns
→ 페이지 폴트 1%만으로도 1000배 느려짐!
```

### 시스템 모니터링
```
Linux 명령어:
- vmstat: 페이지 교체 통계
- sar: 시스템 성능 통계  
- top: 메모리 사용량
- /proc/meminfo: 상세 메모리 정보

Windows 도구:
- 성능 모니터
- 리소스 모니터
- Perfmon
```

---

## 핵심 정리

### 가상 메모리의 본질
```
핵심 아이디어:
- 물리 메모리 크기 제한 극복
- 프로그램의 지역성 활용
- 필요한 부분만 메모리에 유지

구현 기법:
- 요구 페이징 (Demand Paging)
- 페이지 교체 알고리즘
- 성능 최적화 (TLB, 프리페칭)
```

### 페이지 교체 알고리즘 선택 기준
```
성능 우선: LRU > Clock > FIFO
구현 단순성: FIFO > Clock > LRU  
실제 사용: Clock 알고리즘 (성능과 단순성의 균형)
이론적 기준: OPT (최적해 제공)
```

### 실무에서의 고려사항
```
설계 단계:
- 프로그램의 메모리 접근 패턴 분석
- Working Set 크기 예측
- 지역성을 고려한 자료구조 설계

운영 단계:
- 페이지 폴트율 모니터링
- 스래싱 현상 감지
- 메모리 사용량 최적화

튜닝 포인트:
- 페이지 크기 조정
- TLB 효율성 극대화
- 프리페칭 정책 조정
```

가상 메모리는 **현대 운영체제의 핵심 기능**으로, 제한된 물리 메모리로 거의 무제한의 주소 공간을 제공하는 환상을 만들어냄. 페이지 교체 알고리즘의 선택과 최적화는 시스템 전체 성능을 좌우하는 중요한 요소임